UNIVERSIDAD NACIONAL DEL ALTIPLANO
FACULTAD DE INGENIERÍA MECÁNICA ELÉCTRICA, ELECTRÓNICA Y SISTEMAS

ESCUELA PROFESIONAL DE INGENIERÍA DE SISTEMAS

“ANÁLISIS DE DATOS CON HERRAMIENTAS DE BIG DATA PARA LA
TOMA DE DECISIONES EN LA UGEL DE AZÁNGARO”

TESIS
PRESENTADO POR:
JOSÉ WALTER MAMANI CONDORI

PARA OPTAR EL TÍTULO PROFESIONAL DE:
INGENIERO DE SISTEMAS

PUNO - PERÚ
2015

www.nitropdf.com

ÁREA: Informática
TEMA: Sistemas de información tradicionales y expertos

www.nitropdf.com

AGRADECIMIENTO
A la Universidad Nacional del Altiplano, escuela profesional de Ingeniería de
Sistemas y sus catedráticos que me impartieron en aulas todos sus
conocimientos y mostraron también sus valores permitiendo así una formación
integral en mí y en varios de sus estudiantes, por eso muchísimas gracias a
ustedes mis docentes.
De manera muy especial quiero agradecer al Mg. Ing. Elmer Coyla Idme,
director de tesis; a los señores; M.Sc. Ing. Edelfré Flores Velásquez, Mg. Ing.
Oliver Amadeo Vilca Huayta, Ing. Magali Gianina Gonzales Paco, jurados de
tesis y docentes que por varios años impartieron sus conocimientos y que por
ultimo aportaron con criterios, consejos, conocimientos y tiempo valiosos para
la culminación de este trabajo. Para todos ustedes mi gratitud y respeto.
Así mismo, quisiera expresar mi agradecimiento a todos quienes estuvieron
vinculados de alguna manera a este trabajo; al personal que labora en la
Unidad de Gestión Educativa Local de Azángaro, por proporcionarme las
facilidades necesarias para completar la investigación; a todos aquellos que me
acompañaron y con un granito de arena hicieron un inestimable aporte.
Y, desde luego, llego al final de este trabajo gracias a Dios; al invaluable apoyo
e inspiración que género mí querida mamita, hermanos y sobrinos; a mis
amigas/os, a quienes siempre tengo presente; a mis docentes y a quienes
siempre me han enseñado algo.
A todos, mi mayor reconocimiento y gratitud.

www.nitropdf.com

DEDICATORIA

A Dios por darme todo lo necesario para
alcanzar las estrellas. Gracias señor por la
paciencia, fuerza, valor, amor y determinación
que me diste para hacer las cosas siempre de
la mejor manera.

A mi mamita, por su amor y apoyo
incondicional a través de todos estos años.

A mis hermanos y mis amigas/os que con su
entusiasmo y cariño me dieron el valor y coraje
para caminar.

A todos aquellos que son movidos por un gran
amor a la vida y a la naturaleza, que están
convencidos de que un mundo mejor es
posible, si se tiene los conocimientos para
ayudar y el corazón para vencer.

A todos ustedes dedico el producto de mi
esfuerzo.

www.nitropdf.com

ÍNDICE
ÍNDICE .......................................................................................................5
ÍNDICE DE TABLAS ...................................................................................8
ÍNDICE DE FIGURAS ...............................................................................10
RESUMEN ................................................................................................14
ABSTRACT ..............................................................................................15
INTRODUCCIÓN ......................................................................................16
CAPÍTULO I
PLANTEAMIENTO DEL PROBLEMA DE INVESTIGACIÓN
1.1

DESCRIPCIÓN DEL PROBLEMA .....................................................19

1.1.1 Problema general. ....................................................................21
1.1.2 Problemas específicos. ............................................................21
1.2

JUSTIFICACIÓN DEL PROBLEMA....................................................22

1.3

OBJETIVOS DE LA INVESTIGACIÓN ................................................23

1.3.1 Objetivo General ......................................................................23
1.3.2 Objetivos Específicos ...............................................................23
CAPÍTULO II
MARCO TEÓRICO
2.1

ANTECEDENTE DE LA INVESTIGACIÓN ...........................................25

2.1.1 Antecedentes Nacionales ........................................................25
2.1.2 Antecedentes Internacionales ..................................................28
2.2

SUSTENTO TEÓRICO...................................................................33

2.2.1 Sistemas de soporte a la toma de decisiones (DSS). ..............33
2.2.2 Toma de decisiones. ................................................................34
2.2.3 La computación en la nube. .....................................................48
2.2.4 Big Data. ..................................................................................56
2.2.5 Hadoop. ...................................................................................67
2.2.6 Hadoop versión 1. ....................................................................70
2.2.7 Hadoop versión 2. ....................................................................82

www.nitropdf.com

2.2.8 Procesamiento de datos. .........................................................94
2.2.9 Cross Industry Standard Process for Data Mining (CRISPDM). ...................................................................................... 111
2.2.10 Métricas y calidad de software .............................................114
2.3

GLOSARIO DE TÉRMINOS BÁSICOS.............................................123

2.4

HIPÓTESIS DE LA INVESTIGACIÓN ...............................................130

2.4.1 Hipótesis general. ..................................................................130
2.4.2 Hipótesis específicas. ............................................................130
2.5

OPERACIONALIZACIÓN DE VARIABLES.........................................130
CAPÍTULO III
DISEÑO METODOLÓGICO DE INVESTIGACIÓN

3.1

TIPO Y DISEÑO DE LA INVESTIGACIÓN .........................................133

3.2

POBLACIÓN Y MUESTRA DE LA INVESTIGACIÓN ............................133

3.2.1 Población. ..............................................................................133
3.2.2 Muestra. .................................................................................134
3.3

ÁMBITO DE ESTUDIO .................................................................134

3.4

MATERIAL EXPERIMENTAL .........................................................136

3.5

TÉCNICAS E INSTRUMENTOS PARA RECOLECTAR INFORMACIÓN ....137

3.6

TÉCNICAS PARA EL PROCESAMIENTO Y ANÁLISIS DE DATOS ..........137

3.7

PROCEDIMIENTO DEL EXPERIMENTO...........................................138

3.7.1 Comprensión del Negocio ......................................................138
3.7.2 Comprensión de los datos .....................................................151
3.7.3 Preparación de los datos .......................................................166
3.7.4 Modelado ...............................................................................168
3.7.5 Evaluación .............................................................................175
3.7.6 Implementación ......................................................................178
3.8

PLAN DE TRATAMIENTO DE DATOS ..............................................181

3.9

DISEÑO ESTADÍSTICO PARA LA PRUEBA DE HIPÓTESIS ..................182
CAPÍTULO IV
ANÁLISIS E INTERPRETACIÓN DE RESULTADOS DE LA
INVESTIGACIÓN

www.nitropdf.com

4.1

LA CORRELACIÓN DE DATOS EN LA SOLUCIÓN DEL ANÁLISIS DE
DATOS PARA LA TOMA DE DECISIONES. ...................................... 185

4.2

EL ALMACENAMIENTO DE DATOS HISTÓRICOS EN LA SOLUCIÓN
DEL ANÁLISIS DE DATOS PARA LA TOMA DE DECISIONES. .............. 191

4.3

EL PROCESAMIENTO DE DATOS EN LA SOLUCIÓN DEL ANÁLISIS DE
DATOS PARA LA TOMA DE DECISIONES. ...................................... 195

4.4

LA VISUALIZACIÓN DE DATOS EN LA SOLUCIÓN DEL ANÁLISIS DE
DATOS PARA LA TOMA DE DECISIONES. ...................................... 203

4.5

EVALUACIÓN DE LA SOLUCIÓN DEL ANÁLISIS DE DATOS ................206

4.5.1 Métrica de calidad de la solución ...........................................206
4.6

PRUEBA DE HIPÓTESIS .............................................................211

CONCLUSIONES ...................................................................................214
SUGERENCIAS......................................................................................216
BIBLIOGRAFÍA .......................................................................................217
ANEXOS .................................................................................................221

www.nitropdf.com

ÍNDICE DE TABLAS
TABLA Nº 1: TABLA 'PERSONAS' DE UNA BBDD RELACIONAL. .................................65
TABLA Nº 2: DESCRIPCIÓN DE ALGUNOS DE LOS MÉTODOS DE ANÁLISIS DE DATOS
TRADICIONALES MÁS UTILIZADOS. ....................................................

95

TABLA Nº 3: DESCRIPCIÓN DE LAS TÉCNICAS UTILIZADAS PARA ANALIZAR GRANDES
CONJUNTOS DE DATOS. ..................................................................

96

TABLA Nº 4: LOS CAMPOS DE INVESTIGACIÓN EN EL CAMPO DEL ANÁLISIS DE DATOS. .97
TABLA Nº 5: DIFERENTES ETAPAS DE LA MINERÍA....................................................99
TABLA Nº 6: RESPONSABILIDADES DE LOS COMPONENTES DE UN SISTEMA DE
MINERÍA DE DATOS. ........................................................................

101

TABLA Nº 7: ALGUNOS EJEMPLOS DE LOS COMANDOS DE R EN CONSOLA. ..............103
TABLA Nº 8: CARACTERÍSTICAS DE PIG. ...............................................................107
TABLA Nº 9: ENTIDADES FUNCIONALES DE SPLUNK. ..............................................107
TABLA Nº 10: HERRAMIENTAS DE VISUALIZACIÓN DE D3........................................110
TABLA Nº 11: FACTORES Y MÉTRICAS DE CALIDAD. ...............................................118
TABLA Nº 12: OPERACIONALIZACIÓN DE VARIABLES. .............................................131
TABLA Nº 13: DISEÑO DE LA INVESTIGACIÓN. .......................................................133
TABLA Nº 14: POBLACIÓN DE AFECTADOS POR EL ALGORITMO DE ANÁLISIS DE
DATOS. ....................................................................................... 134

TABLA Nº 15: MUESTRA DE USUARIOS DEL ALGORITMO. ........................................134
TABLA Nº 16: INVENTARIO DE RECURSOS.............................................................142
TABLA Nº 17: LISTA DE RIESGOS DEL PROYECTO. .................................................143
TABLA Nº 18: PLAN DE CONTINGENCIA.................................................................144
TABLA Nº 19: DESCRIPCIÓN DE LAS VARIABLES EXTRAÍDAS PARA LISTAR LAS
OFICINAS. ................................................................................... 155
TABLA Nº 20: DESCRIPCIÓN DE LAS VARIABLES EXTRAÍDAS PARA LISTAR LOS TIPOS
DE DOCUMENTOS. ........................................................................ 156

TABLA Nº 21: DESCRIPCIÓN DE LAS VARIABLES EXTRAÍDAS PARA LISTAR LOS TIPOS
DE DOCUMENTOS QUE SE ENCUENTRAN EN CADA OFICINA. .............. 157

TABLA Nº 22: DESCRIPCIÓN DE LAS VARIABLES EXTRAÍDAS PARA LISTAR LOS TIPOS
DE DOCUMENTOS QUE SE PRESENTAN MÁS. ................................... 158

www.nitropdf.com

TABLA Nº 23: DESCRIPCIÓN DE LAS VARIABLES EXTRAÍDAS PARA LISTAR LOS TIPOS
DE SOLICITUDES QUE SE PRESENTAN MÁS. ..................................... 159

TABLA Nº 24: DESCRIPCIÓN DE LAS VARIABLES EXTRAÍDAS PARA LISTAR EL
ESTADO DEL DOCUMENTO EN CADA OFICINA................................... 160
TABLA Nº 25: PUNTUACIÓN DE FACTORES DE CALIDAD SEGÚN USUARIOS DE LA
SOLUCIÓN. .................................................................................. 207

TABLA Nº 26: PUNTUACIÓN DE MÉTRICAS DE CALIDAD DE MCCALL SEGÚN
USUARIOS DE LA SOLUCIÓN........................................................... 207

TABLA Nº 27: RESUMEN DE LOS FACTORES MCCALL. ...........................................209

www.nitropdf.com

ÍNDICE DE FIGURAS
FIGURA Nº 1: MODELO OPTIMIZADOR. ...................................................................42
FIGURA Nº 2: MODELO NO OPTIMIZADOR. .............................................................47
FIGURA Nº 3: LA COMPUTACIÓN EN LA NUBE. .........................................................50
FIGURA Nº 4: CICLO DE PUBLICIDAD PARA LA COMPUTACIÓN EN LA NUBE 2014. ........55
FIGURA Nº 5: CICLO DE GESTIÓN DE UNA ARQUITECTURA BIG DATA. .......................61
FIGURA Nº 6: COMPONENTES DE UNA ARQUITECTURA BIG DATA..............................62
FIGURA Nº 7: CÓDIGO HTML, EJEMPLO DE DATOS SEMI-ESTRUCTURADOS. ..............66
FIGURA Nº 8: MÓDULOS HADOOP. ........................................................................69
FIGURA Nº 9: ARQUITECTURA DEL HDFS. .............................................................73
FIGURA Nº 10: ESQUEMA ARQUITECTURA MAPREDUCE. .........................................78
FIGURA Nº 11: ARQUITECTURA DEL HDFS FEDERATION. ........................................88
FIGURA Nº 12: MAPREDUCE CON YARN. ..............................................................92
FIGURA Nº 13: LAS DIFERENTES ENTIDADES DE UN SISTEMA DE MINERÍA. .................98
FIGURA Nº 14: EJEMPLO DE LA ARQUITECTURA DE UN SISTEMA DE MINERÍA DE
DATOS. .......................................................................................

101

FIGURA Nº 15: METODOLOGÍA CRISP-DM. .........................................................112
FIGURA Nº 16: FACTORES DE CALIDAD DE MCCALL. .............................................116
FIGURA Nº 17: PARTE DEL MODELO DE ANÁLISIS DEL SOFTWARE DE HOGAR
SEGURO. .................................................................................. 120
FIGURA Nº 18: ORGANIGRAMA DE LA UGEL AZÁNGARO. ......................................135
FIGURA Nº 19: ARQUITECTURA DE APACHE SQOOP. .............................................149
FIGURA Nº 20: CRONOGRAMA DEL PROYECTO DE INVESTIGACIÓN. .........................150
FIGURA Nº 21: ESTRUCTURA DE LAS TABLAS (EXPEDIENTES, SEGUIMIENTOS,
PARAMETROS, OFICINAS Y ÁREAS)............................................... 153

FIGURA Nº 22: CONSULTA SQL UTILIZADA PARA EXTRAER LA LISTA DE OFICINAS. ...155
FIGURA Nº 23: CONSULTA SQL UTILIZADA PARA EXTRAER LA LISTA DE TIPOS DE
DOCUMENTOS. .......................................................................... 156

FIGURA Nº 24: CONSULTA SQL UTILIZADA PARA EXTRAER LA LISTA DE TIPOS DE
DOCUMENTOS QUE SE ENCUENTRAN EN CADA OFICINA. ................ 156

FIGURA Nº 25: CONSULTA SQL UTILIZADA PARA EXTRAER LA LISTA DE LOS
DOCUMENTOS QUE SE PRESENTAN MÁS. ..................................... 157

www.nitropdf.com

FIGURA Nº 26: CONSULTA SQL UTILIZADA PARA EXTRAER LA LISTA DE TIPOS DE
SOLICITUDES QUE SE PRESENTAN MÁS.

....................................... 158

FIGURA Nº 27: CONSULTA SQL UTILIZADA PARA EXTRAER LA LISTA DE ESTADO
DEL DOCUMENTO EN CADA OFICINA. ............................................ 159

FIGURA Nº 28: SALIDA DE APACHE SQOOP AL TERMINAR LA IMPORTACIÓN DE
OFICINAS. ................................................................................. 161

FIGURA Nº 29: SALIDA DE R STUDIO PARA MOSTRAR LA LISTA DE OFICINAS............161
FIGURA Nº 30: SALIDA DE APACHE SQOOP AL TERMINAR LA IMPORTACIÓN DE LOS
TIPOS DE DOCUMENTOS. ............................................................ 162

FIGURA Nº 31: SALIDA DE R STUDIO PARA MOSTRAR LA LISTA DE TIPOS DE
DOCUMENTOS. .......................................................................... 162

FIGURA Nº 32: SALIDA DE APACHE SQOOP AL TERMINAR LA IMPORTACIÓN DE LOS
TIPOS DE DOCUMENTOS QUE SE ENCUENTRAN EN CADA OFICINA. ... 163

FIGURA Nº 33: SALIDA DE APACHE SQOOP AL TERMINAR LA IMPORTACIÓN DE LOS
DOCUMENTOS QUE SE PRESENTAN MÁS. ...................................... 164

FIGURA Nº 34: SALIDA DE APACHE SQOOP AL TERMINAR LA IMPORTACIÓN DE LOS
TIPOS DE SOLICITUDES QUE SE PRESENTAN MÁS.

......................... 164

FIGURA Nº 35: SALIDA DE APACHE SQOOP AL TERMINAR LA IMPORTACIÓN DE
ESTADO DEL DOCUMENTO EN CADA OFICINA. ................................ 165

FIGURA Nº 36: SALIDA DE R STUDIO PARA MOSTRAR EL RESUMEN DE LOS DATOS
LEÍDOS DE LA LISTA DE ESTADO DEL DOCUMENTO EN CADA
OFICINA. ................................................................................... 165

FIGURA Nº 37: ECOSISTEMA DE APACHE HADOOP YARN.......................................169
FIGURA Nº 38: ARQUITECTURA DE DATOS PARA EL MODELO DE MAPREDUCE. ........170
FIGURA Nº 39: LOS 20 PRIMEROS TIPOS DE SOLICITUDES CON RSTUDIO. ...............172
FIGURA Nº 40: MODELO DE REGRESIÓN LOGÍSTICA OBTENIDO CON R. ....................173
FIGURA Nº 41: MATRIZ DE CONFUSIÓN DEL MODELO DE REGRESIÓN.......................174
FIGURA Nº 42: TASA DE ACIERTO PARA LOS DOCUMENTOS FINALIZADOS OBTENIDO
CON R. ..................................................................................... 175

FIGURA Nº 43: CANTIDAD DE DOCUMENTOS FINALIZADOS ANTES Y DESPUÉS DE LA
PREDICCIÓN. ............................................................................. 175

FIGURA Nº 44: TIPOS DE DOCUMENTOS QUE SE ENCUENTRAN EN CADA OFICINA. ....186
FIGURA Nº 45: DOCUMENTOS QUE SE PRESENTAN MÁS. .......................................187

www.nitropdf.com

FIGURA Nº 46: RESULTADO DEL MODELO MAPREDUCE CON RSTUDIO PARA LOS
TIPOS DE SOLICITUDES. ............................................................. 189
FIGURA Nº 47: ESTADO DEL DOCUMENTO EN CADA OFICINA...................................190
FIGURA Nº 48: FRAGMENTO DEL CÓDIGO PARA EL MODELO DE REGRESIÓN
LOGÍSTICA. ............................................................................... 191
FIGURA Nº 49: LA MULTICOLINEALIDAD DE LAS VARIABLES.....................................193
FIGURA Nº 50: TASA DE ACIERTO GENERAL Y DE LOS DOCUMENTOS FINALIZADOS. ..194
FIGURA Nº 51: RESULTADO DEL MODELO DE REGRESIÓN LOGÍSTICA. .....................195
FIGURA Nº 52: CLASE MAPPER DEL PROGRAMA MAPREDUCE. ..............................197
FIGURA Nº 53: CLASE REDUCER DEL PROGRAMA MAPREDUCE. ............................199
FIGURA Nº 54: CLASE DRIVER DEL PROGRAMA MAPREDUCE.................................201
FIGURA Nº 55: PROCESAMIENTO DE LOS DATOS PARA LA BÚSQUEDA DE PATRONES
MÁS COMUNES. ......................................................................... 202

FIGURA Nº 56: RESULTADO EN EL CLÚSTER DE HADOOP, DEL PROCESAMIENTO DE
LOS DATOS PARA LA BÚSQUEDA DE PATRONES MÁS COMUNES. ...... 203

FIGURA Nº 57: ARQUITECTURA DEL CLÚSTER HADOOP. ........................................204
FIGURA Nº 58: PANTALLA DE INICIO DE LA HERRAMIENTA RSTUDIO. ......................205
FIGURA Nº 59: UTILIZACIÓN DE LA HERRAMIENTA RSTUDIO PARA LA VISUALIZACIÓN
DE LOS TIPOS DE DOCUMENTOS. ................................................. 206

FIGURA Nº 60: DISTRIBUCIÓN T...........................................................................213
FIGURA Nº 61: ADMINISTRADOR DE HYPER-V CON LAS TRES MÁQUINAS VIRTUALES.222
FIGURA Nº 62: ESQUEMA DE LA ARQUITECTURA EMPLEADA EN EL PROYECTO. ........222
FIGURA Nº 63: TRABAJANDO REMOTAMENTE EN UN NODO A TRAVÉS DE PUTTY. ....223
FIGURA Nº 64: VERIFICACIÓN DE LOS SERVICIOS EN EL NODO “HADOOP2NN”. .........228
FIGURA Nº 65: VERIFICACIÓN DE LOS SERVICIOS EN EL NODO “HADOOP2DN1”. .......229
FIGURA Nº 66: VERIFICACIÓN DE LOS SERVICIOS EN EL NODO “HADOOP2DN2”. .......229
FIGURA Nº 67: LA CONSOLA DEL NAME NODE ESCUCHA EL PUERTO 50070
“HTTP://HADOOP2NN:50070”. ..................................................... 229
FIGURA Nº 68: LA GESTIÓN DE RECURSOS DE YARN ESCUCHA EL PUERTO 8088
“HTTP://HADOOP2NN:8088/CLUSTER/NODES”............................... 230
FIGURA Nº 69: LA CONSOLA DE HISTORIAL DE TRABAJOS ESCUCHA EL PUERTO
19888 “HTTP://HADOOP2NN:19888/JOBHISTORY/APP”. ................ 230

www.nitropdf.com

FIGURA Nº 70: CONSOLA DE SQOOP CLIENT DONDE MUESTRA LOS CONECTORES Y
LA VERSIÓN DEL SERVIDOR SQOOP. ............................................ 233

FIGURA Nº 71: SERVIDOR DE SQOOP ESCUCHA EL PUERTO 12000
“HTTP://HADOOP2NN:12000/”. .................................................... 233
FIGURA Nº 72: CONSOLA DE SQOOP CLIENTE, DONDE SE MUESTRA LOS LINK Y
TODOS LOS TRABAJOS DE IMPORTACIÓN DE DATOS DE MYSQL
HACIA HDFS DEL CLÚSTER DE HADOOP. ..................................... 234

FIGURA Nº 73: INSTALACIÓN DE PAQUETES EN RSTUDIO. ......................................235

www.nitropdf.com

RESUMEN
El presente trabajo de investigación titulado “Análisis de Datos con
Herramientas de Big Data para la Toma de Decisiones en la UGEL de
Azángaro”, tuvo como objetivo general: analizar los datos con herramientas de
Big Data del sistema de gestión de los documentos administrativos para el
apoyo en la toma de decisiones en la UGEL de Azángaro. Se utilizó la
metodología CRISP-DM para el desarrollo de la solución, desde la
comprensión del negocio hasta la implementación, permitiendo que los
usuarios finales (UGEL) analicen y exploren la información de manera sencilla
e intuitiva. La solución contiene el comportamiento de los tipos documentos que
se encuentran en cada oficina, documentos que se presentan más, tipos de
solicitudes que se presentan con más frecuencia y el estado del documento en
cada oficina, la información es presentada en función al tiempo. Para la
implementación de la solución se utilizó la herramienta Hadoop como sistema
de ficheros distribuidos, Sqoop para la importación de datos, MapReduce como
modelo de procesamiento de datos, Regresión logística como modelo
predictivo y R para la visualización de los resultados. La investigación siguió el
diseño de investigación post test, donde se demostró estadísticamente la
hipótesis planteada y como resultado final se obtuvo que la solución de análisis
de datos con herramientas de Big Data apoya significativamente a la toma de
decisiones en la UGEL de Azángaro 2014.
Palabras claves: Apoyo a las decisiones, análisis de datos, Big Data, Hadoop,
sistema distribuidos, computación paralela, minería de datos.

14

www.nitropdf.com

ABSTRACT
This paper titled “Data Analytics with Tools Big Data for the Decision Making in
UGEL’s Azángaro”, had as its overall objective: to analyze the data with tools
for Big Data management system for administrative documents support
decision-making in the UGEL’s Azángaro. The CRISP-DM methodology for
developing the solution was used, from understanding the business to
implementation, allowing end users (UGEL) analyze and explore the information
simple and intuitive way. The solution contains the behavior of document types
found in every office, documents are presented, types of requests that occur
more frequently and the status of each office document, information is
presented according to time. The Hadoop tool was used as distributed file
system, Sqoop to import data modeling MapReduce data processing, logistic
regression as predictive model and R for displaying the results to implement the
solution. The research followed the design of post test, where he showed
statistically the hypothesis and the final result was obtained that the solution
data analysis tools significantly Big Data support decision making in research
UGEL’s Azángaro 2014.
Key words: Decision support, Data analytics, Big Data, Hadoop, Distributed
systems, Parallel Computing, Data Mining.

15

www.nitropdf.com

INTRODUCCIÓN
La ciencia y la tecnología avanzan a pasos acelerados, Big Data es una de las
tecnologías de la información y la comunicación del futuro. No solamente
porqué se adapta mejor a los cambios y a la evolución que está sufriendo la
sociedad (tanto en tecnologías como en necesidades) sino también por la gran
aceptación que tiene entre las empresas. Esta aceptación es una realidad a
dos niveles: a nivel de desarrollo, donde cada vez más compañías desarrollan
nuevas tecnologías y apuestan más por las existentes; y a nivel corporativo,
donde las empresas empiezan a ver con más interés las nuevas posibilidades
que aportan las soluciones Big Data. En términos coloquiales podría decirse
que Big Data consiste en el tratamiento masivo de grandes cantidades de datos
informáticos los cuales son, muchas veces, dispares. La finalidad del
tratamiento de todos estos datos es el análisis de los mismos para ayudar a la
empresa y/o organización en la toma de decisiones y el entendimiento de los
mismos. Los datos que se compone una solución Big Data, son normalmente
imposible de tratar en una base de datos relacional, debido a que sería muy
costoso y llevaría mucho tiempo.
Este trabajo presenta una solución de análisis de datos con herramientas de
Big Data para el apoyo en la toma de decisiones en la UGEL de Azángaro, en
la cual el tratamiento y análisis de la información almacenada en la base de
datos del sistema de gestión de los documentos administrativos (SGDA),
aportó información relevante y ayudó a las decisiones de la institución.
La investigación se ha elaborado de acuerdo a la organización de información
que se detalla a continuación:
16

www.nitropdf.com

En el capítulo I “PLANTEAMIENTO DEL PROBLEMA DE INVESTIGACIÓN”,
se identifica el problema a investigar, además se plantea la justificación y los
objetivos.
En el capítulo II “MARCO TEÓRICO”, se presentan los antecedentes
investigados, hipótesis y el señalamiento de las variables de la hipótesis.
En el capítulo III “METODOLOGÍA”, se determina la metodología de
investigación a utilizar, el tipo de investigación, la población y muestra.
En el capítulo IV “ANÁLISIS E INTERPRETACIÓN DE LOS RESULTADOS”, se
procede al análisis e interpretación de los resultados.
Por último se presenta las conclusiones a las que se ha llegado y se detallan
algunas sugerencias.

17

www.nitropdf.com

CAPÍTULO I
PLANTEAMIENTO DEL PROBLEMA DE
INVESTIGACIÓN

18

www.nitropdf.com

1.1

Descripción del Problema
La Unidad de Gestión Educativa Local de Azángaro, es una institución

pública que tiene como función principal dirigir, coordinar y supervisar la
aplicación de la política educativa y normatividad en materia de educación,
cultura, deporte y recreación, en los centros y programas educativos,
adecuándola a las necesidades locales en concordancia con la realidad
nacional, así como administrar el personal docente y administrativo, los
recursos materiales, financieros y patrimoniales y ejecutar las acciones
administrativas que se genera en su jurisdicción de acuerdo a ley y las normas
de los respectivos áreas y sistemas. Para cada una de estas acciones
administrativas, el área de administración es la encargada de mejorar
permanentemente los procesos técnicos de la gestión administrativa de los
documentos simplificando su ejecución.
Para la simplificación de la gestión administrativa de los documentos
presentados en mesa de partes, la cual se rige de acuerdo a los
procedimientos estipulados en el TUPA (Texto Único de Procedimientos
Administrativos) de la institución; se necesita los datos la persona que remite,
el origen del documento, la descripción, número de folios, la oficina de destino,
recepcionar los documentos enviados por otras áreas y/o oficinas dentro de la
institución y generar un cargo de recepción, elaborar búsquedas para saber la
situación actual de cada uno de los documentos presentados a la institución.
Toda la información referida a la atención de los documentos presentados
en mesa de partes de la institución es ingresada en el sistema de gestión de
los documentos administrativos (SGDA), del cual se registra abundante
19

www.nitropdf.com

información que genera escasez en su atención o enfoque, perdiendo la
posibilidad de descubrir patrones que puedan ser aprovechados en beneficio
de la institución mediante la analítica predictiva. La velocidad, variedad y
volumen con la se genera la información puede generar un reto abrumador
para la institución que desea enriquecer su toma de decisiones y mejorar sus
procesos.
En un sentido amplio, se define a los sistemas de apoyo a las decisiones
como un conjunto de programas y herramientas que permiten obtener
oportunamente la información requerida durante el proceso de la toma de
decisiones, en un ambiente de incertidumbre. En la mayoría de los casos, lo
que constituye el detonante de una decisión es el tiempo límite o máximo en el
que se debe tomar una decisión. Así, en cada decisión que se toma, siempre
se podrá pensar en qué tipo de información tiene valor; sin embargo al llegar al
límite de tiempo, se deberá llegar a una decisión. Esto implica necesariamente
que al verdadero objetivo de un sistema de apoyo a las decisiones sea
proporcionar la mayor cantidad de información relevante en el menor tiempo
posible, con el fin de decidir lo más adecuado.
Para esto, surge la necesidad de analizar los datos con herramientas de
Big Data para la toma de decisiones adecuadamente y en forma oportuna en
las diferentes áreas que la institución tiene para la atención de los documentos
registrados por el SGDA.
Por lo descrito anteriormente, se formuló el siguiente problema:

20

www.nitropdf.com

1.1.1 Problema general.
¿Cuáles son los efectos del análisis de datos con herramientas de Big
Data del sistema de gestión de los documentos administrativos para el apoyo
en la toma de decisiones en la Unidad de Gestión Educativa Local de
Azángaro?
1.1.2 Problemas específicos.
-

¿En qué medida el análisis de datos identifica la correlación de los datos
para la toma de decisiones?

-

¿En qué medida el análisis de datos almacena datos históricos para la
toma de decisiones?

-

¿En qué medida el análisis de datos procesa los datos para la toma de
decisiones?

-

¿En qué medida el análisis de datos visualiza los datos para la toma de
decisiones?

21

www.nitropdf.com

1.2

Justificación del Problema
La importancia de la solución está dada respecto a los siguientes

aspectos:
-

Es importante que en las diferentes áreas de la Unidad de Gestión
Educativa Local de Azángaro cuenten con una herramienta tecnológica
que permita el análisis de datos históricos de manera que se pueda
servir de información con valor agregado, y así signifique una ventaja
competitiva apoyando a la toma de decisiones.

-

Esta solución propuesta es de gran utilidad, ya que tiene la ventaja de
realizar un análisis predictivo a partir de los datos históricos que han sido
almacenados en una base de datos relacional del sistema de gestión de
los documentos.

-

Las diferentes áreas de la Unidad de Gestión Educativa Local de
Azángaro realiza muchas actividades administrativas con respecto a los
documentos remitidos en mesa de partes correspondientes a cada
institución pública y/o privada, persona natural y/o jurídica, por esta
razón los beneficios de las tecnologías y el análisis predictivo de los
datos históricos a través de la utilización de distintos algoritmos y
técnicas, permiten generar conocimiento y optimizar las actividades.

-

En los últimos años se han desarrollado numerosas herramientas para
apoyar óptimamente las actividades de los negocios, tecnologías que
permiten obtener conocimiento, y las diferentes áreas de la Unidad de
Gestión Educativa Local de Azángaro no ausentes a ello por lo que
necesita de un análisis predictivo.
22

www.nitropdf.com

1.3

Objetivos de la Investigación

1.3.1 Objetivo General
Analizar los datos con herramientas de Big Data del sistema de gestión de
los documentos administrativos para el apoyo en la toma de decisiones en la
Unidad de Gestión Educativa Local de Azángaro.
1.3.2 Objetivos Específicos
Los objetivos específicos de la investigación fueron:
-

Probar que el análisis de datos identifica la correlación de los datos para
la toma de decisiones.

-

Determinar que el análisis de datos almacena datos históricos para la
toma de decisiones.

-

Probar que el análisis de datos procesa los datos para la toma de
decisiones.

-

Determinar que el análisis de datos visualiza los datos para la toma de
decisiones.

23

www.nitropdf.com

CAPÍTULO II
MARCO TEÓRICO

24

www.nitropdf.com

2.1

Antecedente de la Investigación

2.1.1 Antecedentes Nacionales
Se tiene como antecedentes nacionales los siguientes trabajos de
investigación:
La investigación denominada “Guía Metodológica para Obtener Patrones
de Accidentabilidad Laboral usando Data Mining” presentado en la Universidad
de Piura por Cevallos (2013), ha concluido lo siguiente:
La metodología expuesta en el presente documento no pretende
implantar un método único e infalible para determinar patrones de
accidentabilidad. El espectro de investigación y las posibilidades que ofrece el
data mining son muy amplias como para pretender establecer un estándar
único. No obstante, recomienda una serie de pasos a seguir para obtener un
resultado específico que es hallar patrones de accidentabilidad. Ya se ha
mencionado que las empresas tienen la posibilidad de usar la estadística
descriptiva tradicional con la salvedad que la posibilidad de obtener
conocimiento es reducida. El data mining ofrece herramientas más robustas
para este fin. Por otro lado, es preciso reconocer que actualmente el Perú vive
un contexto nuevo en relación a la gestión de la seguridad y salud de sus
trabajadores. La publicación de la legislación en estas materias ha establecido
un nuevo orden de trabajo para las empresas en las que esta disciplina ya no
será considerada como “algo más” sino que pasa a ser protagonista con una
concepción más interesante: ya no se trata de un gasto, sino de una inversión.
El conocimiento que se pueda obtener para entender la dinámica de la

25

www.nitropdf.com

accidentabilidad de las empresas resulta crucial en la implementación de
medidas claras. Este trabajo debe ser tomado como una guía referencial ya
que en cierta medida es abstracto respecto a los caminos que debe seguir un
analista, es imposible determinar lo que se debe hacer en cada etapa del
proceso respecto al análisis que cualquiera decida realizar, aunque sí se
pueden dar recomendaciones y pautas a seguir, siempre apoyados en la
metodología KDD. (p. 50)
La investigación denominada “Implantación de un Sistema de Ventas que
emplea una herramienta de Data Mining” presentado en la Pontificia
Universidad Católica del Perú por Berrospi (2012), ha concluido lo siguiente:
La herramienta OpenERP de código abierto es de fácil uso; sin embargo
esto no quiere decir que una empresa con mayor número de transacciones
diarias la pueda usar normalmente, ya que la versión gratuita posee algunas
desventajas que pueden ocasionar problemas a futuro en la empresa. La
implantación de un ERP no incluye solamente la instalación del software en la
empresa, sino que también conlleva posibles problemas por resistencia de los
usuarios al cambio, capacitación de usuarios, comprensión de las tablas de la
base de datos, etc. El producto final permite que la empresa beneficiada pueda
analizar y comprender porque sus clientes se comportan en sus ventas de
formas distintas. El algoritmo usado para el proceso algorítmico de Data Mining
es uno de los más robustos; sin embargo, se pudo haber usado otros y
obtenidos diferentes resultados; es decir, cada algoritmo es usado en un
escenario distinto, y la forma para escoger el adecuado es en muchas
ocasiones la experiencia de la persona encargada del modelado. (p. 93)
26

www.nitropdf.com

La investigación denominada “Desarrollo de un Data Mart en el Área de
Administración y Finanzas de la Municipalidad Distrital de los Baños del Inca”
presentado en la Universidad Privada del Norte por Ocas (2012), ha concluido
lo siguiente:
Se logró recolectar información de las partes interesadas del área gerencial
y de sistemas con el fin de poder contar con los requerimientos claros. Se logró
analizar y diseñar el modelo multidimensional para el soporte de toma de
decisiones, basado en un esquema en estrella con las dimensiones de
contribuyente, tiempo, predios, fuente financiamiento, con sus respectivas
jerarquías, hechos, y atributos., que incluían las jerarquías requeridas por los
usuarios. El uso de una interfaz de inteligencia de negocios con reportes
permite un manejo intuitivo y sencillo a los usuarios finales para generar sus
propios reportes y análisis, acorde a las necesidades del negocio en
comparación del uso de hojas de cálculo o de los sistemas transaccionales
utilizados. La implementación del data mart en la institución contribuye a la
mejor administración y gestión de la información, disminuyendo los tiempos del
desarrollo de reportes y tiempos en la toma de decisiones que son necesarios
para decisiones estratégicas basadas en información de calidad. (p. 90)

27

www.nitropdf.com

2.1.2 Antecedentes Internacionales
Se tiene como antecedentes internacionales los siguientes trabajos de
investigación:
La investigación denominada “Estudio Análisis y Evaluación del
Framework Hadoop” presentado en la Universidad Pontificia Comillas por
Rubio (2014), ha concluido lo siguiente:
Uno de los objetivos principales del proyecto era conseguir una idea
personal sobre Hadoop y ver si la revolución en el mundo Big Data es Hadoop
o por el contrario simplemente es una moda sin mucha utilidad. En cuanto al
uso de Hadoop, está generalizado y de verdad se puede concluir después de
leer tanto sobre el tema que Hadoop es uno de los frameworks más usados en
el mundo Big Data ya que muchas organizaciones importantes como Yahoo! o
Google hacen uso de la idea Hadoop. Bien es cierto que muchas veces no se
usa Hadoop en su integridad si no que, al ser un software libre, se modifica la
lógica Hadoop para acercar la distribución a los intereses de la compañía.
Además muchas organizaciones hacen uso de distribuciones Enterprise a más
alto nivel que facilitan el uso y la implantación de Hadoop. Después de la
realización del proyecto, puede verse la dificultad de manejar Hadoop sin la
ayuda de ninguna herramienta ya que si se quiere realizar un código
MapReduce con una lógica complicada y con unos datos de entrada
complicados, las diferentes clases, paquetes, comunicación entre nodos
distribuidos y acciones a controlar son tantas que su uso se vuelve mucho más
complicado, por esta razón es por la que las compañías hacen uso de las
herramientas. Otra de las conclusiones obtenidas es que es verdad que
28

www.nitropdf.com

Hadoop está muy en auge pero al ser un sistema relativamente nuevo toda la
documentación que hay, que es mucha, es muy dispar y pueden encontrarse
documentos que difieren unos de otros. Por todo esto, se concluye que Hadoop
es uno de los sistemas de tratamiento de datos distribuido más en auge pero
que su uso a bajo nivel es complicado por lo que la mayoría de las
organizaciones y compañías hacen uso de herramientas a más alto nivel que
permiten la implantación de Hadoop de una manera más fácil y sencilla. Se
concluye también que Hadoop puede ser un sistema de tratamiento de datos
distribuidos muy potente gracias al paralelismo y la distribución de las
ejecuciones pero siempre hay que adaptarlo al problema en cuestión que se
quiera tratar, no es un paquete cerrado que sin modificarlo pueda ser usado por
alguna compañía, por esta razón, la implantación de Hadoop en una
organización siempre conlleva unos expertos que lo adapten al caso en
cuestión y con ello unos costes económicos. (p.76)
La investigación denominada “Análisis y Comparación entre el Motor de
Bases de Datos Orientado a Columnas Infobright y el Framework de
Aplicaciones Distribuidas Hadoop en Escenarios de Uso de Bases de Datos
Analíticas” presentado en la Universidad de Chile por Silva (2014), ha concluido
lo siguiente:
La solución construida durante este trabajo de memoria evidencia que la
combinación de distintas herramientas en un ambiente con tecnología Hadoop
puede ofrecer un buen desempeño y mejor utilización de máquinas que la
solución actual, en los casos de uso abordados. Si se observan estrictamente
los resultados numéricos se puede concluir que el motor Impala es el que
29

www.nitropdf.com

ofrece el mejor desempeño en casi todos los escenarios a excepción de las
consultas de tipo agregación donde es superado por Infobright. La diferencia
más clara se ve en las consultas tipo OLAP, en las cuales el tamaño de los
datos es menor y los accesos son de sólo lectura, en esos casos presenta
tiempos hasta 45 veces menores a Infobright. Por otra parte, se puede ver que
realizar consultas con Hive (trabajos de tipo Map Reduce) requiere un
overhead que no se superó en las pruebas. Por esta razón este sistema no
está hecho para realizar trabajos en tiempo real. Sin embargo, muestra una
mejora en los tiempos con las consultas tipo ETL, aunque sin superar a las
otras herramientas analizadas. En lo que respecta al uso de los recursos, los
requerimientos de memoria de Impala son superiores a los de Hive. Estos
dependen del tipo de consulta, y no existe una regla general para determinar la
correlación, ya que Impala no carga tablas completas en memoria, por lo que la
cantidad de memoria disponible no limita directamente el tamaño de las tablas
que puede manejar. Sin embargo, Impala genera una menor carga de CPU del
cluster que Hive. Con respecto a la compresión Infobright presenta el mejor
ratio, sin embargo, la compresión columnar de Parquet mostró una buena
relación entre el nivel de compresión y tiempos de respuesta, superando en
rendimiento a las pruebas sobre archivos sin comprimir. En los casos de Hive y
de Impala es importante mencionar que las consultas son optimizables, esto
permite aprovechar al máximo el potencial del hardware. Las optimizaciones se
pueden realizar de variadas formas, tales como el orden en que se hacen los
joins, creación de particiones, elegir el formato más adecuado, entre otros. Se
debe considerar que Hive e Impala no son excluyentes sino que
completamente compatibles, ya que ambos trabajan sobre HDFS y comparten
30

www.nitropdf.com

la metadata, de modo que no se debe comenzar de cero al momento de
requerir una solución Impala. Todo el hardware que se utiliza para MapReduce
puede utilizarse para Impala. Se puede concluir que Impala aplica mejor en
consultas ligeras, donde prime la velocidad de los resultados sobre la robustez.
A su vez Hive se comporta mejor en trabajos pesados de tipo ETL, donde la
robustez sea lo primordial, y no el tiempo de ejecución. Esto porque es la única
solución que presenta tolerancia a fallos, evitando la necesidad de relanzar los
trabajos al fallar un nodo. En lo que respecta a ambas soluciones (Infobright y
Hadoop), Hadoop tiene la ventaja de ser escalable, ya que basta con agregar
un nodo para aumentar la capacidad de almacenamiento y computación del
cluster. Esto implica ventajas en costos de hardware igualmente, ya que el
software es opensource y el escalamiento horizontal permite construir un
cluster poderoso sin tener una máquina poderosa. Sin embargo, debe tenerse
en cuenta el obstáculo que presenta la empinada curva de aprendizaje de
Hadoop y el costo que implica. A pesar de los beneficios, los clústers Hadoop
no son útiles para cualquier organización. Aunque se requiera análisis intensivo
de datos, si se cuenta con pocos datos pueden no verse beneficiados. Otro
punto importante a considerar es que Hadoop está basado en la idea de que
los datos puede dividirse y ser analizados por procesos paralelos ejecutándose
en distintos nodos. Si el análisis no se adapta al modelo de procesamiento
paralelo, la herramienta no es útil. (p.79)
La investigación denominada “Large-Scale Distributed Data Management
and Processing Using R, Hadoop and MapReduce” presentado en la
Universidad de Oulu por Lampi (2014), han concluido lo siguiente:

31

www.nitropdf.com

Todo el trabajo fue muy educativo y espero dar instrucciones e ideas en
cuanto a qué tipo de soluciones y sistemas podría ser utilizado por cada una o
ambas en Universidades y Global RF Spectrum Opportunity Assessment. Para
evitar los problemas que se produjeron durante la ejecución, un servicio PaaS
escalable basada en la nube, podría ser un buen punto de partida. Para los
problemas en la red (y los retrasos que fijan ellos) se hicieron la configuración
del clúster Hadoop y RHadoop como software algo inestable debido al carácter
aleatorio de los errores. Por el hecho de que el autor no residía en la
universidad, mientras que la segunda mitad del trabajo se hizo definitivamente
más complicado para terminar el trabajo. Horas y días dedicados configurando
el clúster, de los cuales cerca de un tercio se gastó leyendo foros, dejaron bien
claro que un clúster Hadoop podría ser trivial para configurar, pero conseguir
estable y funcionando sin problemas implica un conocimiento profundo de las
diferentes partes de la sistema, permisos de root, y los derechos
administrativos en el clúster. (p.64)

32

www.nitropdf.com

2.2

Sustento Teórico

2.2.1 Sistemas de soporte a la toma de decisiones (DSS).
La determinante más importante para la toma de decisiones es: “El tiempo
límite o máximo en el que se debe tomar la decisión”. Por lo tanto el objetivo de
un sistema de apoyo a las decisiones es proporcionar la mayor cantidad de
Información relevante en el menor tiempo posible, con el fin de decidir lo más
adecuado. (Power, 2002)
2.2.1.1 Definición.
El autor Power menciona en su libro que: “Un sistema de apoyo a las
decisiones es un conjunto de programas que nos ayuda a obtener información
requerida en el proceso de la toma de decisión”. (Power, 2002)
2.2.1.2 Tipos de sistemas de apoyo a las decisiones.
a. Sistemas de Soporte a la Toma de Decisiones ( DSS = DECISION
SUPPORT SYSTEMS)
b. Sistemas de Información para Ejecutivos ( EIS = EJECUTIVE
INFORMATION SYSTEMS)
c. Sistemas para la Toma de Decisiones en Grupo ( GDSS = GROUP
DECISION SUPPORT SYSTEMS)
d. Sistemas Expertos de Soporte a la Toma de Decisiones ( EDSS =
EXPERT DECISION SUPPORT SYSTEMS)
2.2.1.3 Características.


Interactividad.- Interactuar en tiempo real con el tomador de decisiones.
33

www.nitropdf.com



Tipo de Decisiones.- Apoya el proceso de toma de decisiones
estructuradas y no estructuradas.



Frecuencia de Uso.- Utilización por parte de la administración media y
alta.



Variedad de Usuarios.- Se usa en diferentes áreas, como ventas,
producción, almacén, recursos humanos, etc.



Flexibilidad.- Que funcione a distintos estilos administrativos.



Desarrollo.- El usuario puede realizar sus propios modelos.



Interacción Ambiental.- Interactuar con información externa.



Comunicación Interorganizacional.- Comunicación a todos los niveles
de la organización.



Acceso a Bases de Datos.- Tener acceso de información a las bases
de la organización.



Simplicidad.- Que sea fácil el manejo para el Usuario final.

2.2.2 Toma de decisiones.
La toma de decisiones es algo natural en el ser humano. Siempre se
están tomando decisiones y muchas veces ni nos percatamos de ello. Los
autores Kahneman & Tversky (2000) mencionaron que “tomar decisiones es
como hablar en prosa, la gente lo hace en forma permanente, de manera
consciente o inconsciente”. Utiliza el concepto de modelo como alternativa para
visualizar la realidad y hace énfasis en la necesidad de que los modelos

34

www.nitropdf.com

construidos reflejen esta realidad, si bien en forma sencilla y sintética, de
manera fiel y práctica.
2.2.2.1 Modelos.
Para analizar estas situaciones es necesario simplificar la realidad; una
manera de hacerlo es visualizarla y representarla por medio de un modelo.
Cuando se representa una realidad, generalmente se crea un modelo. Un
modelo es una representación de una realidad, esta representación será tan
detallada, precisa como se desee y como permitan los recursos disponibles.
La realidad es demasiado compleja para representarla con fidelidad en un
modelo. Sin embargo, las nuevas tecnologías tales como la realidad virtual
pueden involucrar más detalles que nunca antes en la historia. Un modelo
desecha ciertos aspectos de la realidad que no se consideran pertinentes para
la comprensión y análisis de un determinado problema. Mientras más detalles
de la realidad se incluyan en el modelo, mayor es la probabilidad de resolver
acertadamente el problema.
Existen diversas clases de modelos: Según el modo de representar la
realidad.
a. Diagramas. Es una forma esquemática de presentar una realidad.
Indican relaciones, flujos, posiciones, etc. Ejemplos de estos modelos
son los organigramas que indican posiciones y eventualmente
relaciones entre los miembros de una organización.
b. Caja negra. Este modelo no explica lo que sucede dentro del modelo.
Se piensa que el proceso generador interno se desconoce y sólo se
35

www.nitropdf.com

sabe qué entra (insumo) y qué sale (producto) por esto se conoce
también con él nombre de insumo-producto.
c. Causa-efecto. Como en el modelo anterior se conoce la forma como se
comporta en la realidad y se puede establecer una relación de causa y
efecto entre variables de entrada y salida, entonces se llama causaefecto. En algunos casos, el comportamiento del modelo se puede
expresar en forma matemática.
d. Modelos físicos. Éstos también tratan de representar la realidad, sin
llegar a ser tan esquemáticos como los anteriores. Dentro de esta
clasificación se pueden incluir las maquetas (tridimensionales) y los
mapas y planos (bidimensionales).
e. Modelos virtuales. Por medio de vistas desde diversas perspectivas se
puede lograr una ilusión de realidad, como si se estuviera frente a la
presencia real de un objeto.
f. Mapas conceptuales. Son métodos de representación visual de una
cierta cantidad de información. Permiten entender con una sola mirada
un contenido complejo de información, logrando que nuestro cerebro
entienda cierta cantidad de información muy compleja, gracias al poder
de nuestra visión. El cerebro humano puede comprender mejor las ideas
cuando éstas se presentan en forma visual.
Según el uso:
a. Normativos. Es una manera teórica de concebir una realidad, muestra
cómo debe operar. Como su nombre lo indica, establece unas reglas de
funcionamiento; este tipo de modelo no siempre coincide con la
36

www.nitropdf.com

realidad. Por ejemplo, en un organigrama se muestra que el rector de
una universidad es la “máxima autoridad”; sin embargo, puede existir
una estructura informal que no aparece en el modelo, por ejemplo, con
el vicerrector como la autoridad real dentro de la organización.
b. Descriptivos. Este tipo de modelo trata de representar la realidad tal
como la percibe un observador. Por ejemplo, un investigador puede
observar el funcionamiento del proceso de decisión de una organización
y posteriormente elaborar un modelo que describa lo visto.
Los autores (Keeny y Raiffa (1976) y Bell, Raiffa y Tversky (1988))
piensan que existen además modelos prescriptivos que indican la manera de
enfocar sistemáticamente un problema y tomar decisiones. Estos autores
permiten pensar que realmente la clasificación de acuerdo con el uso no es
muy clara y que los modelos no son estrictamente normativos ni estrictamente
descriptivos. Por otro lado, French y Xie (1994) anotan que el propósito de
cualquier análisis de modelos es proporcionar una mejor comprensión de la
realidad. Al utilizar modelos descriptivos se pretende entender cómo los demás
toman las decisiones; los modelos normativos pretenden explorar las
implicaciones de ciertas normas o patrones ideales de comportamiento; y por
último, el análisis prescriptivo trata de explorar los juicios, criterios y
preferencias de quienes toman las decisiones al enfrentarse con problemas
reales, y su propósito es hacer que los que deciden entiendan y profundicen
sobre su proceso de decisión; más precisamente, este análisis debe conducir a
un modelo que indique cómo tomar decisiones para mantener la consistencia
entre ellas.

37

www.nitropdf.com

El autor Salinas menciona que: “Como los modelos simplifican la realidad,
parten de supuestos fuertes que no siempre se cumplen. Una de las cualidades
de un buen analista es conocer bien el modelo que escoge, dé tal manera que
pueda verificar si las condiciones de la realidad que pretende estudiar se
cumplen. Por otro lado, un buen modelo debe incluir la cantidad adecuada de
elementos de la realidad que permitan confirmar o predecir su comportamiento.
El modelo debe contemplar todas las variables y elementos de la realidad y sus
interrelaciones, aunque no siempre sea posible incluirlos o medirlos. El
modelador debe tener conciencia de todo lo que hay es medible, en esa
realidad se debe tratar de predecir o establecer las posibles consecuencias que
lo involucran: algunos elementos que la determinan. Después de configurado el
modelo, y sólo entonces, será posible reducir o minimizar el conjunto de
supuestos y condiciones, en el entendido de poder determinar o medir las
consecuencias, de eliminarlos sobre el comportamiento del modelo”. (Salinas
Ortiz, 2013)
Muchas de las fallas que se les atribuyen a modelos conocidos se deben
a que se ha escogido o utilizado incorrectamente el modelo. Esto es, un
modelo explicativo se ha utilizado sin los ajustes apropiados para convertirlo en
un modelo aplicativo.
Algunos modelos parten de condiciones y supuestos que en la realidad no
siempre se cumplen; Algunos de los supuestos son invisibles o implícitos. En
muchos casos se deben a las circunstancias de la época en que se formuló el
modelo, cuando los recursos computacionales eran deficientes o inexistentes.
Pero hoy es inaceptable admitir algunos de esos supuestos implícitos ya que
38

www.nitropdf.com

se cuenta con máquinas de alta velocidad y gran precisión que permiten
incorporar variables adicionales o hacer cálculos más exactos, a precios
razonables.
2.2.2.2 Decisiones.
En la vida de las organizaciones o del individuo siempre se presentan
situaciones por resolver. Las formas de solucionarlas son variadas y por lo
general los recursos disponibles son escasos.
La función de un gerente es tomar decisiones. Se enfrenta a un problema
cuando hay escasez de recursos (restricciones) y varias soluciones. Cuando
hay exceso de recursos o cantidades en la práctica ilimitadas, no hay
dificultades en la elección. Sin embargo siempre, aun en la abundancia, habrá
que escoger un curso de acción. Un problema tiene seis componentes:
1. La persona que lo enfrenta, en general se llamará el que decide. Ésta
puede ser o un individuo o una organización.
2. Las variables controlables por el que decide. Son aquéllas sobre las
cuales puede influir de manera efectiva.
3. Las variables no controlables o del entorno. Son aquéllas sobre las
cuales el que decide no tiene influencia.
4. Las alternativas. En el proceso de análisis de la situación para encontrar
una solución, se encuentran alternativas que “resuelven” el problema.
Estas alternativas de solución son los diferentes cursos de acción que
cumplen las restricciones.

39

www.nitropdf.com

5. Las restricciones. Algunas variables o combinaciones de variables
pueden tener una o más restricciones que deben satisfacerse. No hay
que olvidar que la toma de decisiones no es un ejercicio obvio, ni trivial,
debido precisamente a la escasez de recursos.
6. La decisión. Se trata de escoger una alternativa que sea eficiente y
produzca resultados satisfactorios en relación con lo que el que decide
valora o aprecia. Aquí eficiencia se entiende por una alta relación entre
los resultados obtenidos y los recursos empleados.
Entre estas soluciones o alternativas satisfactorias habrá una que es la
mejor y se llama óptima. Si se busca la mejor, se está optimizando. Se puede
tratar de encontrar una solución o alternativa que produzca resultados
satisfactorios pero no óptimos. Hay que escoger entre alternativas.


Cuando hay por lo menos dos cursos de acción posibles,



Cuando esos cursos de acción tienen por lo menos dos valores
diferentes entre sí y



Cuando los cursos de acción tienen diferente eficiencia y eficacia.
Ahora bien, se debe precisar que no todo lo que implica escogimiento es

un problema, pero todo problema sí exige escogimiento.
2.2.2.3 El proceso de decisión.
Cuando se identifica un problema o hay que tomar una decisión en la
realidad, se hace una abstracción, un modelo, como ya se dijo, y se eliminan
algunos aspectos poco importantes, para hacer el análisis y encontrar una
solución con mayor facilidad. Al resolver un problema, básicamente se está
40

www.nitropdf.com

tomando una decisión. Una decisión no se puede identificar como una instancia
única, es un proceso. El resultado del proceso de decisión es la solución de un
problema; como se puede observar en la Figura Nº 1 y Figura Nº 2, este
proceso consta de cuatro fases las cuales se subdividen en etapas.
(Janakiraman & Sarukesi, 2006)
Estas cuatro fases son:
FASE I: Identificación y definición del problema.
FASE II: Búsqueda de alternativas.
FASE III: Evaluación de alternativas.
FASE IV: Ejecución y control.
No se puede decir que las etapas sean estrictamente secuenciales; en la
práctica se superponen.

41

www.nitropdf.com

Figura Nº 1: Modelo Optimizador.
Fuente: Elaboración propia.

FASE I: Identificación y definición del problema
La primera etapa requiere una observación intensa, es necesario
“meterse” en el problema para entenderlo, identificarlo con precisión y así
poder definirlo. Este conocimiento es necesario para realizar la labor de
síntesis y de abstracción que se ha mencionado arriba. Para enfatizar aún más
la importancia del proceso de identificación del problema, basta recordar un
pensamiento de Albert Einstein: “Si se me concediese sólo una hora para
resolver un problema del que dependiese mi propia vida, yo dedicaría 40
minutos a estudiarlo, 15 minutos a revisarlo y 5 minutos a solucionarlo”. En la
siguiente etapa se identifican los objetivos de la organización o de la entidad en

42

www.nitropdf.com

la cual se debe resolver el problema. Esta etapa es de suma importancia pues
en cierta forma los objetivos pueden descartar algunas alternativas de solución
(véase la Figura Nº 1). Teóricamente, la tercera etapa consiste en la
recolección de información, aunque el proceso de definición del problema ya
produce información. Al recogerla, quien toma las decisiones conocerá sus
recursos y limitaciones, que a su vez pueden influir sobre las alternativas de
solución (véase la Figura Nº 1). En la Fase I se debe definir una medida de
eficiencia en función, obviamente, de los objetivos de la organización en cuyo
seno aparece el problema. La medida de eficiencia es una forma de cuantificar
los objetivos de la organización; por ejemplo: si el objetivo es la ganancia
contable, la medida de eficiencia estará en pesos; si el objetivo es aumentar el
nivel educativo de la población, la medida de eficiencia estará expresada en
número de estudiantes graduados, etc. Sin embargo, existen muchas
situaciones en las cuales los resultados son medibles pero no son
cuantificables por ser aspectos de tipo intangible; además, es necesario hacer
consideraciones de tipo ético y moral muy difíciles de involucrar en una medida
de eficiencia pero que de todos modos es necesario introducir en el análisis.
Esta medida de eficiencia se llama también función objetivo, la cual se trata de
optimizar. La forma de optimizarla dependerá de la estructura del problema;
generalmente se trata de maximizar o minimizar el valor de la función objetivo.
Con el problema identificado y la información recolectada, se procede a
construir el modelo. Tanto en el diseño del modelo de una realidad como en la
identificación de un problema que exige una decisión es necesario validar las
suposiciones que se hacen acerca del mismo. Es muy frecuente encontrar
aplicaciones de “técnicas” o “modelos” (por ejemplo modelos de regresión
43

www.nitropdf.com

lineal) que no corresponden a la realidad de la situación; se piensa que la falla
está en la técnica (por ejemplo la estadística) y no se tiene presente que la falla
radica en quien la escogió y aplicó en forma inadecuada. La Fase I, se puede
establecer una secuencia así:
a. Identificación de la situación actual y de la situación deseada, o sea del
estado actual y del estado final que se desea alcanzar. Esto es la
identificación del problema.
b. Identificación de las restricciones que limitan el problema. Este tipo de
restricciones reduce el número de alternativas por analizar, el cual, en
principio, puede ser infinito. Por ejemplo, si se tiene que pavimentar una
carretera particular y existe una limitación de recursos, se explorarán
aquellas alternativas que no impliquen maquinaria pesada y costosa.
c. Identificación de los objetivos de la organización donde se haya
encontrado el problema. El objetivo de quien decide puede también
limitar el número de las alternativas por analizar. Por ejemplo, si uno de
los objetivos de una situación es la creación de fuentes de trabajo para
la comunidad, probablemente las alternativas que impliquen altos grados
de automatización o mecanización no serán consideradas.
d. Identificación de una medida de eficiencia o función objetivo. Ésta debe
ser de tal índole que pueda ser optimizada en cualquier forma o por lo
menos satisfacer ciertos valores aceptables. Esta medida de eficiencia
no debe ser ambigua para el que decide.
e. Construcción y validación del modelo.
FASE II: Búsqueda de alternativas
44

www.nitropdf.com

Es un proceso que tiene algo de racional, pero también grandes dosis de
creatividad y de azar: puede llegar a ser aleatorio. Recuérdese cómo grandes
descubrimientos e invenciones han sido producto del azar y no de un proceso
de búsqueda metódica. En ningún momento se desea menospreciar la
búsqueda sistemática de soluciones, sino que paralela a ella debe existir el
proceso intuitivo y no descartar las soluciones generadas por él, por
descabelladas que parezcan en un principio.
Dentro de la Fase II se debe contemplar el hecho de que una decisión
generalmente tiene implicaciones hacia el futuro. Aquí realmente se plantea el
problema básico del que decide: tomar decisiones con consecuencias Futuras.
La definición de alternativas debe estar acompañada de la (en lo posible)
clara definición de las Consecuencias de cada una de ellas.

FASE III: Evaluación de alternativas
Esta etapa comprende la valoración de la función objetivo de cada una de
las alternativas para elegir luego la mejor de ellas; esta selección se hace
comparando cada uno de los valores de la función objetivo con los criterios de
escogimiento.
Aquí debe tenerse en cuenta que algunos objetivos son contradictorios, lo
cual significa que no todos se pueden lograr al tiempo. Habrá que hacer
algunas concesiones. Ceder en el logro de alguno a favor de Otro y viceversa.

45

www.nitropdf.com

Así mismo, hay que tener en cuenta los elementos de riesgo e incertidumbre
asociados con todo el problema. Aquí es importante tener en cuenta la actitud
hacia el riesgo.
FASE IV: Ejecución y control
Consiste básicamente en poner en práctica la alternativa elegida y
controlar que en la ejecución de la solución se satisfagan los objetivos. En
realidad deben distinguirse dos clases de modelos para la toma de decisiones:
a) modelo optimizador y b) modelo no optimizador.
El primero (véase la Figura Nº 1) supone que se pueden y se deben
analizar todas las alternativas y de éstas escoger la mejor. El segundo (véase
la Figura Nº 2) supone que se analiza una alternativa y si ésta cumple con los
objetivos propuestos entonces el proceso se detiene y se escoge esa
alternativa. O sea que no necesariamente se selecciona la mejor, sino que se
selecciona una alternativa satisfactoria.

46

www.nitropdf.com

Figura Nº 2: Modelo No Optimizador.
Fuente: Elaboración propia.

En los dos modelos siempre hay que tener presente que el problema por
resolver se encuentra inmerso en una realidad que ella misma define
restricciones y variables exógenas -fuera del control del que decide- que hacen
impredecibles los posibles resultados. Todo ello implica que quien decide debe
reconocer las interacciones con el entorno: económico, social, político, cultural,
etc. Esta percepción del entorno hace realista el análisis y, por ende, el proceso
mismo de decisión. Es necesario hacer énfasis en que la realidad es diferente y
que el esquema ofrecido es simplemente una forma de visualizar el proceso

47

www.nitropdf.com

mental que se sigue en la realidad. De ninguna manera se pretende afirmar
que el proceso mental de toma de decisiones sigue etapas y fases claramente
definidas y en forma secuencial. En la realidad, éstas no tienen una frontera
clara que las separe. Dentro de la racionalidad humana existen mecanismos
que no pueden ser ni medidos ni representados en forma gráfica; existen
variables que no se pueden cuantificar ni representar en forma simbólica. En la
medida en que se analicen más alternativas, las probabilidades de alcanzar la
solución óptima aumentan. Al plantear alternativas de acción se requiere de
información pertinente, la cual está, en todos los casos, relacionada con el
futuro. Esto lleva a tratar el problema de la incertidumbre. Si el análisis se hace
suponiendo certeza absoluta, es porque así se puede manejar más fácilmente;
en ningún caso porque se considere que el mundo es determinístico.
Precisamente, la mayor dificultad de quien decide es enfrentarse a cursos de
acción para los cuales no tiene certeza acerca de los diferentes resultados y es
por ello que debe hacer predicciones y previsiones; esto es, deberá no sólo
hacer un esfuerzo por escudriñar el futuro en cuanto a los posibles valores de
una variable, sino prever o predecir eventos o escenarios futuros. (Janakiraman
& Sarukesi, 2006)

2.2.3 La computación en la nube.
Atendiendo a la definición dada por el NIST (National Institute of Standard
and Technology), la computación en la nube (cloud computing en inglés) es un
modelo tecnológico que permite el acceso ubicuo, adaptado y a baja demanda
en red a un conjunto compartido de recursos de computación configurables
48

www.nitropdf.com

(por ejemplo: redes, servidores, equipos de almacenamiento, aplicaciones y
servicios), que pueden ser rápidamente aprovisionados y liberados con un
esfuerzo de gestión reducido o interacción mínima con el proveedor del
servicio. (Sosinsky, 2011)
Otra definición complementaria es la aportada por el RAD Lab de la
Universidad de Berkeley, desde donde se explica que el cloud computing se
refiere tanto a las aplicaciones entregadas como servicio a través de Internet,
como el hardware y el software de los centros de datos que proporcionan estos
servicios. (Gómez Orts, 2013)
Así como el autor Velte da a entender en su libro, que la idea básica es
que: “toda la información se almacena de forma distribuida en servidores,
siendo accesible en cualquier momento por el usuario sin que este se preocupe
de nada, el propio sistema de cloud es el que se encarga de mantener siempre
la información disponible. En el caso de que se esté almacenando una
aplicación en la nube, el propio sistema es el que se encarga de subir la
capacidad de computo, memoria, etc. en función del uso que se le está dando
a la aplicación, con lo cual, en la nube no solo se delega la capacidad de
almacenamiento sino también se distribuye en los servidores el procesamiento
de datos” (véase la Figura Nº 3). (Velte, Velte, & Elsenpeter, 2010)

49

www.nitropdf.com

Figura Nº 3: La computación en la nube.
Fuente: Elaboración propia.

2.2.3.1 Características.
Para poder entender de una manera rápida y sencilla cuales son las
claves del concepto del cloud computing, se recurre a una serie de
características principales que lo diferencian de los sistemas tradicionales de
explotación de las TIC. Entre las características asociadas al cloud computing
se encuentran las siguientes (Gómez Orts, 2013):
a) Pago por uso
Una de las características principales de las soluciones cloud es el
modelo de facturación basado en el consumo, es decir, el pago que debe
abonar el cliente varía en función del uso que se realiza del servicio cloud
contratado.

50

www.nitropdf.com

b) Abstracción
Característica o capacidad de aislar los recursos informáticos
contratados al proveedor de servicios cloud de los equipos informáticos del
cliente. Esto se consigue gracias a la virtualización, con lo que la
organización usuaria no requiere de personal dedicado al mantenimiento de
la infraestructura, actualización de sistemas, pruebas y demás tareas
asociadas que quedan del lado del servicio contratado.
c) Agilidad en la escalabilidad
Característica o capacidad consistente en aumentar o disminuir las
funcionalidades ofrecidas al cliente, en función de sus necesidades
puntuales sin necesidad de nuevos contratos ni penalizaciones. De la
misma manera, el coste del servicio asociado se modifica también en
función de las necesidades puntuales de uso de la solución. Esta
característica, relacionada con el “pago por uso”, evita los riesgos
inherentes de un posible mal dimensionamiento inicial en el consumo o en
la necesidad de recursos.
d) Multiusuario
Capacidad que otorga el cloud que permite a varios usuarios compartir
los medios y recursos informáticos, permitiendo la optimización de su uso.
e) Autoservicio bajo demanda

51

www.nitropdf.com

Esta característica permite al usuario acceder de manera flexible a las
capacidades de computación en la nube de forma automática a medida que
las vaya requiriendo, sin necesidad de una interacción humana con su
proveedor o proveedores de servicios cloud.
f) Acceso sin restricciones
Característica consistente en la posibilidad ofrecida a los usuarios de
acceder a los servicios contratados de cloud computing en cualquier lugar,
en cualquier momento y con cualquier dispositivo que disponga de conexión
a redes de servicio IP. El acceso a los servicios de cloud computing se
realiza a través de la red, lo que facilita que distintos dispositivos, tales
como teléfonos móviles, tablets u ordenadores portátiles, puedan acceder a
un mismo servicio ofrecido en la red mediante mecanismos de acceso
comunes.
2.2.3.2 Modelos de servicio.
El autor Gómez en su investigación menciona que: “En base a la
documentación analizada y tomando como referencias principales los informes
del NIST (NIST Cloud Computing Standards Roadmap) y Deloitte (Cloud
Computing: Forecasting change. Market Overview and Perspective) se definen
tres familias fundamentales que marcan la clasificación de las soluciones cloud
atendiendo al servicio que ofrecen” (véase la Figura Nº 3). (Gómez Orts, 2013)
Infraestructura como un servicio (IaaS).
Familia de cloud computing consistente en poner a disposición del
cliente el uso de la infraestructura informática (capacidad de computación,
52

www.nitropdf.com

espacio de disco y bases de datos entre otros) como un servicio. Los
clientes que optan por este tipo de familia cloud en vez de adquirir o
dotarse directamente de recursos como pueden ser los servidores, el
espacio del centro de datos o los equipos de red optan por la
externalización en busca de un ahorro en la inversión en sistemas TI. Con
esta externalización, las facturas asociadas a este tipo de servicios se
calculan en base a la cantidad de recursos consumidos por el cliente,
basándose así en el modelo de pago por uso.
Algunos ejemplos de servicios IaaS son: (Sosinsky, 2011)


Amazon Elastic Compute Cloud (EC2).



Eucalyptus.



GoGrid.



FlexiScale.



Linode.



RackSpace Cloud.



Terremark.

Software como un servicio (SaaS).
Familia de cloud computing consistente en la entrega de aplicaciones
como servicio, siendo un modelo de despliegue de software mediante el
cual el proveedor ofrece licencias de su aplicación a los clientes para su
uso como un servicio bajo demanda. Los proveedores de los servicios
SaaS pueden tener instalada la aplicación en sus propios servidores Web
(permitiendo a los clientes acceder, por ejemplo, mediante un navegador
53

www.nitropdf.com

Web), o descargar el software en los sistemas del contratante del servicio.
En este último caso, se produciría la desactivación de la aplicación una vez
finalice el servicio o expire el contrato de licencia de uso. La solución de
cloud computing de Software como un Servicio puede estar orientada a
distintos tipos de clientes según su condición:
o Usuarios particulares:
Dropbox, Redes sociales, Gmail, Hotmail.
o Usuarios profesionales
CRM, ERP.
Ejemplos de proveedores de servicios SaaS: (Sosinsky, 2011)


GoogleApps.



Oracle On Demand.



SalesForce.com



SQL Azure.

Plataforma como un servicio (PaaS).
El concepto conocido como Plataforma como un Servicio es
básicamente un ambiente de desarrollo en donde se pueden crear otras
aplicaciones que hagan uso de las características del Cloud Computing.
Algunos ejemplos de servicios PaaS son: (Sosinsky, 2011)


Force.com.



GoGrid CloudCenter.



Google AppEngine.
54

www.nitropdf.com



Windows Azure Platform.

Figura Nº 4: Ciclo de publicidad para la computación en la nube 2014.
Fuente: Lowendahl (2014). p.6.

Como podemos ver en el ciclo de publicidad de Gartner para la
computación en nube, publicado en 2014 (ver Figura Nº 4), Big Data es uno de
los temas más esperados y entre los que tienen el mayor tiempo de
estancamiento. Soluciones de Big Data disponibles en la nube ofrecen la
escalabilidad a baja demanda y facilidad de empleo para hacer frente a algunos
problemas con éxito. Dropbox y Netflix han demostrado con éxito, cómo las
organizaciones pequeñas de alto crecimiento pueden aprovechar con eficacia
las tecnologías de Big Data a través de la plataforma de computación en la
nube, para el almacenamiento y gestión de datos de datos. (Bhagattjee, 2014)

55

www.nitropdf.com

2.2.4 Big Data.
El autor Hurwitz menciona que el tratamiento y el análisis de datos
ofrecen

muchos

beneficios

pero

también

muchos

desafíos

a

las

organizaciones. Éstas se han encontrado con muchos problemas a la hora de
buscar una manera eficaz de obtener datos sobre sus clientes, productos,
servicios, etc. Cuando los datos con los que trabaja una organización son todos
del mismo tipo las cosas son más sencillas pero, a lo largo de la historia, tanto
las organizaciones como los mercados en los que éstas operan han crecido
aumentando su complejidad y con ello la complejidad de todos estos datos
(Hurwitz, Nugent, Halper, & Kaufman, 2013).
Muchos de los datos están estructurados y están almacenados en las
tradicionales bases de datos relacionales pero muchos otros como información
del servicio al cliente o fotos y videos no están estructurados. Además, las
organizaciones tienen que trabajar y considerar nuevos datos como los
generados por máquinas (sensores, etc.) o los generados por las personas
(redes sociales, acciones en la web, etc.).
Cada tipo de datos puede manejarse de manera independiente sin mucha
dificultad, el problema llega cuando las organizaciones quieren obtener
información analizando todos los diferentes tipos de datos de manera conjunta.
Cuando se trabaja con una gran cantidad de información y diferentes tipos de
datos es imposible pensar en una manera tradicional de gestionar los mismos.
El problema actual es que cada vez existen más cantidad de datos y éstos
varían en tipo, además, las organizaciones están aumentando el uso de estos
datos ya que pueden ser útiles para el desarrollo del negocio. Aquí es cuando
56

www.nitropdf.com

llega la idea de manejar toda esta información de una manera diferente, es
cuando comienza el reto de Big Data.
Big Data no es una única tecnología, es la combinación del desarrollo
tecnológico desde hace años hasta nuestros días. Como ya se explica en la
introducción, Big Data es la capacidad de manejar grandes volúmenes de datos
dispares a una velocidad aceptable que permita análisis y acciones en tiempo
real. Normalmente, para una definición más detallada de Big Data se hace
referencia a las tres V’s:
 Volumen: Cantidad de datos/información que se trata.
 Velocidad: Velocidad a la que la información es procesada en el sistema.
 Variedad: Los diferentes tipos de datos que están presentes en el
sistema.
Además de estas tres V’s que IBM propuso en un primer momento,
dependiendo de la fuente, se habla también de otras dos V’s:
 Variabilidad: la tecnología de un sistema Big Data debe ser flexible a la
hora de adaptarse a nuevos cambios.
 Valor: El objetivo final del sistema Big Data es generar un valor a partir
de toda la información almacenada de manera eficiente y al menor coste
posible.
Evolución.
La gestión de datos incluye, además de una parte software, avances
tecnológicos

en

hardware,

almacenamiento,

redes

y

sistemas

57

www.nitropdf.com

computacionales como sistemas distribuidos. Los avances emergentes en
la tecnología y la reducción de los costes ha transformado el mundo de la
gestión de datos y ha hecho posible el desarrollo de nuevos sistemas. El
mundo Big Data es la última tecnología que ha emergido gracias a estos
avances.
Big data tiene una gran importancia debido a que permite a las
empresas obtener, almacenar, manejar y manipular grandes cantidades de
datos a una alta velocidad y en el momento concreto para obtener
conocimiento y nuevas percepciones del negocio. Big Data no es una
tecnología independiente, es la combinación de 50 años de evolución
tecnológica. Los diferentes sistemas de gestión de datos han ido surgiendo
en función de las necesidades encontradas y las limitaciones de anteriores
sistemas de datos. La evolución de los últimos 50 años ha desembocado
en la actualidad en la era Big Data.
 Primer tipo de sistemas de gestión de datos.
Cuando en los años 1960 la informática empezó a usarse en los
mercados comerciales, los datos eran almacenados en ficheros simples
sin ningún tipo de estructuración. Más tarde en los años 1970, la gestión
de datos cambió gracias a la creación del sistema de datos relacional y
del sistema de gestión de bases de datos relacionales (RDBMS, por sus
siglas en inglés) que establecieron una estructuración y un método para
la mejora del rendimiento, además, surgió el leguaje SQL que añadía un
mayor nivel de abstracción y hacía posible obtener información de los
datos de una manera más sencilla. La cantidad de información seguía
58

www.nitropdf.com

creciendo y el almacenamiento de los datos era caro, además, acceder
a ellos era cada vez más lento, en este momento surgió el modelo
Entidad-Relación (ER) que añadía mayor abstracción al modelo
relacional para facilitar el uso de los datos. El volumen de datos de las
organizaciones creció hasta límites inmanejables, en este momento
surgió la idea del Data Warehouse. El data warehouse permitía a las
organizaciones seleccionar un subconjunto de los datos almacenados
en función de la información que se quisiera obtener, además, facilitaba
la tarea de obtención de conocimiento del negocio. Los data
warehouses se comenzaron a comercializar en los años 1990 y, con
grandes avances y mejoras, todavía se usan en la actualidad.
Los data warehouses no fueron suficientes cuando comenzaron a surgir
datos semi-estructurados. Para poder resolver este problema de los
datos no estructurados surgió la idea de objetos binarios (BLOBs, por
sus siglas en inglés) donde los datos no estructurados pueden ser
almacenados en bases de datos relacionales como trozos contiguos de
datos. Además, se creó el sistema de gestión de bases de datos de
objetos (ODMS, por sus siglas en inglés) que permitió el manejo de los
datos no estructurados de una manera más fácil y sencilla.
 Segundo tipo de sistemas de gestión de datos.
La mayoría de los datos hoy en día en el mundo son datos no
estructurados como por ejemplo documentos. En los años 1990, con el
crecimiento de la web (www), las organizaciones empezaron a querer
almacenar y gestionar datos como contenido web, audio, video,
imágenes, etc. Desde las organizaciones se invirtió sobre todo en
59

www.nitropdf.com

sistemas de datos estructurados, sin embargo, surgieron sistemas que
ofrecían la gestión de datos no estructurados. Estos sistemas añadieron
metadata (información de la organización e información sobre la
información almacenada) y permitieron a las organizaciones manejar y
gestionar toda su información de una manera lógica. En este momento
las organizaciones empezaron a entender que necesitaban un sistema
de datos que permitiese manejar una gran cantidad de información de
una gran variedad y a una gran velocidad.
 Tercer tipo de sistemas de gestión de datos.
Big Data surgió después de los avances tecnológicos de las últimas
cinco décadas, cundo el coste de cómputo y almacenamiento llegó a un
punto asequible. Si las compañías pueden analizar petabytes de
información con un rendimiento aceptable el negocio puede hacer uso
de los datos en diversas nuevas maneras. El afloramiento de Big Data
no es sólo hacia los negocios, si no también hacia la ciencia, la
investigación y actividades gubernamentales.
El autor Hurwitz también menciona que: “Las tecnologías necesarias para
cubrir las necesidades de las organizaciones están todavía aisladas unas
de otras. Para obtener el estado final requerido, las tecnologías de los 3
tipos de sistemas descritos anteriormente deberán unificarse.” (Hurwitz,
Nugent, Halper, & Kaufman, 2013)
2.2.4.1 Arquitectura Big Data.
Debido a que la parte de gestión de datos se ha convertido en una de las
más importantes dentro del sistema de las organizaciones, es muy importante
60

www.nitropdf.com

tener una arquitectura que soporte el sistema de datos y que sea escalable
para soportar los nuevos requerimientos que puedan darse en el sistema.
A grandes rasgos, en un sistema de gestión Big Data, el primer paso del
ciclo es la captura de los datos que formarán parte del sistema así como su
organización e integración. Una vez que esta fase se realiza, los datos pueden
ser analizados para resolver el problema en cuestión. Finalmente, un plan de
actuación se lleva a cabo en función del resultado de ese análisis. A
continuación se muestra un esquema que representa este ciclo. (Hurwitz,
Nugent, Halper, & Kaufman, 2013)
La Figura Nº 5 muestra las fases de un sistema de gestión Big Data a
gran escala, dentro de cada fase se desarrollan muchas otras como por
ejemplo la parte de validación de los datos cuando éstos son capturados.

Figura Nº 5: Ciclo de Gestión de una Arquitectura Big Data.
Fuente: Elaboración propia.

61

www.nitropdf.com

Los requerimientos y características de la arquitectura Big Data
dependerán de las necesidades de la organización y del tipo de análisis que se
quiera realizar con el sistema Big Data en cuestión. El sistema deberá tener
una adecuada potencia computacional, velocidad, redundancia, escalabilidad,
etc. en función de sus características.
La arquitectura de un sistema de datos Big Data está compuesta por
diversos complementos que hacen posible el uso de los datos del sistema de
una manera eficiente y rápida. En la Figura Nº 6 se muestran a gran escala los
componentes de una arquitectura Big Data.
 Interfaces: como puede observarse en el diagrama, hay interfaces de
input y output tanto para los datos internos almacenados en el sistema
como para los datos de fuentes externas.

Figura Nº 6: Componentes de una arquitectura Big Data.

62

www.nitropdf.com

Fuente: Hurwitz, Nugent, Halper, & Kaufman (2013). p.18.

 Infraestructura física redundante: la infraestructura física de un
sistema Big Data está basada en un sistema distribuido, por lo que los
datos están guardaos físicamente en diferentes localidades y se
conectan mediante redes. La redundancia es muy importante porque en
el sistema se opera con datos de diversas fuentes diferentes.
 Infraestructura de seguridad: el sistema Big Data debe contar con una
infraestructura de seguridad, la implementación de la seguridad
depende del tipo de datos que se traten en el sistema así como de la
finalidad del sistema. Se debe tener en cuenta, por ejemplo, quién
puede acceder a qué datos y bajo qué circunstancias.
 Fuentes de datos: se deben incorporar en el sistema todas las fuentes
de datos necesarias para obtener la información deseada por la
organización y que de una amplia visión del negocio. Las fuentes de
datos, como ya se ha hablado y más adelante se desarrollará en más
detalle,

son

dispares,

pueden

ser

datos

estructurados

o

no

estructurados. Se necesita transformar todos los datos de las diferentes
fuentes para que coincidan con el tipo de datos que se usan en las
transacciones. De esta manera, el sistema tiene disponible los datos
correctos cuando los necesite. La arquitectura debe soportar datos no
estructurados, bases de datos relacionales y bases de datos no
relacionales. El rendimiento y las tareas llevadas a cabo por el sistema
también determinan el tipo de bases de datos que se use en el sistema.
En el pasado, debido a la tecnología desarrollada, las organizaciones no
eran capaces de capturar y/o almacenar estas grandes cantidades de
63

www.nitropdf.com

datos. Actualmente, con la tecnología desarrollada, es posible la
implantación y el desarrollo de sistemas Big Data. Las nuevas
tecnologías están transformando el mercado de los sistemas de gestión
de datos. Hadoop, MapReduce y Big Table son ejemplos que muestran
este avance dentro de los grandes sistemas de gestión de datos.
2.2.4.2 Tipos de datos en Big Data.
El desarrollo de Big Data requiere que una gran cantidad de tipos
diferentes de datos puedan ser integrados y procesados por el sistema. Como
ya se ha dicho con anterioridad, dos grandes grupos principales de datos
deben tratados en el sistema: datos estructurados y no estructurados. Los
datos estructurados suelen ser el 20% de los datos totales disponibles para una
organización, mientras que los datos no estructurados forman el 80% restante
(Hurwitz, Nugent, Halper, & Kaufman, 2013).
Datos estructurados.
El término ‘Datos estructurados’ normalmente se refiere a datos que
tienen una longitud y formato definidos. Las fuentes de datos estructurados
están dividas en dos grandes grupos:
 Datos generados por ordenadores/máquinas: normalmente se refiere
a datos que son creados por las máquinas sin la intervención del
hombre. Ejemplos de este tipo de datos pueden ser: datos generados
por sensores, datos del log de las web’s, datos producidos por los
sistemas financieros, etc.

64

www.nitropdf.com

 Datos generados por personas: datos que, a través de un
ordenador/máquina, proporcionan las personas. Entre estos datos
pueden estar: datos que una persona introduce en un ordenador
directamente, datos producidos con el clic del ratón durante la
navegación web, datos que guardan información sobre video juegos,
etc. (ver Tabla Nº 1)

DNI

Nombre

Apellido 1

Apellido 2

Teléfono

Email

13773163

Julián

Cano

González

654738

julianc@gmail.com

72102459

Marta

Fernández

Urrutia

673980

martaf@gmail.com

74635672

María

Solana

Ruiz

637808

marias@gmail.com

Tabla Nº 1: Tabla 'Personas' de una BBDD Relacional.
Fuente: Elaboración propia.

Datos no estructurados.
Los datos no estructurados son datos que no guardan una longitud y
un formato específicos. Al igual que en los datos estructurados, hay dos
tipos generales de datos no estructurados:
 Datos generados por ordenadores/máquinas: como por ejemplo
imágenes de satélites, datos científicos, videos y fotografías o datos de
radares.
 Datos generados por personas: entre los que pueden estar
documentos de una compañía, datos de redes sociales o contenido web
entre otros.

65

www.nitropdf.com

Datos semi-estructurados.
Son datos con un formato definido pero no estructurado. La
información puede estar definida pero con un formato variable. Entre este
tipo de datos podemos encontrar códigos HTML o XML. (Ver Figura Nº 1)

Figura Nº 7: Código HTML, ejemplo de datos semi-estructurados.
Fuente: Elaboración propia.

Componentes conectores.
En los sistemas Big Data, como ya se ha dicho con anterioridad, es
necesario integrar diferentes fuentes de datos, lo que conlleva a diferentes
tipos de datos. Los datos pueden provenir de diferentes fuentes externas al
sistema, de fuentes internas del sistema o de ambas. No se puede obtener
los resultados buscados si las diferentes fuentes de información se tratan
como independientes, por esta razón, es necesario añadir componentes al
sistema que hacen posible la conexión de estas fuentes. Entre estos
componentes destacan los conectores y los datos metadatos:
 Conectores: conectores que permiten obtener datos de diferentes
fuentes del sistema Big Data.
66

www.nitropdf.com

 Metadatos: es un componente muy importante de los sistemas Big
Data. Los metadatos contienen la definición, las relaciones y otras
características que describen como encontrar, acceder y usar los datos
y componentes del sistema. Los metadatos ayudan a organizar el
almacenamiento de los datos, así como, a manejar las nuevas fuentes
de datos o las variaciones en las mismas. Aunque el concepto de
metadatos no es nuevo debido a Big Data, este concepto está
cambiando y evolucionando para adaptarse a los nuevos requerimientos
de Big Data.

2.2.5 Hadoop.
El autor Wadkar menciona que: “Originalmente Hadoop fue creado por un
ingeniero de Yahoo! Llamado Doug Cutting. Las empresas innovadoras en el
mundo de los motores de búsqueda como Yahoo! o Google necesitaban
encontrar una manera de procesar y obtener significado de la cantidad de
información que sus máquinas recolectaban. Necesitaban por un lado entender
toda la información y por otro, obtener valor de todos esos datos que les
ayudase en su modelo de negocio.” (Wadkar & Siddalingaiah, 2014)
Las necesidades y avances que fueron surgiendo en el mundo de la
tecnología, como ya se ha explicado, dieron lugar al término Big Data. Hadoop
fue desarrollado para cubrir estas necesidades, permite a las organizaciones
manejar grandes volúmenes de información de una manera práctica y sencilla,
además, hace que los grandes problemas puedan ser divididos en partes más

67

www.nitropdf.com

pequeñas por lo que ayuda a un procesamiento más rápido y eficaz. Dividiendo
el problema Big Data en partes más pequeñas, se posibilita el procesado en
paralelo de esas pequeñas partes que se reagrupan una vez procesadas para
presentar los resultados deseados.
Hadoop posibilita la creación de aplicaciones capaces de procesar
grandes cantidades de información distribuida en un sistema de ficheros
distribuido mediante un modelo de programación sencillo. Está diseñado para
ser escalable, lo que hace que pueda ser utilizado en un clúster con un solo
nodo o en los que están formados por miles. El alto grado de tolerancia a fallos
es otra de las características principales de Hadoop, pudiendo gestionar fallos
en los distintos nodos gracias a la detección de errores a nivel aplicación.
El éxito de Hadoop en los últimos tiempos ha sido tal que la mayoría de
implementaciones Big Data distribuidas que hacen uso del paradigma Map
Reduce (se explicará más adelante) tienen como base el framework Hadoop.
Visión general.
Rubio Echevarría menciona que: “Hadoop está diseñado para
procesar cantidades enormes de información (terabytes-petabytes) tanto
estructurada como no estructurada y está implementado en racks de
hardware estándar que forman los clusters Hadoop. Además tiene un alto
grado de escalabilidad y permite pasar de un solo servidor a miles de
ordenadores, además, cuenta con un alto grado de tolerancia a fallos.”
(Rubio Echevarría, 2014)

68

www.nitropdf.com

Desde una perspectiva general puede decirse que Hadoop está divido
en tres piezas (módulos) fundamentales las cuales forman su arquitectura
(ver Figura Nº 8):
 Hadoop Common: Módulo de utilidades comunes el cual soporta los
demás módulos por los que está compuesto Hadoop.
 Hadoop Distributed File System (HDFS): Sistema de archivos
distribuidos que facilita la gestión de los ficheros y los archivos con un
alto grado de fiabilidad y banda ancha, además, es muy económico.
 Hadoop MapReduce: Implementación del algoritmo de procesamiento
de datos Map Reduce, cuenta con un alto rendimiento y trabaja de
manera paralela con los datos distribuidos a través del HDFS.

Figura Nº 8: Módulos Hadoop.
Fuente: Rubio Echevarría (2014).

Cabe destacar que éstos son los tres primeros componentes básicos
generales de Hadoop en su origen. Actualmente existen dos versiones de
Hadoop: Hadoop Versión 1 y Hadoop Versión 2. Además de estos bloques
propios de Hadoop, el ecosistema Hadoop está formado por muchos otros
proyectos que lo complementan y que facilitan y mejoran el entorno Hadoop
para desarrollar soluciones Big Data. Algunas de estas herramientas son
proyectos a su vez Open Source o soluciones Enterprise como el
69

www.nitropdf.com

Biginsights de IBM. (Murthy, Vavilapalli, Eadline, Niemiec, & Markham,
2014)

2.2.6 Hadoop versión 1.
Como ya se ha dicho con anterioridad, el proyecto original de Hadoop
cuenta con tres bloques (módulos) fundamentales:
 Hadoop Common.
 Hadoop Distributed File System (HDFS).
 Hadoop MapReduce.
2.2.6.1 Hadoop Distributed File System (HDFS).
El HDFS es un sistema de archivos distribuido a lo largo de un cluster con
un enfoque versátil y resistente que permite gestionar ficheros en un entorno
Big Data, está diseñado para poder ser implementando en hardware estándar
de bajo coste y tiene una gran tolerancia a fallos. Es el sistema sobre el que se
ejecutan las aplicaciones Hadoop. Las principales características del HDFS
son: (Rubio Echevarría, 2014)


Sistema de ficheros sencillo.
El esquema de HDFS está diseñado para que se parezca lo máximo posible
a los sistemas de ficheros conocidos, especialmente al de Unix. Desde el
espacio de nombres a los permisos de los ficheros y la seguridad.



Tolerancia a fallos.

70

www.nitropdf.com

Las instancias de HDFS pueden llegar a tener hasta miles de máquinas
trabajando como servidores, almacenando cada una de ellas una parte del
sistema de ficheros. Con tal cantidad de componentes formando un
sistema, un fallo de hardware que implique la caída o desconexión de uno o
más de estos componentes es la regla, no la excepción.


Acceso en tiempo real.
En las aplicaciones para las que está pensado HDFS, el usuario necesita
acceder a los datos con un rendimiento constante y elevado.



Gran volumen de datos.
No solo en cuanto a la capacidad total del sistema de ficheros sino también
al volumen individual de los ficheros que lo componen. Un tamaño normal y
aconsejable para un fichero de HDFS puede ir de los gigabytes a los
terabytes.



Modelo simple y coherente.
HDFS está pensado para aplicaciones que necesiten escribir el fichero una
sola vez y que, una vez cerrado, no necesite cambios. De esta manera se
puede conservar la coherencia de los datos y habilita su acceso rápido.



Portabilidad.
El sistema ha de ser portable a una gran cantidad de plataformas, tanto de
hardware como de software.



Escalabilidad.
HDFS también permite la fácil expansión del sistema en caliente, pudiendo
añadir nuevos nodos sin tener que pausar o parar los procesos que hay en
71

www.nitropdf.com

ejecución en el clúster y sin tener que configurarlo; es decir, que el propio
sistema se encarga de determinar que bloques de ficheros almacenará y
que trabajos realizará.

Arquitectura de Hadoop Distributed File System (HDFS).
La arquitectura del HDFS es del tipo maestro-esclavo (master-worker),
está compuesta por un nodo maestro llamado NameNode y por muchos
nodos esclavos llamados DataNodes. El nodo maestro NameNode se
encarga de coordinar el sistema de ficheros y gestiona los accesos a los
ficheros de los diferentes clientes mientras que los nodos DataNode se
encargan de servir los datos que contienen. Un único nodo del cluster hace
la función de NameNode mientras que el servicio DataNode está presente
en todos (o casi todos) los nodos del cluster.
La característica principal del HDFS es que cada fichero almacenado
se divide en bloques de un mismo tamaño, estos bloques se almacenan en
nodos distintos, de esta manera se facilita la programación paralela del
MapReduce que puede acceder a varios bloques de un mismo fichero de
forma paralela. Otra característica es que cada bloque está replicado varias
veces en diferentes nodos para asegurar la disponibilidad de los datos y
evitar fallos si ocurre un error en alguno de los nodos, de esta manera, el
fallo en un nodo no implica la pérdida de todos los datos que contiene.

72

www.nitropdf.com

Figura Nº 9: Arquitectura del HDFS.
Fuente: Rubio Echevarría (2014).

La Figura Nº 9 muestra un ejemplo de distribución de tres archivos (A,
B y C) en un sistema con 3 DataNodes y un factor de replicación 3. A la
hora de configurar el HDFS hay dos parámetros importantes:
 Tamaño de bloque: indica el tamaño en megabytes que deben tener
como máximo los bloques de un fichero. Por defecto está configurado a
64MB o a 128MB que son los valores aconsejados para un buen
rendimiento del sistema.
 Factor de replicación: indica el número de veces que debe estar
replicado cada bloque, por defecto está configurado a 3. No se
recomiendo una factor menor, además, hay que tener en cuenta que si
el factor es muy alto el rendimiento del sistema se puede ver afectado.

73

www.nitropdf.com

NameNode.
El servicio NameNode, como se ha explicado con anterioridad, es el
coordinador del HDFS y es la parte más importante del mismo. El
NameNode tiene varias funciones básicas e importantes:
 Mantiene los metadatos del sistema de archivos (HDFS).
Contiene datos metadatos correspondientes al HDFS como por ejemplo
la jerarquía, los permisos, etc.
 Tiene la localización de los archivos en el cluster.
Tiene la información sobre qué máquina contiene qué archivos.
 Gestiona las peticiones de los clientes.
Cuando un usuario o proceso pide acceso a un fichero el NameNode es
el encargado de redirigirlo a los DataNodes correspondientes,
intentando mantener siempre una uniformidad en la carga del sistema.
 Detección de errores.
Si una petición de acceso a un fichero falla, también es el encargado de
redirigir las peticiones a un nuevo nodo que puedan satisfacerlas.
 Redistribución de datos.
Cuando se elimina o pierde un nodo, los datos que éste contenía no se
pierden debido a la replicación pero sí que se rompe el factor de
replicación. Para solventar este problema, el NameNode realiza copias
de los bloques perdidos y los redistribuye entre los nodos para volver a
tener integridad en los datos.
 Balanceo de carga de datos.

74

www.nitropdf.com

Por la naturaleza de los sistemas Hadoop, lo más probable es que con
el tiempo la carga del sistema no sea siempre uniforme. La adición o
supresión de nodos, por ejemplo, hace que queden algunos DataNodes
vacíos o demasiado llenos. El NameNode tiene varias políticas para
asegurarse la integridad de los datos y también el correcto balanceo del
clúster cada vez que se añaden nuevos datos al clúster. Algunas de
estas políticas son la de mantener una de las réplicas de los bloques en
un rack distinto al que se está escribiendo, otra en el mismo (pero en un
nodo distinto) o intentar mantener la uniformidad de espacio en el disco
entre los nodos.
 Comprobar la integridad del sistema y mantenimiento de la misma.
Cada vez que el sistema arranca, el NameNode se pone en modo
seguro o safemode -el equivalente a modo read-only de un sistema de
ficheros convencional- que no permite la modificación de ninguno de los
bloques. Este estado dura mientras comprueba con todos los DataNode
que la mayoría de los bloques están disponibles.
El NameNode guarda el estado del sistema en un fichero de log llamado
fsimage y, cada vez que se realiza una operación de modificación
dentro de HDFS, escribe en el fichero de log edits los cambios
realizados. Cada vez que HDFS se inicializa, el NameNode carga el
estado del sistema desde el fichero fsimage y realiza todos los cambios
marcados en edits, volviéndolo a dejar vacío. También se pueden crear
checkpoints o puntos de guardados para que el NameNode recupere un
estado anterior mediante la importación de los ficheros fsimage y edits.

75

www.nitropdf.com

Cuando un sistema HDFS lleva mucho tiempo encendido, el fichero edits
puede llegar a ser demasiado grande debido a que solamente se vacía durante
el proceso de arranque del sistema, haciendo que la carga sea más lenta.
Existe un segundo servicio, llamado Secondary NameNode, que tiene como
objetivo principal mantener la integridad del sistema y evitar que esto último
ocurra. Este servicio (que debería ejecutarse en un nodo distinto) puede tener
varias instancias y realiza periódicamente las modificaciones de fsimage
marcadas en edits en lugar del NameNode, manteniendo así los logs dentro de
unos límites de tamaño.
El NameNode es sumamente importante dentro de un sistema HDFS y su
pérdida o desconexión debido a un error puede hacer que el sistema quede
totalmente bloqueado y deje de funcionar hasta su recuperación. Por lo que el
NameNode es un punto único de fallo en el sistema.

2.2.6.2 MapReduce 1.0
Como ya se ha explicado con anterioridad, la versión MapReduce de
Hadoop está basada en el paradigma Map Reduce que Google propuso
inicialmente. El MapReduce de Hadoop está desarrollado para trabajar en el
HDFS y para que se ejecute en el mismo clúster. (Rubio Echevarría, 2014)
El MapReduce de Hadoop está programado en Java y el propio Hadoop
se encarga de gestionar los trabajos MapReduce, ofreciendo la máxima
sencillez al programador. Aun así, el paradigma MapReduce es complejo,

76

www.nitropdf.com

razón por la que han ido surgiendo diferentes herramientas de más alto nivel
que facilitan y simplifican el ecosistema Hadoop.

Arquitectura MapReduce.
La arquitectura MapReduce es, al igual que el HDFS, del tipo maestroesclavo. Cuenta con dos tipos de servicios:
 Servicio JobTracker:
El encargado de gestionar, monitorizar y distribuir la carga de los
trabajos MapReduce.
 Servicio TaskTracker:
Se ejecuta en un nodo con un servicio DataNode de HDFS. Recibe las
órdenes del JobTracker y se encarga de realizar el proceso sobre el
bloque de datos que contenga.

77

www.nitropdf.com

Figura Nº 10: Esquema arquitectura MapReduce.
Fuente: Rubio Echevarría (2014).

En la Figura Nº 10 podemos observar cual es el workflow habitual (sin
errores, entre otras eventualidades) de un trabajo MapReduce en una
arquitectura Hadoop con HDFS como sistema de ficheros. La arquitectura
de la figura se compone de cuatro nodos: tres nodos de trabajo
(DataNode+TaskTracker)

y

uno

de

gestión

o

administración

(NameNode+JobTracker). Los pasos que se ejecutan en el clúster
explicados con detalle son los siguientes:
1. Se ejecuta el proceso MapReduce. El JobTracker recibe los datos de
ejecución (qué proceso se lanza y sobre qué datos -almacenados en

78

www.nitropdf.com

HDFSse realiza). Se comunica con el NameNode para conocer la
distribución de los datos.
2. Una vez sabe qué DataNodes contienen los bloques con los que hay
que trabajar, distribuye los procesos map a los TaskTrackers
candidatos, los que se ejecutan en los nodos que contengan los
bloques.
3. Cada TaskTracker realiza el trabajo sobre el bloque asignado
comunicándose con el DataNode.
4. Una vez obtiene los resultados, los TaskTrackers los envían al
JobTracker.
5. Una vez todos los procesos map han terminado, el JobTracker envía los
resultado a un nodo de trabajo para que realice el reduce del proceso.
6. El TaskTracker realiza el reduce y envía los resultados al JobTracker.
7. El JobTracker envía los resultados a HDFS para que puedan ser
consultados.
Puede que durante la ejecución de un proceso MapReduce alguno de
los nodos de trabajo tenga un error o se desconecte del clúster, en ese
caso el JobTracker lo detecta y, gracias a que los datos están replicados en
HDFS y en distintos nodos, delega el trabajo fallido sobre otro nodo que
contenga el bloque. De esta manera cuando hay un error no se tiene que
volver a rehacer todo el proceso sino que solamente tiene que volver a
ejecutarse la parte perdida.
Etapas MapReduce.

79

www.nitropdf.com

En la ejecución de un MapReduce hay diferentes etapas en las que
los datos son tratados:

 Mapping.
En esta etapa los nodos escogidos por el JobTracker (llamados
Mappers) realizan el trabajo de forma paralela y distribuida sobre los
datos de entrada -uno o varios ficheros en HDFS- y obtienen una salida
con la forma:

 Shuffle.
Los resultados obtenidos en la etapa anterior se mezclan, obteniendo
una lista con todas las parejas clave-valor.
 Sort.
Se agrupan las parejas resultado del mapeo basándose en las claves.
En caso de buscar otro orden de ordenación, el usuario puede
implementar una segunda ordenación usando un objeto Comparator y
configurando el trabajo para usarlo. Se obtiene un resultado con el
siguiente formato:

 Partitioning.
Parte la lista resultante de la etapa de ordenación delegando cada parte
a un Reducer. El objeto Partitioner puede ser implementado si el usuario
80

www.nitropdf.com

busca una partición distinta a la que viene por defecto (un
HashPartitioner).

 Combining.
Esta es la etapa que menos suele aparecer ya que se acostumbra a
configurar los trabajos para que el Combiner sea el mismo que el
Partitioner y, en caso contrario, acostumbra a usarse para liberar de
trabajo a este último.
 Reducing.
Se realiza la reducción final del trabajo. La entrada del Reducer (el nodo
que realiza esta función) recibe las claves y las listas de valores para
estas claves. Puede haber más de un Reducer o ninguna, si lo que se
busca es la salida de la fase de mapeo. El número de Reducers
aconsejables acostumbra a ser:

Cuando se usa la constante 0.95 todos los Reducers pueden lanzarse a
la vez, mientras que usando 1.75, se lanzaran por olas a medida que
vayan terminando (puede darse el caso que los nodos más rápidos
realicen más reduces que los más lentos).
Los datos obtenidos una vez transcurridas todas las etapas del
MapReduce se almacenan en un directorio del sistema HDFS que contiene
los siguientes ficheros:

81

www.nitropdf.com

 _SUCCESS: Fichero vacío que indica que la ejecución ha sido correcta.
 _FAIL: Fichero vacío que indica que la ejecución ha sido errónea.
 Part-r-XXXXX: Resultado de Reducer.
 Part-m-XXXXX: Resultado del Mapper.
2.2.7 Hadoop versión 2.
El autor Murthy menciona que la versión 2 de Hadoop está basada en la
versión 1 pero modifica partes de la primera versión para resolver algunos
problemas

así

como

para

mejorar

el

rendimiento.

Además,

añada

características nuevas que mejoran también el ecosistema Hadoop.
La primera modificación y más llamativa es que ahora los módulos
básicos en los que está divido Hadoop no son tres si no cuatro. En esta versión
se añade un cuarto módulo:
 Hadoop Common.
 Hadoop Distributed File System (HDFS).
 Hadoop MapReduce.
 Hadoop YARN.
Hadoop YARN es un framework para la gestión y planificación de trabajos
y para la gestión de los recursos del cluster. (Murthy, Vavilapalli, Eadline,
Niemiec, & Markham, 2014)
2.2.7.1 Hadoop Distributed File System (HDFS).
Las diferencias entre el HDFS de Hadoop Versión 1 y de Hadoop Versión
2 no son demasiadas. La primera modificación que se hace en el HDFS es la
82

www.nitropdf.com

supresión del NameNode como único punto de fallo en el sistema. En la
primera versión el NameNode era un único punto de fallo en el sistema, lo que
hacía que la disponibilidad del servicio no fuera completa. Un único fallo en el
NameNode podía hacer que el servicio dejara de funcionar. Otra modificación
relevante es la introducción del HDFS Federation que permite tener diferentes
espacios en el HDFS.
Quitando las dos novedades explicadas anteriormente, el resto de
características del HDFS se han dejado prácticamente igual que en Hadoop
Versión 1. (Rubio Echevarría, 2014)
Supresión del NameNode como único punto de fallo.
Para otorgar al sistema una alta disponibilidad y poder eliminar el
NameNode como único punto de fallo se han añadido al sistema dos
maneras diferentes a través de las cuales se puede obtener esta alta
disponibilidad.
 Quorum Journal Manager.
En este tipo de arquitectura se configura, aparte del NameNode
principal, un segundo NameNode que está en modo espera o standby,
llamado precisamente Standby NameNode. Este servicio permanece
inactivo a la espera de un fallo en el NameNode activo, que es el
encargado de realizar las tareas de gestión y administración del sistema.
Para mantener la coherencia de los datos entre los dos NameNodes y
mantenerlos sincronizados se crea un grupo de servicios, llamados

83

www.nitropdf.com

JournalNodes, cuya función es la de actuar como diarios de todas las
operaciones que el NameNode activo va realizando. Este conjunto de
JournalNodes se llama Quorum Journal Manager.
El NameNode comunica a un grupo de JournalNodes (no hace falta que
lo haga con todos ya que entre ellos se sincronizan) todos los cambios
que se van realizando en el sistema de ficheros (es decir, en los
DataNodes). El Standby NameNode, por su parte, va leyendo el estado
del sistema a través de los JournalNodes de manera que cuando se
produce un evento de fallida pueda actuar rápidamente como
NameNode activo.
Para asegurar la coherencia del sistema de ficheros se toman ciertas
medidas:
o Sólo puede haber un NameNode activo ya que la existencia de más
de uno podría provocar fallos de sincronización y una ruptura del
sistema de ficheros. Los JournalNode sólo dan el permiso para
realizar cambios a uno de los NameNodes; de manera que cuando
hay un fallo en el activo, el inactivo pasa a reemplazarlo en el rol de
servicio activo.
o El NameNode inactivo solamente pasa a activo cuando se ha
asegurado de que ha leído todos los cambios contenidos en los
JournalNodes.
o Los DataNodes están configurados con las direcciones de los dos
servicios de NameNode, tanto del activo y como del inactivo, para
enviar constantemente información de los bloques e informar de su
84

www.nitropdf.com

estado. Con esto se logra que la transición entre NameNodes sea lo
más rápida posible, ya que ambos tienen toda la información
necesaria para realizar las tareas de un servicio activo.
o Un JournalNode es un servicio que requiere de pocos recursos por lo
que puede ser configurado en nodos con otros servicios del clúster
(NameNode, DataNode, JobTracker, etc.).
o Se recomienda tener, como mínimo, tres servicios JournalNode para
tener un Quorum preparado para la pérdida de uno de los nodos.
Para poder aumentar el número de caídas toleradas se debe
incrementar el número de servicios en cantidades impares (3, 5, 7...),
ya que el Quorum tolera la caída de(N-1)/2 nodos, donde N es el
número total de servicios JournalNode.
El uso del Quorum Journal Manager es totalmente transparente para el
usuario y la transición del Standby NameNode a servicio NameNode
principal puede ser configurada como automática o manual. Para que
esta transición sea automática se requiere de ZooKeeper para detectar
los fallos en el NameNode y monitorizar su estado.
 Network File System.
Este método para lograr la alta disponibilidad en HDFS también cuenta
con un NameNode principal y un Standby NameNode, realizando las
mismas funciones que en el caso del Quorum Journal Manager.
La diferencia principal es que en lugar de usar un Quorum para la
sincronización entre los NameNodes se utiliza un dispositivo de

85

www.nitropdf.com

almacenamiento conectado mediante una Network File System. NFS es
un protocolo de sistemas de ficheros en red que permite a diferentes
ordenadores acceder a ficheros en remoto.
Garantizando el acceso de los NameNodes a este dispositivo a través de
NFS, el sistema funciona de manera muy parecida a como lo hace con el
Quorum. Cada vez que el NameNode activo realice cambios sobre el
sistema de ficheros lo dejará indicado en un fichero de ediciones,
almacenado en un directorio del dispositivo NFS. De esta manera el
nodo en standby puede ir consultando los cambios realizados en el
espacio de nombres del sistema de ficheros.
Tal y como pasa con el Quorum Journal Manager, solo puede haber un
nodo activo a la vez y uno en standby solo cambia de estado a activo
cuando está seguro de haber leído todos los cambios del fichero de
ediciones.
Introducción del HDFS Federation.
Una división por capas y explicación simplificada de la arquitectura de
HDFS (y generalizando de cualquier sistema de ficheros) sería la siguiente:
 Espacio de nombres: es la parte lógica del sistema y la que el usuario
más acostumbra a ver. El conjunto de directorios, ficheros y bloques así
como su estructura jerárquica. El espacio de nombres de HDFS permite
las operaciones para crear, borrar, modificar y conseguir la localización
de los bloques.

86

www.nitropdf.com

 Servicio de almacenamiento: es la parte más física (a nivel de
software) del sistema y acostumbra a ser transparente al usuario. Tiene
dos partes:
o Administración de bloques: realizado por el NameNode.
o Almacenamiento: realizado por el DataNode.
La primera versión de HDFS trabajaba con un espacio de nombres
único e igual al de un sistema de ficheros Unix (con la excepción de la
operación para conseguir los bloques). Con la aparición de HDFS
Federation esto cambia, ahora se puede tener varios espacios convirtiendo
HDFS en escalable a nivel horizontal.
En la Figura Nº 11 se muestra una arquitectura HDFS que hace uso
de HDFS Federation. En esta arquitectura hay un NameNode para cada
espacio mientras que el conjunto de DataNodes sigue siendo común y no
se ve afectado por los espacios. Los NameNodes de cada espacio trabajan
independientemente unos de otros sin que haya coordinación entre ellos.

87

www.nitropdf.com

Figura Nº 11: Arquitectura del HDFS Federation.
Fuente: Rubio Echevarría (2014).

Características que aporta la HDFS Federation:

o Conjunto de bloques exclusivos.
Es un conjunto de bloques que pertenecen exclusivamente a un
espacio de nombres y, por lo tanto, administrados por un solo
NameNode. De esta manera el fallo en un NameNode no evitaría
a los DataNodes de seguir sirviendo bloques de otros espacios.

88

www.nitropdf.com

Al conjunto de espacio de nombres y bloques exclusivos se le
llama volumen del espacio de nombres.

o Escalabilidad de espacios horizontal.
Añadir datos en un sistema HDFS con un solo NameNode puede
tener escalabilidad horizontal en el almacenamiento pero no si
hablamos de los metadatos. Al tener varios NameNodes habrá
una mejor escalabilidad en cuanto a los espacios.
o Rendimiento.
Por la misma razón, las peticiones de lectura y escritura serán
servidas más rápidamente al tener que procesar menos
información los NameNodes.
o Aislamiento.
Al tener varios espacios de nombres y cada uno con su propio
NameNode, se puede configurar diferentes categorías e incluso
usuarios para aislarlos en distintos espacios, de manera que no
interfieran entre ellos.
En un principio puede parecer que esta nueva característica también
es una buena manera de ofrecer alta disponibilidad en el sistema, pero
nada más lejos de la realidad. Aunque haya varios NameNodes y la caída
de uno no implica la inaccesibilidad de todo los datos almacenados en
HDFS (solamente la de los bloques almacenados en su espacio) sí que hay
pérdida de datos y sigue siendo un punto de fallo único en el sistema.

89

www.nitropdf.com

2.2.7.2 MapReduce 2.0
La parte MapReduce es la parte que más cambios ha sufrido en
comparación con la primera versión de Hadoop. Las características a nivel
usuario (fases de un proceso) se han mantenido pero se ha modificado por
completo la arquitectura del MapReduce así como los servicios que lo
componen.
Arquitectura YARN.
YARN es un motor de gestión de recursos y aplicaciones o procesos
distribuidos y es la principal adición de Hadoop 2.0. Actualmente
prácticamente solo lo usa MapReduce pero en un futuro puede ser usado
para otros frameworks para gestionar las aplicaciones. La principal idea
detrás de YARN es la de hacer una gestión más óptima de los recursos de
un clúster a la hora de realizar un proceso MapReduce.
Hasta ahora el JobTracker realizaba dos funciones destacadas: la
gestión de recursos y la planificación y monitorización de los trabajos. Con
YARN, el JobTracker deja paso a dos servicios nuevos que se encargan
cada uno de una de estas tareas y aparece también el NodeManager. Los
servicios trabajan sobre Containers, un concepto abstracto que se utiliza
para agrupar los diversos recursos de un sistema (procesadores, memoria,
disco, red, etc.) en una misma noción.
 ResourceManager: es un servicio global para todo el clúster y que se
encarga de gestionar los recursos del sistema. Tiene dos partes:

90

www.nitropdf.com

o Scheduler: es el responsable de asignar los recursos a cada
aplicación.

Está

diseñado

específicamente

para

realizar

solamente tareas de planificación, por lo que no es encarga de
monitorizar ni relanzar aplicaciones caídas. Se encarga de dividir
los recursos del clúster a través de diferentes colas, aplicaciones
y

puede

tener

varias

implementaciones

de

su

política

(modificables a modo de plug-in), como el CapacityScheduler o el
FairScheduler.
o ApplicationsManager:

se

responsabiliza

de

aceptar

las

peticiones de creación de trabajos y de crear y asignar un
ApplicationMaster a cada una de ellas.
 ApplicationMaster: es un servicio único para cada aplicación (es decir,
para cada trabajo MapReduce). Su principal cometido es el de negociar
con el ResourceManager la gestión de los recursos y la de trabajar con
los NodeManagers para ejecutar y monitorizar los trabajos.
 NodeManager: es un agente que se ejecuta en cada máquina y es el
responsable de los Containers. Se encarga de gestionar los recursos
asignados al Container y reportar su estado al Scheduler.
El proceso que siguen estos servicios a la hora de crear un proceso
MapReduce se puede ver en la Figura Nº 12 y sigue los siguientes pasos:
1. El

usuario

ejecuta

un

proceso

y

se

crea

una

petición

al

ApplicationsManager para crear un proceso.
2. El ApplicationsManager habla con el NodeManager para crear un
ApplicationMaster.
91

www.nitropdf.com

3. El NodeManager crea el ApplicationMaster.
4. El ApplicationMaster negocia con el Scheduler del ResourceManager los
recursos que serán asignados a los Containers.
5. Una vez sabe los recursos, en ApplicationMaster se comunica con los
NodeManagers

correspondientes

para

que

lancen

el

proceso

MapReduce en un Container con una máquina virtual de Java.
6. El NodeManager lanza y monitoriza el proceso MapReduce sobre un
Container.
7. Los Container van reportando su estado al ApplicationMaster, que se
encarga de coordinar los procesos.

Figura Nº 12: MapReduce con YARN.
Fuente: Elaboración propia

92

www.nitropdf.com

Nueva API.
Con MRv2 también se ha cambiado la API antigua (con el nombre
mapred) a una nueva API más fácil de usar (llamada mapreduce). La
filosofía sigue siendo la misma, es decir implementar las interfaces que
ejecutarán los procesos de las diferentes fases del trabajo MapReduce. En
este punto se comentan algunos de los cambios más significativos de la
nueva API. Para la interfaz Mapper se ha cambiado la signatura de la
función map:

La clase Context sustituye al OutputCollector y al Reporter y asume
sus funciones: la de escribir la salida (con la llamada context.write(K,V)) y la
de reportar el funcionamiento y el estado del proceso. También permite la
configuración del Mapper, reemplazando el método configure.
En la interfaz Reducer (y también en el Combiner) la clase Context
sustituye también el OutputCollector:

Los valores de la clave también se pasan de forma distinta, ahora se
usa una clase Iterable en lugar del Iterator de la API antigua.
 Compatibilidad con MapReduce 1.0

93

www.nitropdf.com

MRv2 mantiene la compatibilidad de API a la hora de programar
aplicaciones de manera que las aplicaciones de MRv1 deberían ser
completamente funcionales sin tener que realizar cambios. Además,
también se aprovechan de las características de YARN ya que a nivel de
arquitectura están en diferentes niveles.

2.2.8 Procesamiento de datos.
El autor Lampi menciona que: “El procesamiento y análisis de datos
consta de numerosos métodos y paradigmas. En esta sección se pasa a través
de algunos de los más comunes y se describe las aplicaciones y herramientas
que se han utilizado a nivel mundial en el procesamiento y análisis de datos
durante años.” (Lampi, 2014)
2.2.8.1 Procesamiento de datos, análisis y visualización.
El análisis de datos tradicional consiste en métodos estadísticos en su
mayoría, y varían bastante de los métodos utilizados con el análisis de datos
grandes (Big Data). Sin embargo, dependiendo de los datos y la tarea, los
métodos tradicionales de análisis de datos pueden ser utilizados para analizar
conjuntos de datos más grandes. Los métodos más comunes de análisis de
datos tradicionales se enumeran en la Tabla Nº 2 (Chen, Mao, & Liu, 2014). El
análisis de clúster es un método estadístico para agrupar objetos. Factor,
correlación y análisis de regresión están enfocados sobre las relaciones de los
elementos, las pruebas A/B compara dos elementos, el análisis estadístico
utiliza estadística y probabilidad, y la minería de datos utiliza diferentes
94

www.nitropdf.com

algoritmos para obtener nueva información de datos aleatorios y difusos.
(Lampi, 2014)
Análisis de
Clúster
Análisis
factorial
Análisis de
correlación

Análisis de
regresión
Pruebas A/B
Análisis
estadístico
Minería de
datos

Método para diferenciar los objetos con características
particulares y agrupándolos en grupos de acuerdo a las
características. No hay datos de entrenamiento.
La agrupación de las variables estrechamente relacionadas
en un factor y luego utilizando el factor para revelar la
información de los datos.
Método analítico para la determinación de las relaciones,
como la correlación y la restricción mutua en los datos
incluidos en la muestra y la realización de previsión en
consecuencia.
Herramienta para la búsqueda de correlaciones entre
variables, identifica las relaciones de dependencias entre las
variables.
Los métodos para la comparación de variables objetivo contra
variables de la prueba.
Análisis basado en la teoría estadística, el azar y la
incertidumbre se modela con la teoría de la probabilidad.
Proceso de extracción de información previamente
desconocida de un conjunto de datos.

Tabla Nº 2: Descripción de algunos de los métodos de análisis de datos tradicionales más
utilizados.
Fuente: Elaboración propia.

Los métodos para el procesamiento de grandes conjuntos de datos
difieren bastante de los métodos de análisis de datos tradicionales. Algunos de
ellos se enumeran en la Tabla Nº 3 (Chen, Mao, & Liu, 2014).
Filtro Bloom

Hashing
Índice

Triel
La computación
paralela

Comprueba si un elemento pertenece a un conjunto
comparando valores hash de los datos aparte de las
idénticas en una matriz de bits.
Transforma los datos en numéricos de longitud fija o
valores de índices.
Estructura de datos para indexar algunos de los datos, se
puede utilizar tanto para los datos estructurados y no
estructurados.
Variante de un árbol de hash que utiliza prefijos comunes
de cadenas de caracteres para reducir la comparación.
Utiliza múltiples unidades de computación en lugar de uno.
El problema se divide a múltiples piezas y resuelve de
95

www.nitropdf.com

forma independiente en paralelo.
Tabla Nº 3: Descripción de las técnicas utilizadas para analizar grandes conjuntos de datos.
Fuente: Elaboración propia.

Como se ha mencionado antes, en algunos casos los métodos
tradicionales se pueden utilizar también para el análisis de datos grandes. De
forma predeterminada, sin embargo, los conjuntos de datos grandes y no
estructurados se analizan mejor utilizando métodos que son creados
exclusivamente para este fin.
Bloom filtra valores hash almacenadas sobre otras entidades de datos.
Hashing es un método simple y eficiente, que da nueva forma a los datos en
índices o valores numéricos. Indexación viene con el precio de la carga de
almacenamiento adicional, pero es muy eficiente. Triel se centra en las
estadísticas de frecuencia de palabras y de recuperación rápida. La
computación paralela divide una tarea entre las unidades de procesamiento.
La investigación en análisis de datos se puede dividir en diferentes
campos técnicos. La división trata de separar los datos (y la investigación) en
diferentes entidades características y mantener la investigación centrada en un
subconjunto más reducido de todo el campo. Incluso algunos de los métodos
se superponen.
La separación básica de estos campos se puede hacer como se ve en la
Tabla Nº 4. (Lampi, 2014)

96

www.nitropdf.com

Análisis de
datos
estructurados
Análisis de los
datos de texto

Análisis de
datos de la Web

Análisis de
datos
Multimedia

Análisis de
datos de la red

Análisis de
datos Mobile

Empresas e Investigaciones
– producen datos
estructurados; análisis basado principalmente en la minería
de datos y análisis estadístico.
Incluye mensajes de correo electrónico, registros, redes
sociales y páginas web, por lo que el formato de la
información textual es la más común de la información
almacenada. Extrae información útil del texto no
estructurado. Técnicas y procesos involucrados incluyen
machine learning, minería de datos, estadísticas y la
lingüística computacional. La mayoría de los sistemas de
análisis de texto se basan en el procesamiento del lenguaje
natural (NLP).
Automatiza la recuperación de información, extracción y
evaluación de los documentos de la web y servicios para
descubrimiento de conocimiento. En estrecha relación con
otros campos de investigación, tales como bases de datos,
NLP y la minería de texto. El análisis de datos de la Web
consta de tres principales sub campos – el contenido, la
estructura y el uso de minería.
Principalmente el análisis de imágenes, videos y audio.
Debido a la costosa calidad de la información, el análisis de
datos multimedia es muy difícil. Utiliza los metadatos de la
sintaxis y el contexto. Elementos como las palabras, las
secuencias y los textos se utilizan para extraer información.
Extrae y analiza información de red producida por el
internet, por ejemplo, de sitios web, empresas como
Twitter, Facebook y LinkedIn. Grandes cantidades de datos
en su mayoría la descripción de la conexión entre dos
compañeros. La predicción de nuevas conexiones y
comunidades, la evolución de la red y las influencias
sociales son los principales campos de análisis.
Analiza los datos producidos por la aplicación móvil, como
información geográfica, las comunidades móviles con
intereses similares.

Tabla Nº 4: Los campos de investigación en el campo del análisis de datos.
Fuente: Elaboración propia.

97

www.nitropdf.com

2.2.8.2 La minería de datos (Data Mining).
Bhushan menciona que “La minería de datos es una parte importante del
análisis de datos. Es un proceso en el que los modelos previamente
desconocidos e información procesable se extraen de los datos en bruto”
(Bhushan Agarwal & Prakash Tayal, 2009). El proceso de minería de datos ha
ido ganando progresivamente el interés de más de una década, y empresas de
diversos tamaños están dando al campo más atención. La creación de la
minería de datos se encuentra en algunos campos diferentes, tales como la
reducción de datos, teoría de la probabilidad, y la microeconomía. La mayoría
del tiempo, los datos son pre-procesados antes de que comience la verdadera
minería y, si es necesario, que sea transformada a un formato más adecuado,
por ejemplo a través de la normalización (ver Figura Nº 13). El preprocesamiento de los datos es la parte más crucial del proceso, ya que los
datos que se almacena por lo general son muy distorsionados y ruidosos. Hay
un montón de diferentes técnicas para lograr este objetivo. Un enfoque
matemático, tales como histogramas, máximos y mínimos o usando la lógica
humana (por ejemplo, el sexo = masculino, embarazada = sí), rellenando los
valores perdidos, y así sucesivamente. (Lampi, 2014)

Figura Nº 13: Las diferentes entidades de un sistema de minería.
Fuente: Elaboración propia.

98

www.nitropdf.com

Los diferentes pasos para hacer una utilidad de los datos en bruto, es
decir, obtener una visión e información accionable desde el conjunto de datos,
como se muestran en la Tabla Nº 5. Un ejemplo va desde la construcción de un
modelo con los datos existentes para compararlo con los nuevos datos y,
finalmente, el uso de la información descubierta para predecir los futuros
eventos (Bhushan Agarwal & Prakash Tayal, 2009).
1. Modelar
2. Descubrir

3. Predecir

La construcción de un modelo para un problema resuelto usando el modelo para resolver un problema sin resolver.
El descubrimiento de modelos previamente desconocidos /
obtener información acerca de algo aparentemente sin
relación.
Predecir la próxima aparición utilizando la información
encontrado previamente.
Tabla Nº 5: Diferentes etapas de la minería.
Fuente: Elaboración propia.

Leskovec menciona que a minería de datos se puede dividir en diferentes
subcategorías y direcciones en función al enfoque de modelado y el campo.
Las direcciones más comunes para el modelado de hoy en día son la
estadística de minería de datos, machine learning de minería de datos, y
computación de minería de datos (Leskovec, Rajaraman, & Ullman, 2014). Los
estadísticos fueron los primeros en modelar los datos, que lo calificó de
“minería de datos”.
El término de minería de datos fue utilizado por primera vez en forma
despectiva para hablar de la extracción de datos que no estaba presente en el

99

www.nitropdf.com

conjunto de datos. Los estadísticos de hoy ven a la minería de datos como una
distribución fundamental de datos, en base a los datos visibles.
Algunos ven la minería de datos como sinónimo de machine learning,
donde se utiliza el conjunto de datos para entrenar el algoritmo, por ejemplo, a
través de las redes de Bayes y modelos ocultos de Markov. El enfoque más
reciente para el campo es la computación de minería de datos, que ve a la
minería de datos como un problema algorítmico. En general, la minería de
datos se compone de 5 elementos principales:


Transformar y cargar los datos en un sistema (almacén, centro de datos,
DFS etc.).



Almacenar y gestionar los datos.



Proporcionar acceso a los datos para los analistas de negocios y otros
profesionales.



Analizar los datos.



Presentar los datos en un formato útil.
Arquitectónicamente un sistema de minería de datos típico consta de los

componentes que se muestran en la siguiente imagen (ver Figura Nº 14 y su
descripción en la Tabla Nº 6). Diferentes configuraciones, programas y
sistemas pueden ser construidos de manera algo diferente y pueden ser
programadas para las tareas más específicas.
Algunos módulos de Hadoop, como Hive y Mahout, son las herramientas
adecuadas para la minería de datos. Hive puede facilitar tareas de minería,
proporcionando su propio lenguaje similar a SQL, QL, que es totalmente
100

www.nitropdf.com

compatible con MapReduce. Mahout añade capacidades de machine learning
con la agrupación y clasificación para el proceso de minería. (Lampi, 2014)

Figura Nº 14: Ejemplo de la arquitectura de un sistema de minería de datos.
Fuente: Elaboración propia.

Interfaz de usuario
Evaluación de
modelos
Base de
conocimientos
Motor de minería
de datos
Servidor de datos
Repositorio de
datos

Consultas especificas por el usuario y las tareas de
minería de datos.
Emplea medidas interesantes de un modelo e interactúa
con el motor de la minería de datos y la base de
conocimientos.
Intereses de diferentes modelos, las jerarquías de
conceptos, creencias de los usuarios, etc.
Se compone de diferentes módulos funcionales para las
tareas, como asociación, caracterización y análisis de
clúster.
Obtiene los datos pertinentes de acuerdo con las
solicitudes de los usuarios.
Una o más bases de datos, un data warehouse,
datacenter, servicios basados en la nube, etc.

Tabla Nº 6: Responsabilidades de los componentes de un sistema de minería de datos.
Fuente: Elaboración propia.

2.2.8.3 Herramientas y aplicaciones para el análisis.
Algunas herramientas de análisis y procesamiento de datos utilizados
comúnmente se describen a continuación. (Lampi, 2014)

101

www.nitropdf.com

R.
R es un lenguaje estadístico de alto nivel y un entorno informático que
se deriva del lenguaje S (la mayoría de las funciones proporcionadas por el
sistema están escritos en S (Barker, 2013)). R se ha convertido en uno de
los softwares de análisis y estadística más utilizada, en parte debido al
hecho de que es de código abierto, tiene una gran comunidad activa, e
incluye miles de paquetes a medida. De acuerdo con la encuesta “¿Qué
software de análisis utilizaste para un proyecto real de, minería de datos,
Big Data en los últimos 12 meses?”, R era la herramienta más utilizada
para el análisis y visualización de datos grandes entre los participantes de
la encuesta en el año 2012 (Chen, Mao, & Liu, 2014). Tiobe.com
(exponente de la comunidad de programación) también ha clasificado de
manera constante a R como uno de los programas más populares en el
campo, incluso superando a los softwares comerciales como Matlab y SAS.
(Cotton, 2013)
La herramienta más importante para el uso de R es la consola. GUI
tiene conjunto muy limitado de características y funcionalidades, lo que
significa que todo el trabajo real se realiza a través de la consola. El uso de
la consola es muy sencillo: el usuario escribe el comando y presiona enter,
R imprime el resultado en la pantalla. Las operaciones matemáticas, la
definición de variables, declaración de funciones y estructuras de datos,
etc. se pueden hacer rápidamente y fácilmente. Técnicamente, todo lo que
el usuario escribe es una expresión (Pace, 2012), (Abedin, 2014). A

102

www.nitropdf.com

continuación se muestra un breve ejemplo de cómo R toma la expresión y
lo que se imprime como una salida (ver Tabla Nº 7).

Expresión
Cálculos
Secuencias
Vectores
Funciones
Variables

Salida
3
1,2,3,4,5
4,6
-1
El usuario escribe
enter.
4
El usuario escribe
enter.

Estructura de datos

Objetos

y la tecla

y la tecla

El usuario escribe
tecla enter.

y la

Tabla Nº 7: Algunos ejemplos de los comandos de R en consola.
Fuente: Elaboración propia.

R se puede ejecutar en modo batch (grupo), lo que significa que una
secuencia de comandos se puede guardar en un archivo y se utiliza
posteriormente

para

el

procesamiento

más

complejo,

acelerando

dramáticamente el proceso de comparación para escribir todos los
comandos de uno en uno. R también se puede ejecutar como una
aplicación web, en un servidor o dentro Emacs. R incluye varios paquetes
por defecto para la visualización de datos. Con estos paquetes, el usuario
puede, por ejemplo, mostrar todas las tablas y gráficos del tipo de una hoja
de cálculo de Microsoft Office Excel Abrir o Apache Open Office (Adler,
2012). Los paquetes por defecto son por lo general todo lo que necesita el
103

www.nitropdf.com

usuario, son muy rica en características. Los nuevos paquetes se pueden
instalar a través de la interfaz gráfica de usuario (GUI) o de la consola.
RStudio presenta un entorno de desarrollo integrado (IDE) para R, que
otorga al usuario un nuevo mundo de la información visual, facilidad de uso
y rendimiento. El uso de R con grandes conjuntos de datos implica algunos
pasos adicionales. Solamente el uso de R no es muy adecuado para el
manejo de grandes cargas de datos, pero la combinación de R con
paquetes personalizados y software adicional hace que sea una
herramienta de análisis muy eficaz para grandes datos (Big Data). PbdR,
que significa ‘la programación con grandes datos en R’, es un conjunto de
paquetes que extiende la funcionalidad básica de R para la ejecución en
paralelo de múltiples núcleos con la introducción de bibliotecas externas a
R. R puede ser también incluido con Hadoop. RHadoop es una colección
de cuatro paquetes de R que permiten a los usuarios gestionar y analizar
datos con Hadoop (Prajapati, 2013).
Apache Pig.
Apache Pig es una plataforma de ejecución y un lenguaje de scripting
(Pig Latín) de alto nivel para el análisis de grandes conjuntos de datos. Se
ejecutan los scripts de Pig sobre los clusters de HDFS y MapReduce, o
localmente en una sola máquina en la que no se requieren HDFS o
MapReduce. Las traducciones del compilador Pig de Pig Latín en una
secuencia de programas MapReduce. Y el alto nivel de paralelización y la
facilidad de uso de Pig Latín, hace que sea muy popular entre los usuarios
de Hadoop. (Pasupuleti, 2014)
104

www.nitropdf.com

Pig Latín permite a los usuarios describir el flujo de datos, es decir,
cómo se deben leer una o más entradas, procesarlas y almacenarlas en
una o más salidas (Gates, 2011). El flujo de datos puede ser de todo, desde
contar una palabra en una línea simple de un conjunto más complejo de
entradas unidas y enviar el flujo de datos divididos para diferentes
operadores. La presencia de los elementos de lenguajes de programación
típicos, como los condicionales, es faltante. Pig Latín se centra en el flujo
de datos, no en el control.
En lugar de utilizar directamente MapReduce, se usa a través de Pig
que proporciona muchas ventajas. El scripts de Pig Latín gestiona los
trabajos de MapReduce y las tres tareas principales involucradas: mapear,
mezclar, y reducir automáticamente. También proporciona todas las
operaciones necesarias en los datos, tales como juntar, ordenar, y unir. Es
posible usar MapReduce por ejemplo para la agrupación (que es hacer
básicamente la mezcla y reducción), pero las tareas más complejas, como
juntar, deben ser programadas por parte del usuario. La Tabla Nº 8
menciona directamente la ‘Filosofía Pig’ (Gates, 2011), señalando algunos
de los aspectos más importantes de Pig. Pig viene con su propia shell,
Grunt, que permite al usuario comunicarse con HDFS directamente desde
la shell. Además, el propio Pig y MapReduce se pueden controlar a través
de Grunt. En su mayor parte, Pig es un lenguaje fuertemente escrito, esto
es, si se define el esquema. Sin esquema definido, la adaptación del tipo
real va a suceder en tiempo de ejecución. Pig Latín es sensible a
mayúsculas y minúsculas. Una de las características más potentes de Pig

105

www.nitropdf.com

es las funciones definidas por el usuario (UDF). Un UDF se puede escribir
en Java o Python (ejecutado en Jython). Los UDF’s creados por el usuario
se almacenan y se coleccionan en Piggybank. No están incluidos en
PigJAR por defecto, por lo que necesitan ser registrados manualmente.
También hay un plugin de Eclipse para Pig, PigPen, que proporciona un
potente IDE (Tiwari, 2011).

Pig toma algo

Pig vive en
cualquier lugar

Pig es un animal
doméstico

Pig puede operar sobre los datos si tiene
metadatos o no. Puede operar con los datos
que son relacionales, jerarquizados, o no
estructurados. Y se puede ampliar fácilmente
para operar en los datos más allá de los
archivos, incluyendo almacenamiento de
clave/valor, bases de datos, etc.
Pig está destinado para ser un lenguaje para
el procesamiento de datos en paralelo. No
está atado a un framework paralelo en
particular. Se ha aplicado por primera vez en
Hadoop, pero no tenemos la intención de que
sea sólo para Hadoop.
Pig está diseñado para ser controlado y
modificado fácilmente por sus usuarios.
Pig permite la integración de código de
usuario siempre que sea posible, por lo que
actualmente es compatible con funciones de
transformación de campo definidas por el
usuario, conjuntos definidos por el usuario, y
condicionales definidos por el usuario. Estas
funciones pueden ser escritas en Java o en
lenguajes de script, que puede compilar bajo
Java (por ejemplo, Jython). Pig es compatible
con funciones de carga y almacenamiento
proporcionados por el usuario. Es compatible
con archivos ejecutables externos a través de
su flujo de comandos y JAR’s MapReduce a
través de su comando mapreduce. Permite
que
los
usuarios
proporcionen
un
particionador personalizado para sus puestos
de trabajo en algunas circunstancias, y para
ajustar el nivel de reducción en el paralelismo
106

www.nitropdf.com

de sus puestos de trabajo.
Pig tiene un optimizador que reorganiza
algunas operaciones en los scripts de Pig
Latín para dar un mejor rendimiento, combina
trabajos MapReduce juntos, etc. Sin embargo,
los usuarios pueden cerrarlo fácilmente este
optimizador para evitar que se realicen
cambios que no tienen sentido en su situación.
Pig procesa datos de forma rápida. Queremos
mejorar constantemente el rendimiento, y no
implementar características en forma pesada
bajo Pig por lo que no puede transportar.

Pig transporta

Tabla Nº 8: Características de Pig.
Fuente: Elaboración propia.

Splunk.
Splunk se inició como un analizador de texto para los registros, pero
ha crecido para ser una plataforma entera. Sus principales funcionalidades
son (ver Tabla Nº 9), (Zadrozny & Kodali, 2013):
Colección de datos

Indexación de datos

Búsqueda y análisis

Tanto la colección de datos estáticos,
seguimiento y la adición de cambios a los
archivos y/o la construcción en tiempo real.
Los datos pueden ser recogidos de los puertos
de red, programas o scripts y bases de datos
relacionales. Otras funciones de RDBMS son
también posibles.
Los datos recogidos son desglosados en
eventos; se procesa y el índice de alta
velocidad se actualiza.
El lenguaje de procesamiento de Splunk
permite al usuario buscar y manipular los
datos para los resultados deseados. Los
resultados pueden ser presentados como
eventos, tablas y gráficos.

Tabla Nº 9: Entidades funcionales de Splunk.
Fuente: Elaboración propia.

107

www.nitropdf.com

Splunk puede ser utilizado a través de una interfaz de usuario basada
en un navegador o utilizando directamente el cliente de línea de comandos.
Splunk puede ser configurado y utilizado muy fácilmente a través de la
interfaz gráfica de usuario. Puede manejar la mayoría de los diferentes
tipos de datos que son lanzados hacia ella, incluyendo diferentes tipos de
archivos de registro, alimentadas por la red, medidas de sistema, datos
estructurados de RDBMS y datos sociales. Inicialmente Splunk tiene que
ser configurado con los diferentes tipos de datos, convirtiéndose cada tipo
en su propia entrada de datos. Los datos pueden ser locales o remotos y
los datos pueden ser cargados dentro de archivos y directorios, fuentes de
red, datos de ventanas, u otros. Splunk puede habilitarse para aceptar la
entrada de un puerto TCP o UDP. Cuando está habilitado, Splunk pondrá
un índice a los datos de entrada de la red, como la información de syslog
que se genera en las máquinas remotas. Splunk también está diseñado
como una infraestructura donde las aplicaciones de terceros y usuarios
pueden escribir sus propios scripts en la parte superior del dispositivo por
defecto, para obtener datos de fuentes que no están incluidos en la
configuración inicial.
Para obtener los datos de todas las instancias y máquinas por
ejemplo, en una empresa, Splunk tiene una configuración especial llamado
reenviadores. Los reenviadores incluyen solamente los componentes
esenciales, y su responsabilidad principal es para enviar datos desde la
máquina principal hacia el indexador principal de Splunk. Utilizar

108

www.nitropdf.com

reenviadores es la mejor práctica para el uso de Splunk. El uso de
reenviadores los usuarios ganan muchos beneficios, tales como:
o Auto guardado en memoria intermedia de datos remotos.
o Soporte de complementos.
o Administración remota.
o Transferencia segura de datos.
o Muy adecuado para la escalabilidad.
Splunk también tiene una plataforma adicional, Hunk, que se ejecuta
en la parte superior de Hadoop. Está construido para explorar rápidamente,
analizar y visualizar los datos en Hadoop. Hunk funciona a la perfección
con YARN y MapReduce. Se centra en hacer el análisis de los datos sin
problemas y orientada a usuarios experimentados. Características tales
como análisis integrados, despliegan rápido y profundo análisis, búsqueda
interactiva, y resultados de pre visualización hacen de Hunk una plataforma
con todas las funciones para Hadoop.
D3.js
D3 es una herramienta de visualización del lado del cliente Javascript
(profunda) para la manipulación de documentos basados sobre datos
(Mike, 2012). En comparación con otras herramientas (actuales) de
análisis, D3 es claramente una solución más ligera y está destinado
únicamente para la visualización de los datos en el navegador web. D3
ayuda enormemente al usuario para analizar los datos; tiene una biblioteca
dinámica e interactiva con cientos de funciones predefinidas para que el

109

www.nitropdf.com

usuario pueda inspeccionar y modificar el modelo de objeto de documento
(DOM) de la página HTML que contiene los datos cargados. Ya que la
presentación se encuentra en una página web, las técnicas de HTML 5 y
CSS son ampliamente utilizados para crear los modelos altamente
visualizados. D3 incluye las siguientes herramientas de presentación
visuales (ver Tabla Nº 10):
Transiciones
Funciones
matemáticas
Arrays
Geometría
Color

La interpolación por ejemplo el color, tamaño,
ubicación, etc.
Transformación 2D, diferentes distribuciones.
Operaciones 3D.
Polígonos,
cortezas convexas,
marcas
cuadradas, etc.
La manipulación de tonos, brillos, etc.
Tabla Nº 10: Herramientas de visualización de D3.
Fuente: Elaboración propia.

Google Prediction API.
Google Prediction API es una herramienta de análisis basada en la
nube que utiliza machine learning para estimar y predecir los resultados de
acuerdo con los datos cargados a la API REST de Google. El usuario no
necesita saber nada acerca de la inteligencia artificial (IA) o machine
learning para poder utilizar las herramientas. Son muy fáciles y sencillas de
usar. Sin embargo, la simplicidad y el enfoque basado en la nube limitan a
la utilidad de Google Prediction API para las consultas más grandes y más
complejos.

110

www.nitropdf.com

Un usuario en primer lugar tiene que cargar los datos de
entrenamiento hacia Google Prediction API (el usuario debe tener una
cuenta de Google y la consola de Google de desarrolladores con Google
Predicción y Google Cloud Storage activada (Tigani & Naidu, 2014)) y
entrenar al sistema de acuerdo con los datos cargados. El formato de los
datos de entrenamiento es una de dos, separados por comas o un archivo
de texto con más columnas y varias filas, donde la primera columna es la
respuesta correcta y las últimas columnas son las características que el
sistema obtiene con su entrenado. Cuando el sistema está capacitado, el
usuario puede empezar a enviar consultas con contenido similar con los
que el sistema se ha entrenado, menos la primera columna.

2.2.9 Cross Industry Standard Process for Data Mining (CRISP-DM).
El autor Moine menciona en su investigación que: “CRISP-DM (Cross
Industry Standard Process for Data Mining) fue presentada en el año 1999 por
las empresas SPSS, Daimer Chrysler y NCR (Chapman, y otros, 2000). Es una
metodología abierta, no está ligada a ningún producto comercial, y fue
construida en base a la experiencia de sus creadores, es decir desde un
enfoque práctico.” (Moine, 2013)
La metodología está estructurada en un proceso jerárquico, compuesto
por tareas descriptas en cuatro niveles diferentes de abstracción, que van
desde lo general a lo específico.

111

www.nitropdf.com

CRISP-DM, propone en el nivel más alto seis fases para el proceso de
minería de datos (ver Figura Nº 15): entendimiento del negocio, entendimiento
de

los

datos,

preparación

de

los

datos,

modelado,

evaluación

e

implementación. La sucesión de fases, no es necesariamente rígida.

Figura Nº 15: Metodología CRISP-DM.
Fuente: Moine (2013).

Cada fase se descompone en un conjunto de tareas genéricas (o
generales) de segundo nivel. Estas tareas son genéricas ya que tratan de
abarcar la mayoría de las situaciones posibles en minería de datos. A partir del
tercer nivel de abstracción, se realiza un “mapeo” de las tareas genéricas
definidas en el modelo a situaciones específicas. De esta forma, las tareas
genéricas se traducen en tareas específicas para casos y proyectos concretos.
En el cuarto nivel, encontramos las instancias de proceso, donde se describen
las acciones, decisiones y resultados de un proyecto particular de minería de
datos.

112

www.nitropdf.com

La metodología proporciona un modelo de referencia y una guía de
usuario. El modelo de referencia presenta un resumen de las fases y tareas a
llevar a cabo en cada una (junto con sus salidas). Es decir, describe “que”
debería hacerse en un proyecto de minería de datos. La guía de usuario
proporciona sugerencias para la ejecución de cada tarea del modelo de
referencia. Analizando el nivel más alto de abstracción del modelo, las seis
fases que componen el proceso de minería de datos son:


Comprensión del negocio: en esta fase se determinan los objetivos y
requerimientos del proyecto desde una perspectiva del negocio, definiendo
el problema de minería y el plan de trabajo.



Comprensión de los datos: fase que consiste en la recolección de datos
que se utilizarán en el proyecto y la familiarización con los mismos. En esta
etapa es posible el surgimiento de las primeras hipótesis acerca de la
información que podría estar oculta.



Preparación de los datos: comprende aquellas actividades de tratamiento
de los datos para construir la vista minable o conjunto de datos final sobre el
cual se aplicarán las técnicas de minería.



Modelado: en esta etapa se aplican las diversas técnicas y algoritmos de
minería sobre el conjunto de datos para obtener la información oculta y los
patrones implícitos en ellos.



Evaluación: fase en la que se analizan los patrones obtenidos en función
de los objetivos organizacionales. En esta etapa se debería determinar si se
ha omitido algún objetivo importante del negocio y si el nuevo conocimiento
será implementado, es decir, si se pasará a la próxima etapa.

113

www.nitropdf.com



Implementación: consiste en la comunicación e implementación del nuevo
conocimiento, el cual debe ser representado de forma entendible para el
usuario.
Cada una de estas fases generales se compone de un conjunto de tareas

en las que se definen las salidas o entregables que se generan. (Moine, 2013)
2.2.10 Métricas y calidad de software
El proceso del software y las métricas del producto son una medida
cuantitativa que permite a la gente del software tener una visión profunda de la
eficacia del proceso del software y de los proyectos que dirigen utilizando el
proceso como un marco de trabajo. Se reúnen los datos básicos de calidad y
productividad. Estos datos son entonces analizados, comparados con
promedios anteriores, y evaluados para determinar las mejoras en la calidad y
productividad.
Las métricas son también utilizadas para señalar áreas con problemas de
manera que se puedan desarrollar los remedios y mejorar el proceso del
software.
El autor Roger Pressman menciona: “La medición es fundamental para
cualquier disciplina de ingeniería, y la ingeniería del software no es una
excepción. La medición nos permite tener una visión más profunda
proporcionando un mecanismo para la evaluación objetiva”. (Pressman, 2002)

114

www.nitropdf.com

Las métricas del software se refieren a un amplio elenco de mediciones
para el software de computadora. La medición se puede aplicar al proceso del
software con el intento de mejorarlo sobre una base continua.
Se puede utilizar en el proyecto del software para ayudar en la
estimación, el control de calidad, la evaluación de productividad y el control de
proyectos. Finalmente, el ingeniero de software puede utilizar la medición para
ayudar a evaluar la calidad de los resultados de trabajos técnicos y para ayudar
en la toma de decisiones táctica a medida que el proyecto evoluciona.
2.2.10.1

Calidad del software.

La calidad del software es una compleja mezcla de factores que variarán
a través de diferentes aplicaciones y según los clientes las pidan.
a) Factores de calidad de McCall
McCall y sus colegas plantearon una categorización de factores que
afectan a la calidad de software, que se muestran en la Figura Nº 16 en
donde se centralizan con tres aspectos importantes de un producto de
software: sus características operativas, su capacidad de cambio y su
adaptabilidad a nuevos entornos. (Pressman, 2002)

115

www.nitropdf.com

Figura Nº 16: Factores de calidad de McCall.
Fuente: Pressman (2002). p.324.

Se definen y emplean un conjunto de métricas para desarrollar
expresiones para todos los factores, de acuerdo con la siguiente
relación:

Donde:

:

Es un factor de calidad.

:

Coeficiente de regresión.

:

Métricas que afectan el factor de calidad.

Las métricas pueden ir en forma de lista de comprobación que se
emplea para <<puntuar>> atributos específicos del software. El
esquema de puntuación propuesto por McCall es una escala del 0 (bajo)

116

www.nitropdf.com

al 10 (alto). Se emplean las siguientes métricas en el esquema de
puntuación:
-

Facilidad de auditoría.

-

Exactitud.

-

Estandarización de comunicaciones.

-

Complexión.

-

Concisión.

-

Consistencia.

-

Estandarización de datos.

-

Tolerancia al error.

-

Eficiencia de ejecución.

-

Capacidad de expansión.

-

Generalidad.

-

Independencia del hardware.

-

Instrumentación.

-

Modularidad.

-

Operatividad.

-

Seguridad.

-

Auto documentación.

-

Simplicidad.

-

Independencia del sistema software.

-

Trazabilidad.

-

Formación.

117

www.nitropdf.com

La relación entre los factores de calidad del software y las métricas de la
lista anterior se muestra en la Tabla Nº 11: Factores y métricas de

Facilidad de auditoria
Exactitud
Estandarización de
comunicaciones

X

Usabilidad

Interoperabilidad

Reusabilidad

Portabilidad

Capacidad de pruebas

Flexibilidad

Integridad

Eficiencia

Factor
de calidad

Fiabilidad

Corrección

Métrica de la
calidad
Del software

Mantenimiento

calidad.

X

X
X

Compleción
Complejidad
Concisión
Consistencia
Estandarización de datos
Tolerancia a errores
Eficiencia de ejecución
Capacidad de expansión
Generalidad
Independencia del
hardware
Instrumentación
Modularidad
Operatividad
Seguridad
Auto documentación
Simplicidad
Independencia del sistema
Trazabilidad
Facilidad de formación

X
X
X
X

X
X

X

X
X
X

X

X
X
X
X
X

X
X

X
X

X

X
X

X
X

X
X

X

X

X

X

X

X
X

X

X
X

X
X

X
X

X

X

X

X

X
X

Tabla Nº 11: Factores y métricas de calidad.
Fuente: Pressman (2002). p. 326.

118

www.nitropdf.com

2.2.10.2

Métricas del modelo de análisis.

El autor Pressman menciona que el trabajo técnico en la ingeniería del
software empieza con la creación del modelo de análisis. En esta fase se
obtienen los requisitos y se establece el fundamento para el diseño. Por tanto,
son deseables las métricas técnicas que proporcionan una visión interna a la
calidad del modelo de análisis.
Métrica de punto de función.
La métrica de punto de función (PF) se puede usar como medio para
predecir el tamaño de un sistema que se va a obtener de un modelo de
análisis. Para instruir el empleo de la métrica PF, se considerará una sencilla
representación del modelo de análisis mostrada por (Pressman, 2002) en la
Figura Nº 17. En donde se representa un diagrama de flujo de datos, de una
función de una aplicación de software llamada Hogar Seguro. La función
administra la interacción con el usurario, aceptando una contraseña de usuario
para activar/ desactivar el sistema y permitiendo consultas sobre el estado de
las zonas de seguridad y varios censores de seguridad. La función muestra una
serie de mensajes de petición y envía señales apropiadas de control a varios
componentes del sistema de seguridad.

119

www.nitropdf.com

Figura Nº 17: Parte del modelo de análisis del software de Hogar Seguro.
Fuente: Pressman (2002). p. 329.

El diagrama de flujo de datos se evalúa para determinar las medidas
clave necesarias para el cálculo de la métrica de PF:
-

Número de entradas de usuario.

-

Número de salidas de usuario.

-

Número de consultas del usuario.

-

Número de archivos.

-

Número de interfaces externas.

2.2.10.3

Métricas del modelo de diseño.

Las métricas para software, como otras métricas, no son perfectas;
muchos expertos argumentan que se necesita más experimentación hasta que
se puedan emplear bien las métricas de diseño. Sin embargo el diseño sin
medición es una alternativa inaceptable.
Métrica de diseño de alto nivel.

120

www.nitropdf.com

Ésta se concentra en las características de la estructura del programa
dándole énfasis a la estructura arquitectónica y en la eficiencia de los módulos.
Estas métricas son de caja negra, en el sentido de que no se requiere
ningún conocimiento del trabajo interno de ningún modo en particular del
sistema.
Card y Glass (Pressman, 2002) proponen tres medidas de complejidad
del software: complejidad estructural, complejidad de datos y complejidad del
sistema.

La complejidad estructural

, de un módulo

se define de la siguiente

manera.

Donde

es la expansión del módulo .

La complejidad de datos

, proporciona una indicación de la

complejidad en la interfaz interna de un módulo y se define como:

Donde

es el número de variables de entrada y salida del módulo .

Finalmente la complejidad del sistema

, se define como la suma de

las complejidades estructural y de datos, y se define como:

121

www.nitropdf.com

A medida que crecen los valores de complejidad, la complejidad
arquitectónica o global del sistema también aumenta. Esto lleva a una mayor
probabilidad de que aumente el esfuerzo necesario para la integración y las
pruebas.
2.2.10.4

Métricas de código fuente.

Pressman menciona que la ciencia del software asigna leyes cuantitativas
al desarrollo del software de computadora, usando un conjunto de medidas
primitivas que pueden obtenerse una vez que se ha generado o estimado el
código después de completar el diseño. (Pressman, 2002)
Estas medidas se listan a continuación.

El número de operadores diferentes que aparecen en el programa.

El número de operandos diferentes que aparecen en el programa.

El número total de veces que aparece el operador.

El número total de veces que aparece el operando.
Para Pressman, Halstead usa las medidas primitivas para desarrollar
expresiones para la longitud global del programa; volumen mínimo potencial
para un algoritmo; el volumen real (número de bits requeridos para especificar
un programa); el nivel del programa (una medida de la complejidad del

122

www.nitropdf.com

software); nivel del lenguaje (una constante para un lenguaje dado); y otras
características tales como esfuerzo de desarrollo, tiempo de desarrollo e
incluso el número esperado de fallos en el software.

La longitud

se puede estimar como:

y el volumen de programa se puede definir como:

Teóricamente, debe existir un volumen mínimo para un algoritmo.
Halstead define una relación de volumen

como la relación de volumen de la

forma más compacta de un programa con respecto al volumen real del
programa. Por tanto,

debe ser siempre menor de uno. En términos de

medidas primitivas, la relación de volumen se puede expresar como:

2.3


Glosario de Términos Básicos
Sistema.
Es

un

conjunto

de

elementos

organizados,

interrelacionados

que

interactúan entre sí para lograr un objetivo en común.


Información.
123

www.nitropdf.com

La información es un conjunto de datos, organizados y relacionados, que
generan valor agregado con una buena manipulación del mismo.


Tecnologías de la información y la comunicación (TIC).
Las TIC conforman el conjunto de recursos necesarios para manipular la
información: los ordenadores, los programas informáticos y las redes
necesarias para convertirla, almacenarla, administrarla, transmitirla y
encontrarla.



Base de datos.
Es una colección de información organizada, es decir una colección de
datos interrelacionados.



NoSQL.
Es un sistema de gestión de bases que no utiliza SQL como el principal
lenguaje de consultas. Los datos almacenados no requieren estructuras
fijas como tablas, normalmente no soportan operaciones JOIN, ni
garantizan completamente ACID (atomicidad, consistencia, aislamiento y
durabilidad), y habitualmente escalan bien horizontalmente.



Proceso ETL.
Es el movimiento y transformación de datos. Se trata del proceso que
permite a las organizaciones mover datos desde múltiples fuentes,
reformatearlos y cargarlos en otra base de datos (denominada data mart o
data warehouse) con el objeto de analizarlos.



MapReduce.
Es un modelo de programación utilizado por Google para dar soporte a la
computación paralela sobre grandes colecciones de datos en grupos de
computadoras.
124

www.nitropdf.com



Procesamiento

de

Transacciones

en

Línea

(OnLine

Transaction

Processing).
Es un tipo de procesamiento que facilita y administra aplicaciones
transaccionales, usualmente para entrada de datos y recuperación y
procesamiento de transacciones (gestor transaccional). Los paquetes de
software para OLTP se basan en la arquitectura cliente-servidor ya que
suelen ser utilizados por empresas con una red informática distribuida.


Procesamiento analítico en línea (On-Line Analytical Processing).
Es una solución utilizada en el campo de la llamada Inteligencia empresarial
(o Business Intelligence) cuyo objetivo es agilizar la consulta de grandes
cantidades de datos. Para ello utiliza estructuras multidimensionales (o
Cubos OLAP) que contienen datos resumidos de grandes Bases de datos o
Sistemas Transaccionales (OLTP).



Interfaz de programación de aplicaciones (API).
Es el conjunto de funciones y procedimientos (o métodos, en la
programación orientada a objetos) que ofrece cierta biblioteca para ser
utilizado por otro software como una capa de abstracción. Son usadas
generalmente en las bibliotecas.



Nodo.
Un nodo es un punto de intersección, conexión o unión de varios elementos
que confluyen en el mismo lugar.



Clúster.
Un clúster es un grupo de múltiples ordenadores unidos mediante una red
de alta velocidad, de tal forma que el conjunto es visto como un único
ordenador, más potente que los comunes de escritorio.
125

www.nitropdf.com



Computación paralela.
Es una forma de cómputo en la que muchas instrucciones se ejecutan
simultáneamente, operando sobre el principio de que problemas grandes, a
menudo se pueden dividir en unos más pequeños, que luego son resueltos
simultáneamente (en paralelo).



Inteligencia artificial (IA).
Es un área multidisciplinaria que, a través de ciencias como la informática,
la lógica y la filosofía, estudia la creación y diseño de entidades capaces de
resolver cuestiones por sí mismas utilizando como paradigma la inteligencia
humana.



Inteligencia de Negocios (Business Intelligence).
Es la combinación de tecnología, herramientas y procesos que me permiten
transformar los datos almacenados en información, esta información en
conocimiento y este conocimiento dirigido a un plan o una estrategia
comercial.



Análisis de Negocio (Business Analytics, BA).
El análisis de negocio es el conjunto de métodos y técnicas utilizadas para
trabajar como enlace entre los stackeholders, con el fin de comprender la
estructura, políticas y operaciones de una organización y recomendar
soluciones que permitan a la organización alcanzar sus objetivos.



Aprendizaje automático (Machine learning).
Es una rama de la inteligencia artificial cuyo objetivo es desarrollar técnicas
que permitan a las computadoras aprender, se trata de crear programas
capaces de generalizar comportamientos a partir de una información no
estructurada suministrada en forma de ejemplos. Sin embargo, el
126

www.nitropdf.com

aprendizaje automático se centra más en el estudio de la complejidad
computacional de los problemas.


Data Warehouse.
Es una base de datos corporativa que se caracteriza por integrar y depurar
información de una o más fuentes distintas, para luego procesarla
permitiendo su análisis desde infinidad de perspectivas y con grandes
velocidades de respuesta.



Data mart.
Es una base de datos departamental, especializada en el almacenamiento
de los datos de un área de negocio específica. Se caracteriza por disponer
la estructura óptima de datos para analizar la información al detalle desde
todas las perspectivas que afecten a los procesos de dicho departamento.
Un data mart puede ser alimentado desde los datos de un data warehouse,
o integrar por sí mismo un compendio de distintas fuentes de información.



Data mining (Minería de datos).
Es el conjunto de técnicas y tecnologías que permiten explorar grandes
bases de datos, de manera automática o semiautomática, con el objetivo de
encontrar patrones repetitivos, tendencias o reglas que expliquen el
comportamiento de los datos en un determinado contexto.



Data center.
Un data center (centro de cómputos, centro de proceso de datos), es una
instalación empleada para albergar los sistemas de información y sus
componentes asociados, como las telecomunicaciones y los sistemas de
almacenamiento.



JavaScript.
127

www.nitropdf.com

Es un lenguaje de programación orientado a objetos para la realización de
cálculos y manipular objetos computacionales en un entorno.


JQuery.
Es una biblioteca e JavaScript rápida y concisa que simplifica el recorrido
del documento HTML, manejo de eventos, animaciones y las interacciones
para el desarrollo web más rápido.



JavaScript Object Notation.
La notación de objetos JavaScript (JavaScript Object Notation), es un
formato ligero para el intercambio de datos.



Navegador web.
Programa que se utiliza para acceder a la web para interpretar los lenguajes
como HTML, CSS y JavaScript.



Servidor web.
Es un programa que implementa el protocolo HTTP para transferir lo que
llamamos hipertextos, páginas web o paginas HTML.



MySQL.
MySQL es un sistema de gestión de bases de datos relacional, multihilo y
multiusuario. MySQL es una base de datos muy rápida en la lectura cuando
utiliza el motor no transaccional MyISAM, pero puede provocar problemas
de integridad en entornos de alta concurrencia en la modificación.



Framework.
Es una

estructura

conceptual

y tecnológica de

soporte

definido,

normalmente con artefactos o módulos de software concretos, que puede
servir de base para la organización y desarrollo de software.
128

www.nitropdf.com



Open Source
Es la expresión con la que se conoce al software distribuido y desarrollado
libremente.



Programación orientada a objetos
Es un paradigma de programación que usa los objetos en sus interacciones,
para diseñar aplicaciones y programas informáticos.

129

www.nitropdf.com

2.4

Hipótesis de la Investigación

2.4.1 Hipótesis general.
El análisis de datos con herramientas de Big Data apoya a la toma de
decisiones en la Unidad de Gestión Educativa Local de Azángaro.
2.4.2 Hipótesis específicas.
-

El análisis de datos, identifica la correlación de los datos para la toma de
decisiones.

-

El análisis de datos, almacena datos históricos para la toma de
decisiones.

-

El análisis de datos, procesa los datos para la toma de decisiones.

-

El análisis de datos, visualiza los datos para la toma de decisiones.

2.5

Operacionalización de Variables
-

Variable independiente: Análisis de datos con herramientas de Big Data.

-

Variable dependiente: Toma de decisiones en la UGEL de Azángaro.

130

www.nitropdf.com

Variables

Dimensiones

Indicadores

Grado de
Comprensión del
conocimiento del
negocio.
negocio.

Análisis de Datos
con herramientas
de Big Data.

Toma de
decisiones en la
UGEL de
Azángaro.

Técnica
Métricas de
software

Preparación de
los datos.

Recolección de
Nivel de integración de
datos mediante
los datos.
test y encuestas.

Modelado del
análisis.

Construcción del
modelo.

Métricas de
software

Evaluación del
análisis.

Revisión del proceso.

Métricas de
software

Tiempo
administrado en
la toma de
decisiones

Comparación del
tiempo en el cual se
tomaban las
decisiones.

Entrevista a partir
del uso de la
aplicación.

Apoyo en la
toma de
decisiones

Análisis de datos y
modelado de
decisiones.

Aplicación de la
escala de likert
para obtener
resultados.

Ayuda en el proceso
de toma de
decisiones.

Prueba o Test
para medir la
eficiencia en la
toma de
decisiones.

Eficiencia en la
toma de
decisiones

Tabla Nº 12: Operacionalización de variables.
Fuente: Elaboración propia.

131

www.nitropdf.com

CAPÍTULO III
DISEÑO METODOLÓGICO DE
INVESTIGACIÓN

132

www.nitropdf.com

3.1

Tipo y Diseño de la Investigación
Esta investigación corresponde a una investigación experimental donde

se manipula intencionalmente el análisis de datos con herramientas de Big
Data del sistema de gestión de los documentos administrativos (SGDA) para el
apoyo en la toma de decisiones.
El diseño de la investigación corresponde al tipo cuasi experimental.

Tratamiento

Experimento

Grupo experimental
Tabla Nº 13: Diseño de la investigación.

Dónde:

: Análisis de datos con herramientas de Big Data del SGDA para el

apoyo en la toma de decisiones.

Cuestionario de opinión sobre el análisis de datos con herramientas de

Big Data del SGDA para el apoyo en la toma de decisiones.

3.2

Población y Muestra de la Investigación

3.2.1 Población.
La población estudiada está conformado por las personas que presentan
los documentos en mesa de partes de la institución, personal docente, personal

133

www.nitropdf.com

administrativo, administradores del SGDA y usuarios de la solución del análisis
de datos con herramientas de Big Data del SGDA.
Afectados por el Frecuencia
algoritmo
absoluta
Usuarios de la
solución,
administradores
del
SGDA,
personas
2065
remitentes,
personal
administrativo
y
docente.

Frecuencia
relativa

Frecuencia
porcentual

1.00

100%

Tabla Nº 14: Población de afectados por el algoritmo de análisis de datos.

3.2.2 Muestra.
La selección de muestra es del tipo no probabilístico donde se utilizó el
método de muestreo por conveniencia, este tipo de muestreo se caracteriza
por obtener muestras accesibles representativas. Por tanto se tomó como
muestra al personal administrativo y los usuarios de la solución del análisis de
datos con herramientas de Big Data del SGDA.
Usuarios
del Frecuencia
algoritmo
absoluta.
Personal
administrativo
y
usuarios de la 40
solución
del
análisis de datos.

Frecuencia
relativa

Frecuencia
porcentual

1.00

100%

Tabla Nº 15: Muestra de usuarios del algoritmo.

3.3

Ámbito de Estudio
El estudio se realizó en las distintas áreas que hay en la Unidad de

Gestión Educativa Local de Azángaro (ver Figura Nº 18).
134

www.nitropdf.com

DIRECCION

COPALE

Área de
Auditoria Interna

Área de
Asesoría
Jurídica

Área de
Administración

Área de
Gestión
Pedagógica

Instituciones y
Programas
Educativos

Área de
Gestión
Institucional

IEI
IES
Centro
Base
IEP
Programa
s

Figura Nº 18: Organigrama de la UGEL Azángaro.
Fuente: UGEL-Azángaro.

135

www.nitropdf.com

3.4

Material experimental
El material experimental que se utilizó para realizar el desarrollo de la

solución son los siguientes:
Hardware
-

Computadora personal Toshiba satélite C845, Intel Core i3, 8 GB
memoria RAM.

-

Memoria flash marca hp 2.0.

-

Impresora láser HP 15540.

Software
-

Sistema operativo Microsoft Windows 8

-

Microsoft Office 2010.

-

Microsoft Excel 2010.

-

Microsoft Project Profesional 2010

-

Microsoft Hyper - V.

-

Sistema operativo Ubuntu Server.

-

Hadoop

-

R

-

Navegadores: Google chrome, Mozilla Firefox, Opera, Microsoft internet
explorer.

Materiales de Escritorio
-

Pizarra acrílica.

-

Plumones.

136

www.nitropdf.com

-

Papel boom.

-

Lapicero.

-

Lápiz.

-

Folder Manila.

-

CD.

-

Marcador de CD.

Servicios
-

Conexión a internet.

-

Llamadas telefónicas.

3.5

Técnicas e instrumentos para recolectar información
Las técnicas que se utilizaron fueron dos:
-

Observación directa: durante la ejecución del experimento se tuvo una
lista de cotejo, el mismo que permitió valorar los progresos de la toma de
decisiones.

-

La encuesta: Se realizó al número de muestra a uno por uno sin tomar
los datos personales ver Anexo Nº 5, Nº 6 y Nº 7.

3.6

Técnicas para el procesamiento y análisis de datos
Para el tratamiento de datos se utilizaron lo siguiente:
-

Tabla de distribución de frecuencia.

137

www.nitropdf.com

-

Gráfico de barras.

-

Desviación estándar.

-

Distribución .

Los datos recopilados fueron tabulados en hojas de cálculo electrónico y
para su representación se utilizara gráficos estadísticos de barras.

3.7

Procedimiento del experimento
Se utilizó la metodología Cross Industry Standard Process for Data Mining

(CRISP-DM), ya que es una de las metodologías más utilizadas en minería de
datos pero sobre todo en Business Analytics para usarlo eficientemente en Big
Data.
La metodología comprende las siguientes fases:
3.7.1 Comprensión del Negocio
La primera fase de la metodología CRISP-DM es llamada "Comprensión
del negocio", la cual propone la realización de cuatro tareas: determinar
objetivos del negocio, evaluar la situación, determinar objetivos de la minería
de datos y crear un plan del proyecto. El resultado de cada tarea se representa
a través de las salidas o entregables.
Tarea 1. Determinar los objetivos del negocio.
Background

138

www.nitropdf.com

La UGEL de Azángaro recibe diariamente una gran cantidad de clientes,
los cuales asisten durante el día. En la institución trabajan 65 administrativos
(cerca de 2000 docentes trabajan en todas las Instituciones Educativas que
dependen de la UGEL de Azángaro) de lunes a viernes, desde las 8:00 hs.
hasta las 12:00 hs. en la mañana y de las 14:00 hs. hasta las 16:00 hs. en la
tarde. La Figura Nº 18 representa la estructura de la organización.
Entre las personas clave de la organización encontramos:
 La encargada(o) de la oficina de trámite documentario: persona
encargada(o) de la recepción de los documentos presentados a la
institución mediante el uso del sistema de gestión de los documentos
administrativos (SGDA).
 Directores y/o Jefes de cada área (Dirección general, Área de gestión
pedagógica, Área de gestión institucional, Área de administración, Área
de administración de personal, Control interno, Área de asesoría
jurídica, Área de sistema, Comisión de procesos administrativos).
El director de administración de la UGEL de Azángaro estimo que
aproximadamente el 20% de las áreas no están sistematizadas, ya que los
documentos presentados en la oficina de trámite documentario no son
atendidos oportunamente (por el transcurso del tiempo), lo que provoca que los
interesados de los documentos no puedan obtener una respuesta oportuna y
adecuada.
Los integrantes del nivel gerencial (directores y jefes encargados) tienen
poco conocimiento en materia de análisis de datos, pero tienen presente que

139

www.nitropdf.com

hay herramientas que pueden colaborar a la solución de su problema. Sus
expectativas se centran en mejorar el servicio al cliente, pudiéndole ofrecer un
tiempo menor de espera a la atención de sus documentos presentados. Una
forma de llevar a cabo este objetivo es detectando en qué áreas se presentan
mayor documentación, para poder sistematizar dicha área y mejorar el tiempo
de atención. Actualmente la solución que se utiliza para la atención a los
clientes es mediante herramientas de ofimática (Microsoft Office). Esta solución
por lo general no es efectiva, ya que la cantidad de atención de documentos
presentados depende del tiempo de respuesta a cada una de ellas, provocando
la saturación de la documentación en el área. Si bien como ventaja genera la
atención inmediata al cliente, provoca la queja de los encargados porque deben
quedarse más tiempo, fuera de su horario laboral, y trabajar con apuros.
Objetivos del negocio
El proyecto de análisis de datos con herramientas de Big Data tiene como
objetivo apoyar a la toma de decisiones en la UGEL de Azángaro y poder
mejorar la gestión de los documentos en cada área, donde han sido
recepcionados y derivados por la oficina de trámite documentario de la
institución, ya que brindó información acerca de qué tipo de documentos son
presentados con mayor frecuencia y en qué áreas. Con esa información se
pudo tomar acciones preventivas, así como la sistematización de aquellas
áreas con alta probabilidad de atención a un tipo de documento.
Desde una perspectiva de negocio se obtuvo:

140

www.nitropdf.com

 Conocer aquellas áreas donde se presentan mayor documentación y el
tipo de documentación presentada.
 Reducir el tiempo de espera para la atención a un documento
presentado, mejorando el tiempo de atención por parte del encargado
de área.
 Ofrecer una respuesta oportuna y adecuada a los clientes lo antes
posible, sin sobrecargar el horario de trabajo del encargado.
Criterios de éxito
Se consideró como criterio de éxito para el proyecto de investigación, el
conocimiento de las áreas donde se presentaban mayor documentación para
así poder realizar la reducción del tiempo de atención al cliente y la reducción
de finalización de un documento en un 50%. Este criterio fue evaluado por el
sector gerencial de la organización (directores y jefes encargados).
Tarea 2. Evaluación de la situación.
Inventario de recursos
La Tabla Nº 16 muestra el inventario de recursos que se utilizó en el
proyecto de investigación.
Recursos de hardware
Recursos de software

Recursos de datos y
conocimiento

Un notebook, con procesador i3 y 8 GB de RAM.
Se trabajara con las siguientes herramientas.
 Virtualizador de maquina Hyper-V.
 Sistema operativo Ubuntu server 14.04 LTS.
 Framework Hadoop 2.6
 Sqoop 1.99.5
 R-Studio
Se utilizara la base de datos del sistema
operacional de la institución (SGDA), la cual corre
sobre una plataforma MySQL.
141

www.nitropdf.com

Recursos humanos

Un analista de explotación de información.
Tabla Nº 16: Inventario de recursos.
Fuente: Elaboración propia.

Requerimientos, supuestos y restricciones
Requerimientos
 Mantener la confidencialidad de los datos de clientes e instituciones.
 Los resultados del proceso deben estar representados de una forma
clara, simple y entendible para los usuarios.
Supuestos
 La descripción del tipo de documento en la base de datos son reales.
 Las fuentes de datos están libres de errores y son accesibles en todo
momento.
 Se cuenta con la colaboración del personal gerencial en todo momento.
Restricciones
 El proyecto de investigación se limitó a la utilización de los recursos
citados en el inventario, y no requirió insumos extra.
Gestión del riesgo
El impacto de un riesgo puede ser alto/medio/bajo como se muestra en la
Tabla Nº 17.
Prioridad Descripción del riesgo
Probabilidad
1
Los patrones encontrados no logran
0.5
satisfacer los objetivos del proyecto.
2
El análisis exploratorio de los datos
0.5
indica baja calidad de los mismos,

Impacto
Alto
Alto

142

www.nitropdf.com

3
4

dificultando la aplicación de técnicas
de minería.
Los patrones encontrados no son
entendibles por la gerencia.
La gerencia no participa activamente
en el proyecto.

0.4

Medio

0.3

Medio

Tabla Nº 17: Lista de riesgos del proyecto.
Fuente: Elaboración propia.

Planes de contingencia
La Tabla Nº 18 muestra el plan de contingencia para el proyecto de
investigación.

Riesgo

Acciones de contingencia
Medidas proactivas
Medidas reactivas

143

www.nitropdf.com

 Utilizar
distintas
técnicas de minería
en
la
fase
de
modelado.
 Utilizar
distintos
parámetros para los
modelos obtenidos.
El análisis exploratorio de los  Recolectar datos que
datos indica baja calidad de los
representen lo mejor
mismos, dificultando la aplicación
posible la realidad
de técnicas de minería.
del problema.
Los patrones encontrados no
logran satisfacer los objetivos del
proyecto.

Los patrones encontrados no son  Capacitar al personal
entendibles por la gerencia.
en análisis de datos
y de resultados.
La
gerencia
no
participa  Señalar a la gerencia
activamente en el proyecto.
la importancia de su
compromiso desde
etapas
tempranas
del proyecto.
 Informar
permanentemente a
la gerencia acerca de
los
avances
del
proyecto.
 Escuchar todas sus
sugerencias
para
que se sientan parte
del mismo.

 Recolectar mayor
cantidad de datos.

 Buscar
otras
fuentes de datos.
 Recolectar mayor
cantidad de datos
(variables
u
observaciones
según el caso).
 Armar
nuevas
representaciones
de los patrones
encontrados.
 Determinar
las
causas de la falta
de interés y trabajar
sobre las mismas.

Tabla Nº 18: Plan de contingencia.
Fuente: Elaboración propia.

Glosarios
Terminología del negocio:

144

www.nitropdf.com

 Documento (o expediente): es un documento presentado en la oficina de
trámite documentario, que un cliente o entidad dirige hacia la UGEL de
Azángaro para obtener una respuesta o dar a conocer sobre una
situación.
 Tipo de documento: representa el tipo de documento que es presentado
en la oficina de trámite documentario: Oficios, Solicitudes, Informes,
Directivas, Memorándums, Notificaciones, Carta múltiples, etc.
Terminología de minería de datos:
 Métodos/técnicas de minería de datos: Son herramientas utilizadas para
buscar los patrones. Un método puede ser descriptivo o predictivo.
o Métodos

descriptivos:

Proporciona

información

sobre

las

relaciones existentes entre los datos. Exploran las propiedades
de los datos.
o Métodos predictivos: Responden a preguntas sobre datos futuros.
Permiten estimar el valor futuro de variables “dependientes” (o
explicadas), a partir de otras llamadas “independientes” (o
explicativas).
 Vista minable: Estructura final de los datos, con formato tabular, que se
proporciona como entrada a los algoritmos de minería. Las filas
representan las observaciones y las columnas las variables de estudio.
 Datos de entrenamiento/prueba: Los datos de entrenamiento son
aquellos que se utilizan para armar el modelo de minería, mientras que
los de prueba se utilizan para testear la calidad del mismo.

145

www.nitropdf.com

 Sistemas de ficheros distribuidos: Los sistemas de ficheros son una
parte fundamental de la arquitectura Big Data ya que es por encima de
ellos que el resto de herramientas están construidas.
 MapReduce: El objetivo de MapReduce es el de mejorar el
procesamiento de grandes volúmenes de datos en sistemas distribuidos
y está especialmente pensado para tratar ficheros de gran tamaño (del
orden de gigabytes y terabytes). El nombre viene dado por la
arquitectura del modelo, dividida principalmente en dos fases que se
ejecutan en una infraestructura formada por varios nodos, formando un
sistema distribuido, y que procede de la siguiente manera:
o Map: Uno de los nodos, con el rango de “master”, se encarga de
dividir los datos de entrada (uno o varios ficheros de gran
tamaño) en varios bloques a ser tratados en paralelo por los
nodos de tipo “worker map”. Cada bloque es procesado
independientemente del resto por un proceso que ejecuta una
función map. Esta función tiene el objetivo de realizar el
procesamiento de los datos y dejar los resultados en una lista de
pares clave-valor (es decir, se encarga de “mapear” los datos).

o Reduce: Los nodos “worker” de tipo reduce ejecutan una función
reduce que recibe como entrada una de las claves generadas en
la etapa de “map” junto con una lista de los valores
correspondientes a esa clave. Como salida genera una lista
resultante de una función con los valores recibidos. La unión de
146

www.nitropdf.com

los resultados puede corresponder a cualquier tipo de función
(agregación, suma, máximo, etc.).

Análisis de costo/beneficio
En este caso, como el proyecto de análisis de datos con herramientas de
Big Data se desarrolló en un contexto de trabajo de investigación, por lo cual no
se realizó un análisis costo/beneficio.
Tarea 3. Determinar los objetivos de la minería de datos.
Objetivos de la minería de datos
Se trabajó con un modelo de programación introducido por Google
llamada MapReduce y que en su evolución han participado decenas de
colaboradores, apareciendo multitudes de implementaciones. De entre todas
esas implementaciones destaca especialmente Hadoop, un proyecto de
Apache para proporcionar una base sólida a las arquitecturas y herramientas
Big Data, ya que este modelo permitió describir en qué áreas se presentaban
mayor documentación y de que tipos.
También se trabajó con algoritmos de clasificación (como arboles de
decisión o regresión logística) para construir un modelo predictivo, que permitió
estimar si un tipo de documento se finalizará.
Criterio de éxito para el proyecto de análisis de datos con
herramientas de Big Data

147

www.nitropdf.com

Se esperó que la capacidad de análisis del modelo MapReduce y el
modelo predictivo fuera buena; para la reducción del tiempo de atención al
cliente a un 50% con la ayuda del modelo MapReduce y con una tasa de
acierto de al menos el 60% en el modelo predictivo.
Tarea 4. Crear el plan para el proyecto de análisis de datos con
herramientas de Big Data.
Plan del proyecto
La duración estimada para la ejecución del proyecto de investigación fue
de 14 semanas. Se esperó que la fase de evaluación del modelo fuera exitosa,
y no requiera retroceder a etapas anteriores. El único recurso humano que
trabajó en el proyecto fue un analista, 20 hs semanales. En la Figura Nº 20 se
representa el cronograma del proyecto. Se puede observar que contempla una
interacción entre la fase de preparación de los datos y el modelado.
Evaluación inicial de técnicas y herramientas
El proyecto se llevó a cabo con herramientas de software libre. Para la
fase de análisis de datos y modelado se utilizaron herramientas de Big Data,
las herramientas candidatas fueron:
 MapReduce:
o Hadoop, Hive, Pig, Cascading, Cascalog, mrjob, Caffeine, S4,
MapR, Acunu, Flume, Kafka, Azkaban, Oozie, Sqoop, etc.

 Almacenamiento:

148

www.nitropdf.com

o S3, Hadoop Distributed File System, etc.
 Procesamiento
o R, Yahoo! Pipes, Mechanical Turk, Solr/Lucene, ElasticSearch,
Datameer, BigSheets, Tinkerpop, etc.
Para la selección de las técnicas de clasificación que se utilizaron, resultó
importante ver la naturaleza de los datos. Como en el análisis las variables
explicativas son mixtas (numéricas y cualitativas) y la explicada es binaria
(Finalizado/No finalizado), las técnicas candidatas fueron:
 Arboles de decisión, Regresión logística, Clasificador Naive Bayes, y el
Método de vecino más próximo (KNN).
Para la importación de datos de la base de datos se utilizó la herramienta
Apache Sqoop (ver Figura Nº 19 de la Arquitectura de funcionamiento).

Figura Nº 19: Arquitectura de Apache Sqoop.
Fuente: Elaboración propia.

149

www.nitropdf.com

150

www.nitropdf.com
Elaboración propia.

Figura Nº 20: Cronograma del proyecto de investigación.

3.7.2 Comprensión de los datos
Tarea 1. Recolectar los datos iniciales.
Reporte inicial de recolección de datos
Los datos fueron extraídos de la base de datos del sistema de gestión de
los documentos administrativos (SGDA). El periodo en estudio fue desde el
01/01/2013 al 31/12/2014.
Las unidades observacionales del problema son aquellas áreas que
presentan mayor presentación de documentación, de acuerdo a un tipo de
documento y descripción del mismo.
No se requirió en esta etapa la integración de distintas fuentes, ya que
toda la información reside en esta base de datos.
La estructura de la base de datos contiene 12 tablas. Sólo 5 de ellas (las
tablas de expedientes, seguimientos, parámetros, oficinas y áreas) contienen
información relevante para el problema en estudio (ver Figura Nº 21).
En cuanto al proceso de presentación de un documento es el siguiente:
Camino básico
1. El cliente presenta el documento al encargado(a) de la oficina de tramite
documentario para su recepción.
2. El encargado(a) registra el documento en el sistema, el cual le asigna un
número de expediente al documento.

151

www.nitropdf.com

3. El cliente recibe el cargo de recepción con el número de expediente
asignado.
Cuando llega el día de la consulta
4. El cliente informa su llegada al encargado(a) de la oficina de tramite
documentario.
5. El encargado(a) revisa el sistema y le informa al cliente en qué oficina y
área se encuentra el documento.
6. El cliente informa su llegada al encargado(a) de la oficina en donde se
encuentra el documento.
7. El encargado(a) de la oficina donde se encuentra el documento llama al
cliente para entregarle la respuesta adecuada.

Caminos alternativos
7. El encargado(a) de la oficina donde se encuentra el documento llama al
cliente para decirle que aún no ha sido procesado su petición del
documento presentado.
a. El encargado(a) propone un nuevo día y hora para la atención del
documento presentado.
8. El cliente tiene que regresar otro día. El tiempo de atención se extiende.

152

www.nitropdf.com

El proyecto de análisis de datos con herramientas de Big Data apunta a
mejorar el camino alternativo del octavo paso en el proceso (el cliente tiene que
regresar otro día).

Figura Nº 21: Estructura de las tablas (expedientes, seguimientos, parametros, oficinas y
áreas).
Fuente: Elaboración propia.

Tarea 2. Describir los datos.
Reporte de descripción de los datos

153

www.nitropdf.com

Para obtener la información de la base de datos, se han ejecutado
consultas SQL dentro de la herramienta Apache Sqoop (mediante la creación
de jobs) para importarlos y almacenarlos dentro del HDFS (Hadoop Distributed
File System):

154

www.nitropdf.com

Para listar las Oficinas (ver Figura Nº 22).

Figura Nº 22: Consulta SQL utilizada para extraer la lista de Oficinas.
Fuente: Elaboración propia.

La consulta arrojó como resultado 58 registros (oficinas y sus áreas), de
los cuales se obtuvo las siguientes columnas (ver Tabla Nº 19 para la
descripción de cada una de las variables extraídas):
 idOficina: Código de la oficina para la cual se presenta un determinado
documento.
 descripcion: Descripción o nombre de la oficina.
 idArea: Código del área al cual pertenece una determinada oficina.
 abreviacion: Abreviación o siglas del área.
Atributo
idOficina

Tipo
INT(11)

descripcion

VARCHAR(255)

idArea
abreviacion

INT(11)
VARCHAR(10)

Valores posibles
1, 2, 3,… 96,… etc.
Director, Técnico en Actas Certif. y
Archivo, Imagen Institucional, etc.
1, 2, 3,… 10,… etc.
DIR, AGP,AGI, ADM, etc.

Tabla Nº 19: Descripción de las variables extraídas para listar las Oficinas.
Fuente: Elaboración propia.

155

www.nitropdf.com

Para listar los Tipos de documentos (ver Figura Nº 23).

Figura Nº 23: Consulta SQL utilizada para extraer la lista de Tipos de documentos.
Fuente: Elaboración propia.

La consulta arrojó como resultado 32 registros (tipos de documentos), de
los cuales se obtuvo las siguientes columnas (ver Tabla Nº 20 para la
descripción de cada una de las variables extraídas):
 codigo: Código del tipo de documento.
 valor: Valor para el código del tipo de documento (descripción).
Atributo
Código

Tipo
VARCHAR(50)

Valor

VARCHAR(255)

Valores posibles
1, 2, 3,…36.
SOLICITUD, OFICIO, OFICIO
CIRCULAR, OFICIO MULTIPLE, etc.

Tabla Nº 20: Descripción de las variables extraídas para listar los Tipos de documentos.
Fuente: Elaboración propia.

Para listar los Tipos de documentos que se encuentran en cada
Oficina (ver Figura Nº 24).

Figura Nº 24: Consulta SQL utilizada para extraer la lista de Tipos de documentos que se
encuentran en cada Oficina.
Fuente: Elaboración propia.

156

www.nitropdf.com

La consulta arrojó como resultado 30,939 registros (tipos de documentos
que se encuentran en cada oficina), de los cuales se obtuvo las siguientes
columnas (ver Tabla Nº 21 para la descripción de cada una de las variables
extraídas):
 idOficinaDestino: Código de la oficina donde se encuentra el documento.
 tipoDocumento: Código del tipo de documento.
Atributo
idOficinaDestino
tipoDocumento

Tipo
INT(11)
INT(11)

Valores posibles
1, 2, 3,… 96,… etc.
1, 2, 3,…36,… etc.

Tabla Nº 21: Descripción de las variables extraídas para listar los Tipos de documentos que
se encuentran en cada Oficina.
Fuente: Elaboración propia.

Para listar los Documentos que se presentan más (ver Figura Nº 25).

Figura Nº 25: Consulta SQL utilizada para extraer la lista de los Documentos que se
presentan más.
Fuente: Elaboración propia.

La consulta arrojó como resultado 30,940 registros (documentos que se
presentan más), de los cuales se obtuvo las siguientes columnas (ver Tabla Nº
22 para la descripción de cada una de las variables extraídas):
 tipoDocumento: Código del tipo de documento.

157

www.nitropdf.com

 anoCreacion: Año de creación del documento.
 mesCreacion: Mes del año de creación del documento.
Atributo
tipoDocumento
anoCreacion
mesCreacion

Tipo
INT(11)
INT(11)
INT(11)

Valores posibles
1, 2, 3,… 36,… etc.
2013, 2014,… etc.
1, 2, 3,…12.

Tabla Nº 22: Descripción de las variables extraídas para listar los Tipos de documentos que
se presentan más.
Fuente: Elaboración propia.

Para listar los Tipos de solicitudes que se presentan más (ver Figura
Nº 26).

Figura Nº 26: Consulta SQL utilizada para extraer la lista de Tipos de solicitudes que se
presentan más.
Fuente: Elaboración propia.

La consulta arrojó como resultado 11,324 registros (tipos de solicitudes
que se presentan más), de los cuales se obtuvo la siguiente columna (ver Tabla
Nº 23 para la descripción de cada una de las variables extraídas):
 asuntoDocumento: Asunto del documento en caso que sea del tipo de
documento solicitud.
Atributo

Tipo

asuntoDocumento

VARCHAR(255)

Valores posibles
Descripción del tipo de solicitud que se
presenta (SOLICITA CONSTANCIA DE
PAGOS, SOLICITA DUPLICADO DE
BOLETA, SOLICITA PLAZA VACANTE
DE EDUCACIÓN INICIAL, etc.).

158

www.nitropdf.com

Tabla Nº 23: Descripción de las variables extraídas para listar los Tipos de solicitudes que
se presentan más.
Fuente: Elaboración propia.

Para listar el Estado del documento en cada oficina (ver Figura Nº 27).

Figura Nº 27: Consulta SQL utilizada para extraer la lista de Estado del documento en cada
oficina.
Fuente: Elaboración propia.

La consulta arrojó como resultado 30,940 registros (estado del documento
en cada oficina), de los cuales se obtuvo la siguiente columna (ver Tabla Nº 24
para la descripción de cada una de las variables extraídas):
 idOficina: Código de la oficina donde se encuentra el documento.
 estado: Estado del documento.
 tipo: Código del tipo de documento.
 num: Número de folios del documento.
 tipoent: Código del tipo de entidad de origen del documento (remitente).

159

www.nitropdf.com

 mes: Mes del año de recepción del documento de la oficina donde se
encuentra.
 ano: Año de recepción del documento de la oficina donde se encuentra.

Atributo
idOficina

Tipo
INT(11)

Estado

INT

Tipo
Num
tipoent
Mes
Ano

INT(11)
INT(11)
INT(11)
INT(11)
INT(11)

Valores posibles
1, 2, 3,… 96,… etc.
0 (No está finalizado) y 1 (Si está
finalizado).
1, 2, 3,… 36,… etc.
1,2,3,… etc.
1, 2, 3,… 45,… etc.
1, 2, 3,… 12.
2013, 2014,… etc.

Tabla Nº 24: Descripción de las variables extraídas para listar el Estado del documento en
cada oficina.
Fuente: Elaboración propia.

Tarea 3. Exploración inicial de los datos.
Reporte de exploración de los datos
Los datos fueron recolectados con Apache Sqoop y se han explorado con
el paquete de software R. La explicación de la estructura, proceso de
instalación, configuración de las máquinas virtuales y herramientas utilizadas
en el proyecto, se puede consultar en el Anexo Nº 1 y Nº 2. La Figura Nº 28
muestra la salida de Apache Sqoop, donde se puede observar la cantidad de
registros extraídos para la lista de Oficinas, de la consulta SQL (ver Figura Nº
22) enviada a la base de datos de MySQL para extraerlas hacia HDFS del
clúster de Hadoop.

160

www.nitropdf.com

Figura Nº 28: Salida de Apache Sqoop al terminar la importación de oficinas.
Fuente: Elaboración propia.

La Figura Nº 29 muestra la salida de R Studio, donde se puede observar
los códigos de las oficinas con su respectivo código de área (mostrando un
color por cada área).

Figura Nº 29: Salida de R Studio para mostrar la lista de Oficinas.
Fuente: Elaboración propia.

La Figura Nº 30 muestra la salida de Apache Sqoop, donde se puede
observar la cantidad de registros extraídos para la lista de Tipos documentos,

161

www.nitropdf.com

de la consulta SQL (ver Figura Nº 23) enviada a la base de datos de MySQL
para extraerlas hacia HDFS del clúster de Hadoop.

Figura Nº 30: Salida de Apache Sqoop al terminar la importación de los tipos de
documentos.
Fuente: Elaboración propia.

La Figura Nº 31 muestra la salida de R Studio, donde se puede observar
los códigos del tipo de documento (mostrando el nombre del tipo de documento
para cada código al que pertenece, como por ejemplo: el código número “1”
pertenece al tipo de documento “SOLICITUD”, el número “2” al tipo de
documento “OFICIO”, etc.).

Figura Nº 31: Salida de R Studio para mostrar la lista de Tipos de documentos.

162

www.nitropdf.com

Fuente: Elaboración propia.

La Figura Nº 32 muestra la salida de Apache Sqoop, donde se puede
observar la cantidad de registros extraídos para la lista de los Tipos
documentos que se encuentran en cada oficina, de la consulta SQL (ver
Figura Nº 24) enviada a la base de datos de MySQL para extraerlas hacia
HDFS del clúster de Hadoop.

Figura Nº 32: Salida de Apache Sqoop al terminar la importación de los tipos de
documentos que se encuentran en cada oficina.
Fuente: Elaboración propia.

La Figura Nº 33 muestra la salida de Apache Sqoop, donde se puede
observar la cantidad de registros extraídos para la lista de los Documentos
que se presentan más, de la consulta SQL (ver Figura Nº 25) enviada a la
base de datos de MySQL para extraerlas hacia HDFS del clúster de Hadoop.

163

www.nitropdf.com

Figura Nº 33: Salida de Apache Sqoop al terminar la importación de los documentos que se
presentan más.
Fuente: Elaboración propia.

La Figura Nº 34 muestra la salida de Apache Sqoop, donde se puede
observar la cantidad de registros extraídos para la lista de Tipos de
solicitudes que se presentan más, de la consulta SQL (ver Figura Nº 26)
enviada a la base de datos de MySQL para extraerlas hacia HDFS del clúster
de Hadoop.

Figura Nº 34: Salida de Apache Sqoop al terminar la importación de los tipos de solicitudes
que se presentan más.
Fuente: Elaboración propia.

164

www.nitropdf.com

La Figura Nº 35 muestra la salida de Apache Sqoop, donde se puede
observar la cantidad de registros extraídos para la lista de Estado del
documento en cada oficina, de la consulta SQL (ver Figura Nº 27) enviada a
la base de datos de MySQL para extraerlas hacia HDFS del clúster de Hadoop.

Figura Nº 35: Salida de Apache Sqoop al terminar la importación de estado del documento
en cada oficina.
Fuente: Elaboración propia.

La Figura Nº 36 muestra la salida de R Studio, donde se puede observar
el resumen de los datos leídos (ver Figura Nº 35 de extracción de datos) para
la lista de Estado del documento en cada oficina; desde el HDFS del clúster
de Hadoop.

Figura Nº 36: Salida de R Studio para mostrar el resumen de los datos leídos de la lista de
Estado del documento en cada oficina.
Fuente: Elaboración propia.

165

www.nitropdf.com

Tarea 4. Verificar la calidad de los datos.
Reporte de calidad de los datos
Los datos analizados no representan grandes problemas en cuanto a su
calidad.
La variable asunto del documento (asuntoDocumento ver Figura Nº 26)
tiene errores de escritura, donde la encargada(o) no ha corregido el asunto del
documento al momento de registrar el documento del cliente en el sistema.
En cuanto a los datos anómalos (ver Figura Nº 36), la variable numérica
(numeroFolios) presenta dato extremo (42712014).
Para los Tipos de documentos que se encuentran en cada Oficina,
Documentos que se presentan más, Tipos de solicitudes que se
presentan más y el Estado del documento en cada oficina, que están
formados por varios registros. Este problema se abordara en la fase de
modelado, utilizando el modelo de programación MapReduce para contabilizar
la cantidad de registros que se tiene en cada caso.

3.7.3 Preparación de los datos
Tarea 1. Seleccionar los datos.
Como el objetivo principal del análisis de datos con herramientas de Big
Data es apoyar a la toma de decisiones a partir de los datos guardados en la
166

www.nitropdf.com

base de datos del SGDA, entonces los documentos fueron una de las variables
de análisis y todo lo que este asociado a este directamente; es decir, las
variables que fueron extraídas para la lista de Oficinas, Tipos de
documentos, Tipos de documentos que se encuentran en cada Oficina,
Documentos que se presentan más, Tipos de solicitudes que se
presentan más y el Estado del documento en cada oficina. No se descartó
ninguno de los atributos obtenidos en la fase anterior.
Tarea 2. Limpieza de los datos.
Reporte de limpieza de datos
En el reporte de calidad de los datos se señaló que la variable asunto del
documento tiene errores de escritura en el momento de haber sido registrado al
sistema. Se realizó un patrón de discriminación para los caracteres especiales
que fueron ingresados, y a la vez este patrón fue utilizado en el modelo de
programación MapReduce al realizarse la lectura de los registros de Tipos de
solicitudes que se presentan más (ver Figura Nº 34).
Tarea 3. Construcción de los datos.
De acuerdo a uno de los objetivos del análisis de datos con herramientas
de Big Data es buscar la correlación de los datos; entonces lo que en el modelo
se busca es conocer qué Tipo de solicitudes se presentan más y predecir la
probabilidad de ocurrencia de finalización de un documento a partir del tipo de
documento y la oficina a la que se presenta. Para que se logre esto se exploró
los registros extraídos en la fase anterior; el asunto del documento para el caso
de conocer qué tipo de solicitudes se presenta más. Y para el caso del modelo
167

www.nitropdf.com

de predicción de finalización de un documento se utilizó los registros del año
2013, para el entrenamiento del modelo y los registros del año 2014 para la
prueba del modelo.
Tarea 4. Integración de los datos.
Conjunto de datos final
No se descartó ninguno de los datos extraídos para el proyecto de
análisis de datos con herramientas de Big Data.

3.7.4 Modelado
CRISP-DM propone cuatro tareas para la fase de modelado: seleccionar
la técnica de modelado, diseñar las pruebas del modelo, construir el modelo y
evaluar el modelo.
Tarea 1. Seleccionar la técnica de modelado.
Técnica de modelado
Debido a la naturaleza del problema y de las variables en estudio, se han
seleccionado los siguientes modelos: Para la explicación de instalación y
configuración de las herramientas se puede consultar en el Anexo Nº 2.
 MapReduce (dentro del ecosistema del framework Apache Hadoop
YARN ver Figura Nº 37.)
 Regresión logística (para el modelo predictivo)

168

www.nitropdf.com

Figura Nº 37: Ecosistema de Apache Hadoop Yarn.
Fuente: Elaboración propia.

Para la construcción del modelo predictivo se utilizaron las variables del
conjunto de datos (ver Tabla Nº 24) y para la explicación del diseño se puede
consultar en el Anexo Nº 4.
Supuestos del modelo
No se necesitaron supuestos, para los modelos con los que se trabajaron.
Para el modelo de regresión logística con más de una variable regresora
cuantitativa se verificará la ausencia de multicolinealidad.
Tarea 2. Diseño de las pruebas del modelo.

169

www.nitropdf.com

Figura Nº 38: Arquitectura de datos para el modelo de MapReduce.
Fuente: Elaboración propia.

La Figura Nº 38 muestra las herramientas seleccionadas para cada etapa
de la arquitectura de datos diseñada para el modelo de MapReduce. Ya que los
datos de la Base de datos del SGDA son estructurados, se trabajó
directamente con la herramienta Apache Sqoop para la extracción y
almacenamiento de los datos hacia el HDFS del clúster de Hadoop. Luego se
realizó el proceso de búsqueda de patrones con el modelo de programación de
MapReduce del framework de Apache Hadoop Yarn y finalmente se visualizó
los resultados con el uso de la herramienta RStudio.
Para el modelo de regresión logística los datos de prueba fueron
construidos a partir de los documentos presentados en el año 2013 (12360

170

www.nitropdf.com

instancias). Y los documentos presentados en el año 2014 fueron utilizados
para entrenar el modelo (18580 instancias).
Tarea 3. Construir el modelo.
MapReduce
Se realizaron dos procesos: El primero fue pasar un diccionario para
depurar el texto. El segundo realizar la búsqueda de patrones sobre el archivo
seleccionado y procesarlo.
En el primer proceso de depuración del texto, se realizó para que al
buscar patrones, los resultados sean más exactos. En la depuración se aplicó
la eliminación de caracteres no alfanuméricos; ya que lo que se busca son
patrones de frases, los símbolos de puntuación y otros caracteres no interesan
en absoluto, además de que pudo llegar a causar problemas.
El segundo proceso se realizó sobre el texto depurado, y consiste en la
utilización de la librería de MapReduce de Apache Hadoop Yarn para encontrar
los patrones más frecuentes. Las funciones que fueron utilizadas se llamaban
map() y reduce(). Para la explicación de la estructura y diseño del código
desarrollado para el modelo MapReduce se puede consultar en el Anexo Nº 3.
Los resultados del modelo MapReduce fueron introducidos en el HDFS
del clúster de Hadoop. Y a continuación se muestra los 20 primeros resultados,
con la herramienta RStudio ver Figura Nº 39, que fueron obtenidos en la
búsqueda de patrones más comunes.

171

www.nitropdf.com

Figura Nº 39: Los 20 primeros tipos de solicitudes con RStudio.

Regresión logística
El mejor modelo obtenido con el software R se representa en la Figura Nº
40. Las variables “idOficina”, “estado”, “tipoDocumento”, “numeroFolios” y
“tipoEntidadDocumento” resultaron significantes para la regresión, con un pvalor menor a 0.05. Como las predicciones para las instancias de prueba son
una probabilidad (probabilidad de ocurrencia de finalización de un documento a
partir de las variables presentadas), se redondea:

172

www.nitropdf.com

 Si la probabilidad de que el documento esté finalizado es menor a 0.5:
se toma 0 (no finalizará).
 Si la probabilidad de que el documento esté finalizado es mayor a 0.5: se
toma 1 (finalizará).

Figura Nº 40: Modelo de regresión logística obtenido con R.

La matriz de confusión (ver Figura Nº 41) muestra que la mayor parte de
las instancias fueron clasificadas con el valor 1, pudiendo predecir
correctamente a los documentos finalizados.

173

www.nitropdf.com

Figura Nº 41: Matriz de confusión del modelo de regresión.

Tarea 4. Evaluar el modelo.
Evaluación de los modelos
El modelo MapReduce ha demostrado que los tipos de solicitudes más
comunes son:
 Devolución de aportes de fonavi.
 Duplicado de boleta de pago.
 Informe escalafonario.
 Inscripción al programa de eib.
 Préstamo de subcafae.
 Constancia de pago.
En el modelo de regresión logística, la tasa de acierto general es del 58%
y la tasa de acierto para los documentos finalizados es del 89% (ver Figura Nº
42). En la Figura Nº 43 se muestra la cantidad de documentos finalizados antes
y después de la predicción.

174

www.nitropdf.com

Figura Nº 42: Tasa de acierto para los Documentos finalizados obtenido con R.

Figura Nº 43: Cantidad de documentos finalizados antes y después de la predicción.

3.7.5 Evaluación
CRISP-DM llevó a cabo una primera evaluación técnica del modelo en la
fase anterior (Modelado). En esta etapa, se realiza una evaluación técnica final,

175

www.nitropdf.com

una evaluación en función de los objetivos del negocio, y una revisión del
proceso.
Tarea 1. Evaluar los resultados.
Evaluación de los resultados obtenidos
Respecto al modelo MapReduce de Apache Hadoop Yarn más
específicamente, un aspecto a tener en cuenta de Hadoop es la potencia que
tiene gracias a su escalabilidad, porque se puede diseñar caso de uso hasta
ahora impensables con infraestructura de coste medio o bajo, como los usados
en este proyecto y obtener resultados muy competitivos comparados con una
solución de más presupuesto. Tener en cuenta que la infraestructura que se ha
usado no es la más adecuada (tres máquinas virtuales ejecutándose en una
misma máquina y compartiendo recursos físicos), que en algunos casos el
disco son cuellos de botella, algo importante al trabajar en paralelo. Y
finalmente destacar la gran flexibilidad a la hora de diseñar procesos de
Hadoop, ya que cualquiera de las capas puede diseñarse independientemente
de las otras e incluso pueden cambiarse por completo. En la capa de Flujo de
trabajo de los datos, con la herramienta Sqoop, puede extenderse la
funcionalidad añadiendo más base de datos relacionales (Oracle, SQL Server,
Teradata, etc.). Esta flexibilidad permite que las soluciones estén totalmente
preparadas para los cambios que se puedan producir en un futuro.
Y respecto al modelo de regresión logística, que resultó con una tasa de
acierto del 58%. Este valor cumple con el criterio de éxito establecido para el
proyecto. Sin embargo, es importante destacar que este modelo podría ser

176

www.nitropdf.com

notablemente mejorado si se dispusiera de más variables. Información como la
prioridad del documento, cantidad de días transcurridos y otras cualidades
referentes al documento y al estado del documento podrían reducir la tasa de
error del modelo.
Tarea 2. Revisión del proceso.
El proyecto de análisis de datos con herramientas de Big Data no ha
presentado mayores inconvenientes en las etapas de análisis del problema y
preparación de los datos. Sin embargo, la fase de modelado para la solución
con Hadoop, resultó un poco confuso al principio, por la cantidad de
herramientas nuevas que hay y por los nuevos paradigmas en los que el
desarrollador se encuentra. Por esto mismo, y aunque se haya realizado un
estudio teórico de la solución y de las distintas herramientas, durante el
desarrollo del modelo MapReduce hubo varias modificaciones en el diseño.
Esto es normal teniendo en cuenta todas las novedades y la curva de
aprendizaje de herramientas como Sqoop, Hadoop y RStudio.
No hay actividades importantes que hayan sido omitidas. Sería importante
tener en cuenta la experiencia obtenida en el desarrollo del modelo
MapReduce para futuros proyectos.
Tarea 3. Determinar las próximas etapas.
Se tomó la decisión de implementar los modelos obtenidos en la UGEL de
Azángaro, específicamente en el área de administración. Porque con la
información obtenida del modelo MapReduce el director de administración pudo
sistematizar algunas áreas de la institución, además de priorizar los
177

www.nitropdf.com

documentos presentados en función a las predicciones del modelo de regresión
logística.
3.7.6 Implementación
CRISP-DM lleva a cabo la fase de implementación mediante cuatro
tareas: planificación de la implementación, planificación del monitoreo, reporte
final y revisión del proyecto.
Tarea 1. Planificar la implementación.
Plan de implementación
La implementación de los modelos obtenidos se realizó mediante las
siguientes tareas:
1. Se dio a conocer los resultados del proyecto a los directivos, personal de
administración y recepcionista (de la oficina de trámite documentario).
2. Se discutió con los directivos los resultados. Para tener en cuenta la
utilidad del conocimiento adicional obtenido (además de los modelos) y
de nuevas hipótesis que pueden surgir para futuras investigaciones.
3. En base a los modelos seleccionados, se implantó la solución para
conocer los tipos de solicitudes que se presentan más y predecir los
documentos que probablemente finalizarán en su trámite.
4. Se Capacitó a los usuarios finales de la solución. Para aclarar que los
datos están sujetas a cambios y que las predicciones a una
probabilidad.

178

www.nitropdf.com

5. Luego de un cierto periodo, se verificó la tasa del tiempo de atención al
cliente

y

finalización

del

documento,

ha

disminuido

(objetivo

organizacional).
Tarea 2. Planificar el mantenimiento y monitoreo.
Plan de mantenimiento y monitoreo
Periódicamente se realizaron pruebas sobre la tasa de acierto de los
modelos, para verificar que se encuentre dentro de los márgenes de tolerancia
(aproximadamente el 50% del tiempo de atención al cliente debe ser
disminuido y el 60% de los documentos finalizados debe ser detectado).
El modelo podría dejar de ser válido cuando:
 Se sistematicen las todas las áreas.
 Factores externos a la organización provoquen un cambio en el hábito
de atención a los documentos presentados en la oficina de trámite
documentario.
En estos casos, se requerirá probar nuevamente los modelos bajo las
nuevas condiciones para validar su vigencia.
Tarea 3. Crear un reporte final.
Reporte final
El proyecto de análisis de datos con herramientas de Big Data se ha
desarrollado exitosamente, no sólo por los modelos seleccionados, sino por

179

www.nitropdf.com

toda la información adyacente que se ha generado, que aporta material valioso
a la organización para el apoyo en la toma de decisiones.
El personal de la organización ha colaborado en todo momento con el
proyecto, participando activamente del mismo.
Los datos no han presentado problemas en cuanto a su disponibilidad y
veracidad. Salvo los errores de escritura al momento de registrarlo en el SGDA,
todos los campos estaban completos y no presentaban datos ausentes.
Los modelos que se han seleccionado para la fase de implementación
fueron MapReduce y regresión logística, construidos a partir de las variables
extraídas en la fase de comprensión de los datos.
Durante el modelado, se realizó especial énfasis en la búsqueda de
patrones más comunes para el conocimiento de las áreas donde se presentan
mayor documentación y los tipos de solicitudes a las que correspondían.
Es importante destacar los inconvenientes que trajo a la fase de
modelado la solución con Hadoop. A pesar de haber se realizado un estudio
teórico de la solución y de las distintas herramientas, la flexibilidad de la misma
permite que la solución esté totalmente preparada para los cambios que se
puedan producir en un futuro.
El modelo se implementó en el área de administración, y fue monitoreado
periódicamente para comprobar su vigencia.
Tarea 4. Revisión del proyecto.
Revisión del proyecto
180

www.nitropdf.com

El proyecto se ha llevado a cabo en forma exitosa, en el tiempo
programado con el cliente. Se ha logrado una alta participación de todo el
personal involucrado en el proyecto.
Los datos que disponía el cliente pudieron ser explorados y explotados,
obteniendo un buen modelo de análisis con MapReduce y con la regresión
logística.
Se han producido retrasos en la fase de modelado, ya que el equipo de
trabajo tenía poca experiencia en materia de nuevos paradigmas para el
desarrollo de una solución con herramientas de Big Data. Esta situación sirvió
como aprendizaje para casos futuros.

3.8

Plan de tratamiento de datos


Terminando el análisis de datos con herramientas de Big Data del
sistema de gestión de los documentos administrativos para el apoyo
en la toma de decisiones se aplicó una encuesta llamado post test.



Se mostró los datos analizados en gráficos para poder mejorar la
toma de decisiones.



Se medió los resultados de la encuesta aplicada al número de
muestra.

181

www.nitropdf.com

3.9

Diseño estadístico para la prueba de hipótesis
El análisis e interpretación de datos mediante la prueba de hipótesis

estadística se desarrolló usando la distribución t de student mediante los
siguientes cinco pasos:
Paso 1: Plantear Hipótesis Nula (Ho) e Hipótesis Alternativa (Hi).
La Hipótesis alternativa plantea matemáticamente lo que se quiere demostrar y
la Hipótesis nula plantea exactamente lo contrario.
Paso 2: Determinar Nivel de Significancia. (Rango de aceptación de hipótesis
alternativa)

Optimista

Confiable

Pesimista
En la investigación se utilizó el nivel confiable, que equivale al 95% de
aceptación y 5% de error.
Paso 3: Se calcula la media y la desviación estándar a partir de la muestra.
Paso 4: Se aplica la distribución t de Student para calcular la probabilidad de
error (P) por medio de la fórmula:

182

www.nitropdf.com

También se determina grados de libertad

Distribución t

Mediana

Valor a analizar

Tamaño de la muestra
Paso 5: En base a la evidencia disponible se acepta o se rechaza la hipótesis
alternativa.
Si la probabilidad de error (P) es mayor que el nivel de significancia: Se
rechaza la hipótesis alterna.
Si la probabilidad de error (P) es menor que el nivel de significancia: se acepta
hipótesis alternativa.

183

www.nitropdf.com

CAPÍTULO IV
ANÁLISIS E INTERPRETACIÓN DE
RESULTADOS DE LA INVESTIGACIÓN

184

www.nitropdf.com

Se analizó la base de datos del sistema de gestión de los documentos
administrativos (SGDA), con la finalidad de probar las hipótesis señaladas en el
Capítulo II de la tesis.

4.1

La correlación de datos en la solución del análisis de datos para la
toma de decisiones.
Se mostró que los datos extraídos de la base de datos están

correlacionados con los Tipos de documentos que se encuentran en cada
Oficina, Documentos que se presentan más, Tipos de solicitudes que se
presentan más y el Estado del documento en cada oficina. Para ello se ha
aplicado las consultas SQL para la importación de los datos con la herramienta
Apache Sqoop, clúster de Hadoop para el almacenamiento de los ficheros
distribuidos y software R para la visualización de los datos.

Tipos de documentos que se encuentran en cada Oficina.
En la Figura Nº 44 se observa que, los tipos de documentos que se
encuentran en cada oficina durante los últimos años son: en la oficina de
“Secretario de PER” se encuentran 5817 Oficios y 5067 Solicitudes, en la
oficina de “Secretario de AGP” se encuentran 2663 Oficios y en la oficina de
“Secretaria ADM” se encuentran 1535 Solicitudes.

185

www.nitropdf.com

Figura Nº 44: Tipos de documentos que se encuentran en cada Oficina.

Documentos que se presentan más.
En la Figura Nº 45 se observa que, los documentos más presentados
durante los años 2013 y 2014 son: se presentaron 3967 Solicitudes en el 2013,
7357 Solicitudes en el 2014, 5840 Oficios en el 2013, 5950 Oficios en el 2014,
960 Notificaciones en el 2013, 2029 Notificaciones en el 2014, 1419 Informes
en el 2013 y 1410 Informes en el 2014.

186

www.nitropdf.com

Figura Nº 45: Documentos que se presentan más.

Tipos de solicitudes que se presentan más.
En la Figura Nº 46 se observa que, los tipos de solicitudes que se
presenta más durante los últimos años son: 476 solicitudes con la
denominación de “solicita-informe-escalafonaria” o “informe-escalafonario”, 279
solicitudes de “solicita-préstamo-de-sub-cafae”, 96 solicitudes de “solicitareembolso-de-energia-electrica”, 83 solicitudes de “solicita-boleta-de-pago”, 49
solicitudes de “solicita-bonificación-especial-de-preparación-de-clase-de-30%”,
209 solicitudes de “solicita-constancia-de-pagos”, 991 solicitudes de “solicitadevolución-de-aportes-de-fonavi”, 450 solicitudes de “solicita-duplicado-deboleta-de-pago”, 618 solicitudes de “solicita-inscripción-en-el-programa-de-eib”,

187

www.nitropdf.com

144 solicitudes de “solicita-medio-geografico” y 62 solicitudes de “solicitapermiso”.

188

www.nitropdf.com

189

www.nitropdf.com

Figura Nº 46: Resultado del modelo MapReduce con RStudio para los Tipos de solicitudes.

Estado del documento en cada oficina.
En la Figura Nº 47 se observa (15 primeros registros de un total de 30940
registros), el estado del documento en cada oficina que está en correlación con
las siguientes columnas: “idOficina” representa el código de la oficina donde se
encuentra el documento, “estado” representa en qué estado está el documento
(0 si el documento no ha finalizado y 1 si el documento está finalizado),
“tipoDocumento” representa el código del tipo de documento (ver Figura Nº 31),
“numeroFolios” representa la cantidad de folios que tiene un documento,
“tipoEntidadDocumento” representa el código del tipo de entidad que tiene el
documento, “mes” representa el mes al que llego a la oficina, “ano” representa
el año al que llego a la oficina.

Figura Nº 47: Estado del documento en cada oficina.

190

www.nitropdf.com

4.2

El Almacenamiento de datos históricos en la solución del análisis de
datos para la toma de decisiones.
Se mostró que los datos extraídos de la base de datos almacenan datos

históricos para la utilización en el modelo de regresión logística. Para ello se
ha aplicado una de las técnicas de clasificación de minería de datos,
específicamente regresión logística binaria y el software R para la visualización
de los datos.
Modelo de regresión logística.
En la Figura Nº 48 se observa, parte del código para el modelo de
regresión logística: la variable “train” almacena los datos de los documentos
presentados en el año 2013 que fueron utilizados para el entrenamiento del
modelo (12360 instancias) y la variable “test” almacena los datos de los
documentos presentados en el año 2014 que fueron utilizados para la prueba
del modelo (18580 instancias).
train <- tmp[tmp$anoRecepcion == 2013,]
test <- tmp[tmp$anoRecepcion == 2014,]
logit <- glm(estado ~ idOficina+tipoDocumento+numeroFolios+tipoEntidadDocumento,
family=binomial(link='logit'), data=train)
test.probs <-predict(logit, test, type='response')
pred.logit <- rep('0',length(test.probs))
pred.logit[test.probs>=0.5] <- '1'
table(pred.logit, test$estado)
confusionMatrix(pred.logit, test$estado)
Figura Nº 48: Fragmento del código para el Modelo de Regresión Logística.

191

www.nitropdf.com

El modelo de regresión logística fue construido a partir de siguientes
variables:


La variable dependiente “estado” que no es cuantitativa y que tiene
dos categorías (0 si el documento no está finalizado y 1 si está
finalizado).



Las

variables

independientes

“idOficina”,

“tipoDocumento”,

"numeroFolios”, “tipoEntidadDocumento” que son cuantitativas y
algunas categóricas.
Para el caso de la variable “logit” como se observa en la Figura Nº 48,
almacena el modelo de regresión logística con los datos de entrenamiento de la
variable “train”. La variable “test.probs” almacena los datos de predicción a
partir del modelo generado y los datos de prueba (almacenados en la variable
“test”). La variable “pred.logit” almacena los datos redondeados a partir de los
siguientes casos:


Si la probabilidad de que el documento esté finalizado es menor a
0.5: se toma 0 (no finalizará).



Si la probabilidad de que el documento esté finalizado es mayor a
0.5: se toma 1 (finalizará).

En suma, la regresión logística es un análisis, para probar un modelo de
predicción de probabilidad de ocurrencia de una variable dicotómica categórica
192

www.nitropdf.com

en este caso la “probabilidad de ocurrencia de finalización de un documento a
partir de las variables presentadas”.
También se puede ver claramente la multicolinealidad de las variables
“idOficina”, “tipoDocumento”, “numeroFolios” y “tipoEntidadDocumento” en la
Figura Nº 49, esto es interpretado como que las variables utilizadas resultaron
significantes para el modelo de regresión logística, con un p-valor menor a
0.05.

Figura Nº 49: La multicolinealidad de las variables.

Por otro lado se observa en la Figura Nº 48 y Figura Nº 50, las funciones
table() y confusionMatrix() son las encargadas de mostrar la tasa de acierto
193

www.nitropdf.com

general que este caso tiene una aproximación del 58% y una tasa de acierto
para los documentos finalizados del 88%, el cual utilizo un modelo ajustado
para la regresión logística.

Figura Nº 50: Tasa de acierto general y de los documentos finalizados.

En la Figura Nº 51 se puede ver claramente, el resultado del modelo de
regresión logística con los datos de entrenamiento y prueba (12360
documentos del año 2013 almacenados en la variable “train” y 18580
documentos del año 2014 almacenados en la variable “test”). Esto es

194

www.nitropdf.com

interpretado como que en el año 2013 se tuvo 6129 documentos finalizados y
6231 documentos no finalizados, y al utilizar el modelo de predicción se obtuvo
que la probabilidad de ocurrencia de finalización de un documento se tendrá
11408 documentos finalizados y 7172 documentos no finalizados.

Figura Nº 51: Resultado del modelo de regresión logística.

4.3

El procesamiento de datos en la solución del análisis de datos para
la toma de decisiones.
Se mostró que los datos extraídos de la base de datos permite el

procesamiento de datos, mediante la utilización del modelo MapReduce. Para
ello se ha aplicado el modelo de programación introducida por Google llamada
MapReduce, de donde existen multitudes de implementaciones dentro de las
cuales destaca especialmente Hadoop, un proyecto de Apache que
proporciona una base sólida a las arquitecturas y herramientas Big Data.
Modelo MapReduce.
195

www.nitropdf.com

Los módulos básicos de un programa MapReduce son tres clases
principales:


Clase Mapper: Se desarrolla el algoritmo map.



Clase Reducer: Se desarrolla el algoritmo reduce.



Clase Drive: Donde se configura y se lanza un trabajo (job) para que
sea ejecutado en el clúster de Hadoop.

Clase Mapper.
En la clase Mapper se implementa el método map, que se encarga de
procesar los datos tal y cual se encuentran en el HDFS.
En la Figura Nº 52 se observa la clase TokenizerMapper que se extiende
de la clase Mapper ofrecida por la API Hadoop, esta clase espera cuatro
parámetros que se refieren a los tipos de datos de entrada y salida. Los dos
primeros definen los tipos de datos del conjunto clave-valor de entrada y los
dos últimos definen los tipos de datos del conjunto clave-valor de salida.
Las declaraciones que se hacen fuera del map son para que cada vez
que se llama al método map no se cree una nueva instancia de estos objetos.
La función map recibe tres parámetros: el primero de ellos define el tipo de la
key, el segundo el tipo del value y el tercero es un objeto context que se
utilizara para escribir los datos intermedios. El objeto value que recibe la clase
map contiene cada línea del fichero que se está leyendo. Primero se pasa a
String para poder operar con él. Cada línea contiene palabras divididas por un

196

www.nitropdf.com

separador ‘ ’, se divide la línea por el separador y se recorre elemento a
elemento en el while.
Finalmente, se escribe en la variable word cada palabra encontrada. La
variable one no se modifica ya que es siempre una constante. Con el write se
escribe los datos intermedios, cada uno de estos tiene como key la palabra y
como value un 1 (constante).
public static class TokenizerMapper extends Mapper<Object, Text, Text, IntWritable>{
static enum CountersEnum { INPUT_WORDS }
private final static IntWritable one = new IntWritable(1);
private Text word = new Text();
private boolean caseSensitive;
private Set<String> patternsToSkip = new HashSet<String>();
private Configuration conf;
private BufferedReader fis;
@Override
public void map(Object key, Text value, Context context) throws IOException,
InterruptedException {
String line = (caseSensitive) ? value.toString() : value.toString().toLowerCase();
for (String pattern : patternsToSkip) {
line = line.replaceAll(pattern, "");
}
StringTokenizer itr = new StringTokenizer(line);
while (itr.hasMoreTokens()) {
word.set(itr.nextToken());
context.write(word, one);
Counter counter = context.getCounter(CountersEnum.class.getName(),
CountersEnum.INPUT_WORDS.toString());
counter.increment(1);
}
}
}

Figura Nº 52: Clase Mapper del programa MapReduce.

197

www.nitropdf.com

Clase Reducer.
En la clase Reducer se implementa el método reduce, que se encarga de
recibir los datos intermedios.
En la Figura Nº 53 se observa la clase IntSumReducer que se extiende de
la clase Reducer ofrecida por la API Hadoop, esta clase espera cuatro
parámetros que se refieren a los tipos de datos de entrada y salida. Los dos
primeros definen los tipos de datos del conjunto clave-valor de entrada y los
dos últimos definen los tipos de datos del conjunto clave-valor de salida.
El método reduce recibe tres parámetros: el primero es la key de entrada,
el segundo es una lista de los valores intermedios asociados a esa key y el
tercero es un objeto Context para escribir los datos que se quieren devolver. Se
recorre la lista de valores y para cada uno de los valores se extrae el valor
correspondiente. Los valores se van sumando para obtener el número de veces
que aparece una palabra.
Finalmente se escribe el resultado en el HDFS usando el context.write().

198

www.nitropdf.com

public static class IntSumReducer extends Reducer<Text,IntWritable,Text,IntWritable> {
private IntWritable result = new IntWritable();
public void reduce(Text key, Iterable<IntWritable> values, Context context) throws
IOException, InterruptedException {
int sum = 0;
for (IntWritable val : values) {
sum += val.get();
}
result.set(sum);
context.write(key, result);
}
}

Figura Nº 53: Clase Reducer del programa MapReduce.

Clase Drive.
En la clase Drive consta de una función main que recibe como parámetros
el input y output del MapReduce y que configura el trabajo (job) que se enviará
al sistema Hadoop (clúster).
En la Figura Nº 54 se observa, la comprobación de los argumentos que
recibe del trabajo (job): primero archivos a tratar (entrada), donde guardar el
resultado (salida) y el archivo de diccionario para depurar el texto (patrones).
Seguidamente se crea el job indicando la clase que se llamará (en este
caso, el ExpedientCount) y el nombre del job. Configuration se utiliza más
adelante para establecer configuraciones diferentes a las que vienen por
defecto. Se indica también, cuales son las clases Mapper y Reducer (en este
caso, TokenizerMapper y IntSumReducer).

199

www.nitropdf.com

Se establecen los directorios de diccionario, entrada y salida, es decir, el
directorio del archivo de diccionario de donde se leerá los patrones para
depurar el texto, luego el directorio del HDFS de donde se leerán los datos de
entrada y en donde se guardará la salida de la ejecución.
El path de la salida no debe estar creado antes de la ejecución, Hadoop lo
crea automáticamente, si este path está creado antes de la ejecución dará
error. Otra de las cosas que hay que establecer son los tipos de la key y del
value que devuelve el reduce y que devuelve el map.

200

www.nitropdf.com

public static void main(String[] args) throws Exception {
Configuration conf = new Configuration();
GenericOptionsParser optionParser = new GenericOptionsParser(conf, args);
String[] remainingArgs = optionParser.getRemainingArgs();
if (!(remainingArgs.length != 2 || remainingArgs.length != 4)) {
System.err.println("Usage: wordcount <in> <out> [-skip skipPatternFile]");
System.exit(2);
}
Job job = Job.getInstance(conf, "expedient count");
job.setJarByClass(ExpedientCount.class);
job.setMapperClass(TokenizerMapper.class);
job.setCombinerClass(IntSumReducer.class);
job.setReducerClass(IntSumReducer.class);
job.setOutputKeyClass(Text.class);
job.setOutputValueClass(IntWritable.class);
List<String> otherArgs = new ArrayList<String>();
for (int i=0; i < remainingArgs.length; ++i) {
if ("-skip".equals(remainingArgs[i])) {
job.addCacheFile(new Path(remainingArgs[++i]).toUri());
job.getConfiguration().setBoolean("wordcount.skip.patterns", true);
} else {
otherArgs.add(remainingArgs[i]);
}
}
FileInputFormat.addInputPath(job, new Path(otherArgs.get(0)));
FileOutputFormat.setOutputPath(job, new Path(otherArgs.get(1)));
System.exit(job.waitForCompletion(true) ? 0 : 1);
}

Figura Nº 54: Clase Driver del programa MapReduce.

En la Figura Nº 48 se observa, el comando con los parámetros de
configuración para realizar el procesamiento de datos y buscar los patrones
más comunes: primero se ingresa el comando hadoop con la especificación del
archivo ejecutable, segundo la definición de la clase que se utilizará (en este
caso ExpedientCount), tercero la configuración para considerar las palabras
mayúsculas y minúsculas por igual, cuarto el directorio de los archivos de

201

www.nitropdf.com

entrada, quinto el directorio de los archivos de salida y sexto el directorio del
archivo de diccionario.

Figura Nº 55: Procesamiento de los datos para la búsqueda de patrones más comunes.

En la Figura Nº 56 se observa, el resultado luego de haber realizado la
ejecución del programa MapReduce (en la Figura Nº 55): Hadoop ofrece una
interface web a través de la cual se puede visualizar todo el contenido del
HDFS (del clúster de Hadoop), así como información detallada sobre la
disponibilidad del archivo (la distribución de los archivos en cada nodo del
clúster de Hadoop).

202

www.nitropdf.com

Figura Nº 56: Resultado en el clúster de Hadoop, del procesamiento de los datos para la
búsqueda de patrones más comunes.

4.4

La visualización de datos en la solución del análisis de datos para la
toma de decisiones.
Se mostró que los datos extraídos de la base de datos permite la

visualización de datos, mediante la utilización de la Herramienta RStudio.
Para ello se ha aplicado una arquitectura de máquinas virtuales (clúster), en
donde cada máquina virtual ha sido instalada y configurada las distribuciones
de Hadoop, Sqoop y RStudio, para que pueda conectarse a la intranet de la
institución y tengan acceso a internet.
Arquitectura del clúster Hadoop.
En la Figura Nº 57 se observa, tres máquinas virtuales instaladas en la
maquina anfitrión. Cada máquina es considerada como un nodo para Hadoop,
en el nodo hadoop2nn se inician los servicios ResourceManager, NameNode,
203

www.nitropdf.com

SecondaryNameNode, JobHistoryServer, Sqoop y RStudio. Y en los nodos
hadoop2nn, hadoop2dn1 y hadoop2dn2 se inician los servicios NodeManager y
DataNode. La configuración del acceso a la intranet, se realizó para poder
visualizar los datos a través de la red interna de la institución, y la configuración
del acceso a internet se utilizó para poder instalar los paquetes necesarios con
la finalidad de que la herramienta RStudio pueda interactuar con el clúster
Hadoop.

Figura Nº 57: Arquitectura del clúster Hadoop.

Para la explicación más detallada de la arquitectura, instalación y
configuración de las máquinas virtuales con sus respectivas distribuciones de
Hadoop, Sqoop y RStudio puede consultar el Anexo 1 y Anexo 2.
Herramienta RStudio para la visualización de los datos.
Como se observa en la Figura Nº 58, la pantalla de inicio de la
herramienta RStudio. El usuario que se utilizó para la herramienta que permitió
visualizar los datos, fue el mismo que se utilizó para el clúster Hadoop. Se

204

www.nitropdf.com

eligió la herramienta RStudio porque es una de las más utilizadas para el
análisis de datos.

Figura Nº 58: Pantalla de Inicio de la Herramienta RStudio.

En la Figura Nº 59 se observa, un ejemplo de la utilización de la
herramienta RStudio para la visualización de los tipos de documentos que la
institución recibe en la oficina de trámite documentario. La herramienta tiene 4
áreas bien importante que se detalla a continuación: Primero en el área número
1 se tiene la codificación del programa que se utilizara para el análisis de los
datos (en esta área se agregara las librerías que utilizara el programa para
realizar el análisis), Segundo en el área número 2 se tiene la consola de salida
de los comandos ejecutados en el área número 1 (visualización de los datos de
salida), Tercero en el área número 3 se tiene la visualización de las variables
utilizadas en el programa escrito en el área número 1, Cuarto en el área
número 4 se tiene la visualización de los gráficos generados a partir de la
ejecución del programa del área número 1.

205

www.nitropdf.com

Figura Nº 59: Utilización de la herramienta RStudio para la visualización de los tipos de
documentos.

4.5

Evaluación de la solución del análisis de datos

4.5.1 Métrica de calidad de la solución
Para la medición de la calidad de la solución del análisis de datos con
herramientas de Big Data, se utilizó la métrica de calidad y factores de McCall.
Para poder aplicar las pruebas de calidad de McCall se realizó una serie
de cuestionarios al usuario administrador de la solución del análisis de datos,
los resultados de estas están representadas en las tablas siguientes, las
respuestas hechas por el usuario administrador están en un rango de 0 (bajo) a
10 (alto).

206

www.nitropdf.com

Factores de calidad
Exactitud
Completitud
Concisión
Consistencia
Tolerancia a errores
Eficiencia de ejecución
Capacidad de expansión
Generalidad
Instrumentación
Modularidad
Operatividad
Seguridad
Auto documentación
Simplicidad
Trazabilidad
Facilidad de formación

Usuario
8
9
8
8
8
8
9
8
8
9
7
8
7
7
8
9

Tabla Nº 25: Puntuación de factores de calidad según usuarios de la solución.
Fuente: Entrevista con el personal administrativo y elaboración propia.

La Tabla Nº 26 nos muestra los principales factores de calidad, que fueron
ponderadas por el usuario administrador de la solución del análisis de datos.
Métricas
Corrección
Fiabilidad
Eficiencia
Integridad
Flexibilidad
Capacidad de pruebas
Reusabilidad
Usabilidad

Usuario
8
8
8
8
8
8
8
8

Tabla Nº 26: Puntuación de métricas de calidad de McCall según usuarios de la solución.
Fuente: Elaboración propia.

El cálculo de cada factor de calidad se realizó con la fórmula de McCall

207

www.nitropdf.com

Donde:
: es un factor de calidad.

: Coeficiente de regresión.

: Métricas que afectan al factor de calidad.
Para calcular los coeficientes de regresión se utilizó la siguiente formula:

son las métricas que afectan al factor de calidad.

Dónde:

En la Tabla Nº 27 se muestra el cálculo de cada factor de McCall.

Métricas McCall
Corrección

Fiabilidad

Eficiencia

Integridad
Flexibilidad

Capacidad de pruebas

Reusabilidad

Factores de
calidad
Completitud

9

0.33

Consistencia

8

0.33

Trazabilidad

8

0.33

Exactitud

8

0.20

Consistencia

8

0.20

Tolerancia a errores

8

0.20

Modularidad

9

0.20

Simplicidad

7

0.20

Concisión

8

0.33

Eficiencia ejecución

8

0.33

Operatividad

7

0.33

Instrumentación

8

0.50

Seguridad

8

0.50

9

0.20

Capacidad
expansión
Generalidad

de

8

0.20

Modularidad

9

0.20

Auto documentación

7

0.20

Simplicidad

7

0.20

Instrumentación

8

0.25

Modularidad

9

0.25

Auto documentación

7

0.25

Simplicidad

7

0.25

Generalidad

8

0.33

Modularidad

9

0.33

8.25

8.00

7.59

8.00
8.00

7.75

7.92

208

www.nitropdf.com

Usabilidad

Auto documentación

7

0.33

Operatividad

7

0.50

Facilidad de formación

9

0.50

8.00

Tabla Nº 27: Resumen de los factores McCall.
Fuente: Entrevista con el administrador y elaboración propia.

Para la métrica de Mantenimiento de McCall se calculó el índice de
madurez del software (

) con la siguiente formula:

Donde:

: Numero de módulos en la versión actual.

: Numero de módulos en la versión actual que se han añadido.

: Número de módulos en la versión actual que se han cambiado.

: Numero de módulos en la versión anterior que se han borrado en la
versión actual.

Se obtuvo un valor de

en el índice de madurez, como es la primera

versión, es decir el producto se comienza a estabilizarse.

209

www.nitropdf.com

Para hallar la puntuación final de calidad de software, es decir el grado de
calidad de la solución del análisis de datos con herramientas de Big Data, se
promediaron los resultados de las métricas de McCall:

Donde:

: Puntuación de calidad de software.

: Corrección.

: Fiabilidad.

: Eficiencia.

: Integridad.

: Flexibilidad.

: Capacidad de Pruebas.

: Reusabilidad.

: Usabilidad.

: Mantenimiento.
Entonces:
210

www.nitropdf.com

El valor de la puntuación de calidad de la solución es de

, en la escala

de 0 a 10, este valor indica que la solución del análisis de datos es una
solución de calidad.

4.6

Prueba de Hipótesis
Para la prueba de hipótesis se formuló las siguientes hipótesis

estadísticas, considerando la aplicación, de escala de 0 a 25, respecto a la
solución del análisis de datos con herramientas de Big Data para el apoyo en la
toma de decisiones en la UGEL de Azángaro.

El análisis de datos con herramientas de Big Data apoya en la toma

de decisiones en una media menor a 20, es decir un apoyo regular en la toma
de decisiones en la UGEL de Azángaro.

El análisis de datos con herramientas de Big Data apoya en la toma

de decisiones en una media mayor a 20, es decir un apoyo alto en la toma de
decisiones en la UGEL de Azángaro.

211

www.nitropdf.com

La prueba estadística para probar la hipótesis se ha utilizado la
distribución t debido a que el número de usuarios de la solución del análisis de
datos para la toma de decisiones es 40.

Para la regla de decisión se trabajó con distribución t, la tabla muestra
para una prueba de una cola para

critico de

y 39 grados de libertad, el valor

es 1.6849; tiene un valor positivo debido a que la hipótesis alterna

especifica que la media es mayor que 20, es decir la región critica se encuentra
a la derecha de la media 0 de la distribución como se observa en la Figura Nº

212

www.nitropdf.com

60. Entonces la regla de decisión es: rechazar la

si

y se acepta la

Figura Nº 60: Distribución t.
Elaboración: El investigador.

Conclusión de la prueba de hipótesis.

Como

es mayor que

se rechaza la hipótesis nula y se

acepta la hipótesis alternativa, entonces se prueba la validez de la hipótesis
con un nivel de significancia

, por lo tanto se puede afirmar que el

análisis de datos con herramientas de Big Data apoyará en la toma de
decisiones en la UGEL de Azángaro.

213

www.nitropdf.com

CONCLUSIONES
Las conclusiones obtenidas a raíz de este proyecto son:
PRIMERO: El análisis realizado a los: tipos de documentos que se encuentran
en cada oficina, los documentos que se presentan más y los tipos de
solicitudes que se presentan más a la institución tienen correlación. Ya que se
encontró que el tipo de documento “Solicitud” es una de las más presentadas a
la institución, por lo que se buscó patrones más comunes respecto al asunto
del documento y conocer los resultados obtenidos. Así el personal
administrativo y los usuarios de la solución calificaron como buena a la solución
de análisis de datos con herramientas de Big Data, con una ponderación de 22
en base al puntaje máximo de 25.
SEGUNDO: La implementación del modelo de regresión logística en la fase de
modelado de la metodología CRISP-DM permitió predecir la cantidad de
documentos finalizados, mediante el uso de los datos de prueba y
entrenamiento para el modelo.
TERCERO: El modelo MapReduce utilizado en la fase de modelado de la
metodología CRISP-DM permitió procesar los tipos de solicitudes que se
presentaban más a la institución, para ver la posibilidad de sistematizar las
áreas encargadas de gestionar dicha documentación.
214

www.nitropdf.com

CUARTO: Los reportes mostrados con la herramienta RStudio permitió
visualizar los resultados obtenidos con la implementación de los modelos
construidos. Y la satisfacción de dichos reportes se midió con la métrica de
calidad de McCall obteniendo un grado de satisfacción de 8.17 en una escala
de 0 a 10.
QUINTO: Hadoop ha resultado ser una solución que se adapta y encaja muy
bien con Big Data, mostrando ser un sistema escalable, con tolerancia a
errores y a diferentes cambios en la arquitectura, “tanto a nivel de diseño o
funcionalidades como a nivel de infraestructura”, con un buen rendimiento en el
trato de datos estructurados y con una alta flexibilidad para añadir nuevas
herramientas a una solución.

215

www.nitropdf.com

SUGERENCIAS
PRIMERO: El tema de esta investigación o el área de estudio (Big Data) puede
resultar interesante para cualquier estudiante de Ingeniería de Sistemas, por lo
que sería muy conveniente que se cree un área de estudio del tema.
SEGUNDO: La solución final puede ser analizado desde otra perspectiva, si es
que se tiene otro objetivo para el análisis de datos.
TERCERO: La solución final puede tener una amplia aceptación en el
mercado, ya que actualmente las instituciones necesitan manejar conocimiento
que les permita mejorar sus procesos de trabajo.

216

www.nitropdf.com

BIBLIOGRAFÍA
Abedin, J. (2014). Data Manipulation with R. Birmingham, UK: Packt Publishing.
Adler, J. (2012). R in a Nutshell, Second Edition. United States of America:
O'Reilly Media, Inc.
Barker, T. (2013). Pro Data Visualization using R and Javascript. United States
of America: Apress.
Bhagattjee, B. (2014). Emergence and Taxonomy of Big Data as a Service.
(Working Paper CISL# 2014-06): Massachusetts Institute of Technology,
U.S.A.
Bhushan Agarwal, B., & Prakash Tayal, S. (2009). Data Mining and Data
Warehousing. United States of America: Laxmi Publications.
Chapman, P., Clinton, J., Kerber, R., Khabaza, T., Reinartz, T., Shearer, C., &
Wirth, R. (2000). CRISP-DM 1.0 Step-by-step data mining guide. United
States of America.
Chatterjee, S., & Hadi, A. (2006). Regression analysis by example (Vol. 607).
Wiley-Interscience.
Chen, M., Mao, S., & Liu, Y. (2014). Big Data: A Survey. New York, United
States of America.: Springer.
Cotton, R. (2013). Learning R. United States of America: O'Reilly Media, Inc.
Gates, A. (2011). Programming Pig. United States of America: O'Reilly Media,
Inc.
217

www.nitropdf.com

Gómez Orts, F. J. (2013). Metodología para implementar en la nube
aplicaciones web basadas en java. Universidad Politécnica de Valéncia,.
Hurwitz, J., Nugent, A., Halper, D., & Kaufman, M. (2013). Big Data For
Dummies. United States of America: John Wiley & Sons, Inc. U.S.
Janakiraman, V. S., & Sarukesi, K. (2006). Decision Support Systems. New
Delhi: Prentice-Hall.
Kahneman, D., & Tversky, A. (2000). Choices, Values, and Frames. New York,
NY, USA.: Cambridge University Press.
Lampi, J. (2014). Large-scale distributed data management and processing
using R, Hadoop and MapReduce. (Master's Thesis): University of Oulu,
Finland.
Leskovec, J., Rajaraman, A., & Ullman, J. D. (2014). Mining of Massive
Datasets. United States of America: Stanford University.
Lowendahl, J. (2014). Hype Cycle for Education. Stamford, United States.:
Gartner, Inc.
Mike, D. (2012). Getting Started with D3. United States of America: O'Reilly
Media, Inc.
Moine, J. M. (2013). Metodologías para el descubrimiento de conocimiento en
base de datos: un estudio comparativo. (Tesis en Ingeniería de
Software): Universidad Nacional de la Plata, Argentina.

218

www.nitropdf.com

Murthy, A. C., Vavilapalli, V. K., Eadline, D., Niemiec, J., & Markham, J. (2014).
Apache Hadoop YARN: moving beyond MapReduce and batch
processing with Apache Hadoop 2. United States of America: AddisonWesley.
Pace, L. (2012). Beginning R: An Introduction to Statistical Programing. United
States of America: Apress.
Pasupuleti, P. (2014). Pig Design Patterns. Birmingham, UK: Packt Publishing.
Power, D. J. (2002). Decision Support Systems: concepts and resources for
managers. United States of America: Greenwood Publishing Group, Inc.
Prajapati, V. (2013). Big Data Analytics With R and Hadoop. Birmingham, UK:
Packt Publishing.
Pressman, R. S. (2002). Ingenieria de software: un enfoque practico. Madrid:
McGRAW HILL.
Rubio Echevarría, R. (2014). Estudio, análisis y evaluación del framework
Hadoop. (Tesis de Ingeniero Informático): Universidad Pontificia
Comillas, España.
Salinas Ortiz, J. A. (2013). Análisis de decisiones estratégicas: en entornos
inciertos, cambiantes y complejos. México: CENGAGE Learning.
Sosinsky, B. (2011). Cloud Computing Bible. United States of America: Wiley
Publishing, Inc. U.S.

219

www.nitropdf.com

Tigani, J., & Naidu, S. (2014). Google BigQuery Analytics. United States of
America: John Wiley & Sons, Inc.
Tiwari, S. (2011). Professional NoSQL. United States of America: Wrox Press.
Velte, A. T., Velte, T. J., & Elsenpeter, R. (2010). Cloud computing a practical
approach. U.S.: McGraw-Hill, United States.
Wadkar, S., & Siddalingaiah, M. (2014). Pro Apache Hadoop (Second Edition).
United States of America: Apress.
Zadrozny, P., & Kodali, R. (2013). Big Data Analytics Using Splunk. United
States of America: Apress.

220

www.nitropdf.com

ANEXOS
Anexo 1: Arquitectura.
En este anexo se describe la arquitectura que se ha usado durante el cuarto
capítulo del proyecto, tanto para la realización de las pruebas como para los
modelos desarrollados. La infraestructura era básicamente una máquina con la
siguiente configuración:


Modelo: Toshiba Satellite C845 SP4333KL



Procesador: Intel Core i3 2.20 GHz



Memoria RAM: 8GB



Capacidad del disco: 520 GB



Sistema Operativo: Windows 8



Software de Virtualización de máquinas: Hyper-V 6.2.9200.16384
Dentro de esta máquina se han instalado tres máquinas virtuales (ver Figura

Nº 61), todas con las mismas especificaciones:


Hilos de ejecución: 1 hilo de ejecución



Memoria RAM: 1GB



Espacio de disco: 100GB (expansión dinámica)



Sistema Operativo: Ubuntu Server 14.04.1 LTS



Distribución Hadoop instalada: Hadoop 2.6
221

www.nitropdf.com



Distribución RStudio instalada: RStudio 0.98.1102



Distribución Sqoop instalada: Sqoop 1.99.5

Figura Nº 61: Administrador de Hyper-V con las tres máquinas virtuales.

Figura Nº 62: Esquema de la arquitectura empleada en el proyecto.
Fuente: Elaboración propia.

222

www.nitropdf.com

En la Figura Nº 62 se puede ver el esquema de la arquitectura usada en el
proyecto. Las máquinas virtuales de la máquina anfitrión pueden conectarse a
la intranet de la institución y tenían acceso a internet. Para poder trabajar más
cómodamente y de manera remota se ha usado el software PuTTY que permite
trabajar en una maquina externa (conectada a través de una red) y visualizar la
pantalla en una ventana (ver Figura Nº 63).

Figura Nº 63: Trabajando remotamente en un nodo a través de PuTTY.

223

www.nitropdf.com

Anexo 2: Proceso de instalación y configuración.
En este anexo se detalla la instalación y configuración de las distribuciones
(Hadoop, Sqoop y RStudio) sobre un clúster como el especificado en el Anexo
1: Arquitectura en un entorno Linux Ubuntu Server 14.04.1 LTS. Las
siguientes configuraciones fueron idéntica en todas las máquinas virtuales.


Edición del archivo hosts (sudo nano /etc/hosts).
#127.0.0.1
192.168.0.60
192.168.0.61
192.168.0.62



localhost
hadoop2nn
hadoop2dn1
hadoop2dn2

Instalación de Java y openssh-server.
sudo add-apt-repository ppa:webupd7team/java -y
sudo apt-get update
sudo apt-get install oracle-java7-installer
sudo apt-get install oracle-java7-set-default
sudo apt-get install openssh-server



Creación del usuario “hduser” y otorgación del permiso de sudo.
sudo addgroup hadoop
sudo adduser --ingroup hadoop hduser
sudo adduser hduser sudo



Ingresar a un nodo (hadoop2nn) con la cuenta del nuevo usuario para la
creación de las llaves SSH y poder acceder a los otros servidores (de aquí
en adelante todos los comandos se ejecutarán con el usuario “hduser”).
sudo su - hduser
ssh-keygen -t rsa -P "" -f ~/.ssh/id_rsa
cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys

224

www.nitropdf.com



Copiar todos los archivos creados, sobre los otros nodos del clúster.
ssh-copy-id -i ~/.ssh/id_rsa.pub hduser@hadoop2dn1
ssh-copy-id -i ~/.ssh/id_rsa.pub hduser@hadoop2dn2
ssh localhost



Descargar Hadoop de uno de los servidores disponibles.
wget http://www.eu.apache.org/dist/hadoop/core/hadoop-2.6.0/hadoop-2.6.0.tar.gz
tar xvzf hadoop-2.6.0.tar.gz
rm hadoop-2.6.0.tar.gz
sudo mv hadoop-2.6.0 /usr/local/hadoop/



Cambiar de propietario la carpeta hadoop para el usuario hduser.
sudo chown -R hduser:hadoop /usr/local/hadoop/



Configurar los directorios para los datos y logs.
mkdir -pv /usr/local/hadoop/data/namenode
mkdir -pv /usr/local/hadoop/data/datanode
sudo mkdir /var/log/hadoop/
sudo chown hduser:hadoop /var/log/hadoop/



Configurar las variables de entorno (sudo nano ~/.bashrc).
export JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64/
export PATH=$PATH:$JAVA_HOME/bin/
export HADOOP_PREFIX=/usr/local/hadoop/
export PATH=$PATH:$HADOOP_PREFIX/bin
export PATH=$PATH:$HADOOP_PREFIX/sbin
export HADOOP_CLASSPATH=$JAVA_HOME/lib/tools.jar



Editar los archivos de configuración de Hadoop del nodo “hadoop2nn” que
se encuentran dentro /usr/local/hadoop/etc/hadoop/.
225

www.nitropdf.com

sudo nano $HADOOP_PREFIX/etc/hadoop/hadoop-env.sh
export JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64/
export HADOOP_LOG_DIR=/var/log/hadoop/

sudo nano $HADOOP_PREFIX/etc/hadoop/yarn-env.sh
export YARN_LOG_DIR=/var/log/hadoop/

sudo nano $HADOOP_PREFIX/etc/hadoop/core-site.xml
<configuration>
<property>
<name>fs.defaultFS</name>
<value>hdfs://hadoop2nn:9000</value>
</property>
</configuration>

sudo nano $HADOOP_PREFIX/etc/hadoop/hdfs-site.xml
<configuration>
<property>
<name>dfs.datanode.data.dir</name>
<value>file:///usr/local/hadoop/data/datanode</value>
<description>DataNode directory</description>
</property>
<property>
<name>dfs.namenode.name.dir</name>
<value>file:///usr/local/hadoop/data/namenode</value>
<description>NameNode directory for namespace and transaction logs
storage.</description>
</property>
<property>
<name>dfs.replication</name>
<value>2</value>
</property>
</configuration>

226

www.nitropdf.com

sudo nano $HADOOP_PREFIX/etc/hadoop/yarn-site.xml
<configuration>
<property>
<name>yarn.resourcemanager.hostname</name>
<value>hadoop2nn</value>
</property>
<property>
<name>yarn.resourcemanager.scheduler.address</name>
<value>hadoop2nn:8030</value>
</property>
<property>
<name>yarn.resourcemanager.resource-tracker.address</name>
<value>hadoop2nn:8031</value>
</property>
<property>
<name>yarn.resourcemanager.address</name>
<value>hadoop2nn:8032</value>
</property>
<property>
<name>yarn.resourcemanager.admin.address</name>
<value>hadoop2nn:8033</value>
</property>
<property>
<name>yarn.resourcemanager.webapp.address</name>
<value>0.0.0.0:8088</value>
</property>
<property>
<name>yarn.nodemanager.aux-services</name>
<value>mapreduce_shuffle</value>
</property>
<property>
<name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
<value>org.apache.hadoop.mapred.ShuffleHandler</value>
</property>
</configuration>

sudo nano $HADOOP_PREFIX/etc/hadoop/mapred-site.xml
<configuration>
<property>
<name>mapreduce.framework.name</name>
<value>yarn</value>
</property>
</configuration>

227

www.nitropdf.com

sudo nano $HADOOP_PREFIX/etc/hadoop/slaves
hadoop2nn
hadoop2dn1
hadoop2dn2



Copiar todos los archivos de configuración del nodo “hadoop2nn” hacia los
otros nodos.
scp -r $HADOOP_PREFIX/etc/hadoop hduser@192.168.0.61:$HADOOP_PREFIX/etc/
scp -r $HADOOP_PREFIX/etc/hadoop hduser@192.168.0.62:$HADOOP_PREFIX/etc/



Formatear el sistema Hadoop (namenode) en el nodo “hadoop2nn” para
empezar con su uso.
hdfs namenode -format



En el nodo “hadoop2nn” ejecutar los servicios de HDFS, Yarn y el Historial
de trabajos.
start-dfs.sh
start-yarn.sh
mr-jobhistory-daemon.sh start historyserver



Verificación de la ejecución de los servicios de Hadoop.

Figura Nº 64: Verificación de los servicios en el nodo “hadoop2nn”.

228

www.nitropdf.com

Figura Nº 65: Verificación de los servicios en el nodo “hadoop2dn1”.

Figura Nº 66: Verificación de los servicios en el nodo “hadoop2dn2”.

Figura Nº 67: La consola del Name Node escucha el puerto 50070 “http://hadoop2nn:50070”.

229

www.nitropdf.com

Figura Nº 68: La gestión de recursos de YARN escucha el puerto 8088
“http://hadoop2nn:8088/cluster/nodes”.

Figura Nº 69: La consola de Historial de trabajos escucha el puerto 19888
“http://hadoop2nn:19888/jobhistory/app”.



Descargar Sqoop de uno de los servidores disponibles en uno de los nodos
(se eligió el nodo hadoop2nn para el proyecto).

230

www.nitropdf.com

wget http://mirror.symnds.com/software/Apache/sqoop/1.99.5/sqoop-1.99.5-binhadoop200.tar.gz
tar xvzf sqoop-1.99.5-bin-hadoop200.tar.gz
rm sqoop-1.99.5-bin-hadoop200.tar.gz
sudo mv sqoop-1.99.5-bin-hadoop200 /usr/local/sqoop/



Cambiar de propietario la carpeta sqoop para el usuario hduser.
sudo chown -R hduser:hadoop /usr/local/sqoop/



Configurar las variables de entorno (sudo nano ~/.bashrc).
export SQOOP_HOME=/use/app/sqoop
export PATH=$PATH:$SQOOP_HOME/bin
export CATALINA_BASE=$SQOOP_HOME/server
export LOGDIR=$SQOOP_HOME/logs/



Editar los archivos de configuración de Sqoop del nodo “hadoop2nn” que se
encuentran dentro /usr/local/sqoop/server/conf/ y modificarlos.
sudo nano /usr/local/sqoop/server/conf/sqoop.properties
org.apache.sqoop.submission.engine.mapreduce.configuration.directory=/usr/local/hadoop/e
tc/hadoop/

sudo nano /usr/local/sqoop/server/conf/catalina.properties
common.loader=${catalina.base}/lib,${catalina.base}/lib/*.jar,${catalina.home}/lib,${catalina.h
ome}/lib/*.jar,${catalina.home}/../lib/*.jar,/usr/local/hadoop/share/hadoop/common/*.jar,/u
sr/local/hadoop/share/hadoop/common/lib/*.jar,/usr/local/hadoop/share/hadoop/hdfs/*.jar,
/usr/local/hadoop/share/hadoop/hdfs/lib/*.jar,/usr/local/hadoop/share/hadoop/mapreduce/
*.jar,/usr/local/hadoop/share/hadoop/mapreduce/lib/*.jar,/usr/local/hadoop/share/hadoop/
tools/*.jar,/usr/local/hadoop/share/hadoop/tools/lib/*.jar,/usr/local/hadoop/share/hadoop/y
arn/*.jar,/usr/local/hadoop/share/hadoop/yarn/lib/*.jar



Descargar y copiar el conector de MySQL para Java y verificamos la
configuración de Sqoop Server.

231

www.nitropdf.com

mkdir -pv /usr/local/sqoop/lib
wget http://central.maven.org/maven2/mysql/mysql-connector-java/5.1.34/mysql-connectorjava-5.1.34.jar
cp mysql-connector-java-5.1.34-bin.jar /usr/local/sqoop/lib/
cd /usr/local/sqoop/bin
sh ./sqoop2-tool verify



Otorgar privilegios al usuario “hadoop” en MySQL.
mysql –u root -p
GRANT ALL PRIVILEGES ON *.* TO hadoop@'%' IDENTIFIED BY 'hadoop';
FLUSH PRIVILEGES;



Iniciar el servicio del servidor de Sqoop.
sqoop.sh server start hadoop2nn:12000
sqoop.sh client
set server --host hadoop2nn --port 12000 --webapp sqoop
set option --name verbose --value true



Verificación de los servicios de Sqoop.

232

www.nitropdf.com

Figura Nº 70: Consola de Sqoop client donde muestra los conectores y la versión del
servidor Sqoop.

Figura Nº 71: Servidor de Sqoop escucha el puerto 12000 “http://hadoop2nn:12000/”.

233

www.nitropdf.com

Figura Nº 72: Consola de Sqoop cliente, donde se muestra los link y todos los trabajos de
importación de datos de MySQL hacia HDFS del clúster de Hadoop.



Descargar RStudio y sus paquetes necesarios.
sudo apt-get update
sudo apt-get install r-base
sudo apt-get install r-base-dev
sudo apt-get install gdebi-core
sudo apt-get install libapparmor1
wget http://download2.rstudio.org/rstudio-server-0.98.1102-amd64.deb
sudo gdebi rstudio-server-0.98.1102-amd64.deb



Instalación de paquetes necesarios para trabajar con Hadoop en RStudio
(http://hadoop2nn:8787/).

234

www.nitropdf.com

Figura Nº 73: Instalación de paquetes en RStudio.

235

www.nitropdf.com

Anexo 3: Códigos de los modelos.
En este anexo se detalla los diversos códigos que fueron utilizados en la fase
de modelado de la metodología CRISP-DM.


Código del modelo MapReduce.
import
import
import
import
import
import
import
import
import

java.io.BufferedReader;
java.io.FileReader;
java.io.IOException;
java.net.URI;
java.util.ArrayList;
java.util.HashSet;
java.util.List;
java.util.Set;
java.util.StringTokenizer;

import
import
import
import
import
import
import
import
import
import
import
import

org.apache.hadoop.conf.Configuration;
org.apache.hadoop.fs.Path;
org.apache.hadoop.io.IntWritable;
org.apache.hadoop.io.Text;
org.apache.hadoop.mapreduce.Job;
org.apache.hadoop.mapreduce.Mapper;
org.apache.hadoop.mapreduce.Reducer;
org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
org.apache.hadoop.mapreduce.Counter;
org.apache.hadoop.util.GenericOptionsParser;
org.apache.hadoop.util.StringUtils;

public class ExpedientCount {
public static class TokenizerMapper extends Mapper<Object, Text, Text,
IntWritable>{
static enum CountersEnum { INPUT_WORDS }
private final static IntWritable one = new IntWritable(1);
private Text word = new Text();
private boolean caseSensitive;
private Set<String> patternsToSkip = new HashSet<String>();
private Configuration conf;
private BufferedReader fis;

236

www.nitropdf.com

@Override
public void setup(Context context) throws IOException,
InterruptedException {
conf = context.getConfiguration();
caseSensitive = conf.getBoolean("wordcount.case.sensitive", true);
if (conf.getBoolean("wordcount.skip.patterns", false)) {
URI[] patternsURIs = Job.getInstance(conf).getCacheFiles();
for (URI patternsURI : patternsURIs) {
Path patternsPath = new Path(patternsURI.getPath());
String patternsFileName = patternsPath.getName().toString();
parseSkipFile(patternsFileName);
}
}
}
private void parseSkipFile(String fileName) {
try {
fis = new BufferedReader(new FileReader(fileName));
String pattern = null;
while ((pattern = fis.readLine()) != null) {
patternsToSkip.add(pattern);
}
} catch (IOException ioe) {
System.err.println("Caught exception while parsing the cached file '"
+ StringUtils.stringifyException(ioe));
}
}
@Override
public void map(Object key, Text value, Context context) throws
IOException, InterruptedException {
String line = (caseSensitive) ? value.toString() :
value.toString().toLowerCase();
for (String pattern : patternsToSkip) {
line = line.replaceAll(pattern, "");
}
StringTokenizer itr = new StringTokenizer(line);
while (itr.hasMoreTokens()) {
word.set(itr.nextToken());
context.write(word, one);
Counter counter = context.getCounter(CountersEnum.class.getName(),
CountersEnum.INPUT_WORDS.toString());
counter.increment(1);
}
}
}

237

www.nitropdf.com

public static class IntSumReducer extends
Reducer<Text,IntWritable,Text,IntWritable> {
private IntWritable result = new IntWritable();
public void reduce(Text key, Iterable<IntWritable> values, Context
context) throws IOException, InterruptedException {
int sum = 0;
for (IntWritable val : values) {
sum += val.get();
}
result.set(sum);
context.write(key, result);
}
}
public static void main(String[] args) throws Exception {
Configuration conf = new Configuration();
GenericOptionsParser optionParser = new GenericOptionsParser(conf, args);
String[] remainingArgs = optionParser.getRemainingArgs();
if (!(remainingArgs.length != 2 || remainingArgs.length != 4)) {
System.err.println("Usage: wordcount <in> <out> [-skip
skipPatternFile]");
System.exit(2);
}
Job job = Job.getInstance(conf, "expedient count");
job.setJarByClass(ExpedientCount.class);
job.setMapperClass(TokenizerMapper.class);
job.setCombinerClass(IntSumReducer.class);
job.setReducerClass(IntSumReducer.class);
job.setOutputKeyClass(Text.class);
job.setOutputValueClass(IntWritable.class);
List<String> otherArgs = new ArrayList<String>();
for (int i=0; i < remainingArgs.length; ++i) {
if ("-skip".equals(remainingArgs[i])) {
job.addCacheFile(new Path(remainingArgs[++i]).toUri());
job.getConfiguration().setBoolean("wordcount.skip.patterns", true);
} else {
otherArgs.add(remainingArgs[i]);
}
}
FileInputFormat.addInputPath(job, new Path(otherArgs.get(0)));
FileOutputFormat.setOutputPath(job, new Path(otherArgs.get(1)));
System.exit(job.waitForCompletion(true) ? 0 : 1);
}
}

238

www.nitropdf.com



Código (cantOficinasxArea.R) para mostrar las Oficinas.
Sys.setenv(HADOOP_CMD="/usr/local/hadoop/bin/hadoop")
library(rhdfs)
hdfs.init()
hdfs.ls('/datastore/tramitedocumentario/oficinas/input')
cantOficinas = hdfs.file("/datastore/tramitedocumentario/oficinas/input/ f71245da-dde54c29-8f4b-4441292e23f7.txt", "r",buffersize=5242880)
cantOficinas
m = hdfs.read(cantOficinas)
class(m)
mode(m)
c=rawToChar(m)
print(c)
tmp = read.table(textConnection(c),sep=",")
tmp
class(tmp)
mode(tmp)
attach(tmp)
newtmp <- tmp[order(V1, V3),]
detach(tmp)
newtmp
head(newtmp,n=22)
library('ggplot2')
ggplot(newtmp, aes(x=newtmp$V3,y=newtmp$V1,label=newtmp$V2)) +
geom_text(aes(colour=newtmp$V4)) +
xlab("Codigo del Area") +
ylab("Codigo de la Oficina") +
ggtitle("Cantidad de Oficinas por Area") +
scale_color_discrete(name="Color por cada Area") +
xlim(c(0,6)) +
ylim(c(20,60)) +
theme_linedraw()

239

www.nitropdf.com



Código (tipoDocumentos.R) para mostrar los Tipos de documentos.
Sys.setenv(HADOOP_CMD="/usr/local/hadoop/bin/hadoop")
library(rhdfs)
hdfs.init()
hdfs.ls('/datastore/tramitedocumentario/tipodocumentos/input')
tipoDocumentos = hdfs.file("/datastore/tramitedocumentario/tipodocumentos/input/
f63fa4e2-597e-4157-b47e-1f5df3f764c9.txt", "r",buffersize=5242880)
tipoDocumentos
m = hdfs.read(tipoDocumentos)
class(m)
mode(m)
c=rawToChar(m)
print(c)
tmp = read.table(textConnection(c),sep=",")
tmp
class(tmp)
mode(tmp)
attach(tmp)
newtmp <- tmp[order(V1),]
detach(tmp)
newtmp
plot(newtmp$V1, xlab="Indice", ylab="Codigo del Tipo de Documento") +
text(newtmp$V1, labels=newtmp$V2, col=newtmp$V1+1)

library('ggplot2')
ggplot(newtmp, aes(x=c(1:32),y=newtmp$V1,label=newtmp$V2)) +
geom_text(angle=45) +
xlab("Indice") +
ylab("Codigo del Tipo de Documento") +
ggtitle("Tipos de Documentos") +
xlim(c(0,32)) +
ylim(c(0,38)) +
theme_linedraw()

240

www.nitropdf.com



Código

(cantExpedientesxOficina.R)

para

mostrar

los

Tipos

de

documentos que encuentran en cada Oficina.

241

www.nitropdf.com

Sys.setenv(HADOOP_CMD="/usr/local/hadoop/bin/hadoop")
library(rhdfs)
hdfs.init()
cantExpxOficina =
hdfs.file("/datastore/tramitedocumentario/seguimientos/outputdosvariables/part-r-00000",
"r",buffersize=5242880)
m = hdfs.read(cantExpxOficina)
c=rawToChar(m)
library(stringr)
c=str_replace_all(c, c("\t"), ",")
tmp = read.table(textConnection(c),sep=",")
names(tmp) <- c("idOficina","tipoDocumento","cantExpedientes")
-----------------------cantOficinas = hdfs.file("/datastore/tramitedocumentario/oficinas/input/ f71245da-dde54c29-8f4b-4441292e23f7.txt", "r",buffersize=5242880)
mm = hdfs.read(cantOficinas)
cc=rawToChar(mm)
tmpp = read.table(textConnection(cc),sep=",")
names(tmpp) <- c("idOficina","descripcion","idArea","abreviacion")
------------------------tipoDocumentos = hdfs.file("/datastore/tramitedocumentario/tipodocumentos/input/
f63fa4e2-597e-4157-b47e-1f5df3f764c9.txt", "r",buffersize=5242880)
mmm = hdfs.read(tipoDocumentos)
ccc=rawToChar(mmm)
tmppp = read.table(textConnection(ccc),sep=",")
names(tmppp) <- c("tipoDocumento","valor")
------------------------newtmp <- merge(tmp, tmpp, by = "idOficina")
newtmpp <- merge(newtmp, tmppp, by = "tipoDocumento")
library('ggplot2')
ggplot(newtmpp,
aes(x=newtmpp$tipoDocumento,y=newtmpp$cantExpedientes,label=newtmpp$descripcion))
+
geom_text(aes(colour=newtmpp$valor),size=7) +
xlab("Tipo de Documento") +
ylab("Cantidad de Expedientes") +
ggtitle("Cantidad de Expedientes en cada Oficina") +
scale_color_discrete(name="Color por cada Tipo de Documento") +
xlim(c(0,6)) +
ylim(c(2000,8000)) +
theme_linedraw()



Código (cantExpedientesxCodigo.R) para mostrar los Documentos que se
presentan más.

242

www.nitropdf.com

Sys.setenv(HADOOP_CMD="/usr/local/hadoop/bin/hadoop")
library(rhdfs)
hdfs.init()
tipoDocumentos =
hdfs.file("/datastore/tramitedocumentario/tipodocumentos/input/f294d8b6-6f93-4da5-a7bd00b012c738d7.txt", "r",buffersize=5242880)
tipoDocumentos
m = hdfs.read(tipoDocumentos)
class(m)
mode(m)
c=rawToChar(m)
tmp = read.table(textConnection(c),sep=",")
names(tmp) <- c("tipoDocumento","valor")
------------------cantidadExpedientesCodigo =
hdfs.file("/datastore/tramitedocumentario/expedientes/output2/part-r-00000",
"r",buffersize=5242880)
cantidadExpedientesCodigo
mm = hdfs.read(cantidadExpedientesCodigo)
library(stringr)
cc=str_replace_all(cc, c("\t"), ",")
tmpp = read.table(textConnection(cc),sep=",")
names(tmpp) <- c("tipoDocumento","anoCreacion","mesCreacion","cantExpedientes")
--------------------newtmp <- merge(tmp, tmpp, by = "tipoDocumento")
newtmp$anoCreacion <- as.character(newtmp$anoCreacion)
--------------------library('ggplot2')
ggplot(newtmp, aes(newtmp$valor, newtmp$cantExpedientes, order = anoCreacion,
fill=anoCreacion)) +
xlab("Tipo de Documentos") +
ylab("Cantidad de Documentos") +
ggtitle("Expedientes Presentados Durante los ultimos años") +
geom_bar(stat="identity") +
scale_fill_brewer(palette="Set1") +
theme_linedraw() +
coord_flip()



Código (cantxTipoSolicitud.R) para mostrar los Tipos de solicitudes que
se presentan más.
243

www.nitropdf.com

Sys.setenv(HADOOP_CMD="/usr/local/hadoop/bin/hadoop")
library(rhdfs)
hdfs.init()
cantxTipoSolicitud = system("/usr/local/hadoop/bin/hdfs dfs -cat
/datastore/tramitedocumentario/expedientes/solicitudes/output/part-r-00000", intern=TRUE)
tmp = read.table(textConnection(cantxTipoSolicitud),sep="\t")
attach(tmp)
newtmp <- tmp[order(-V2),]
detach(tmp)
tipoDoc <- head(newtmp$V1,n=20)
tipo <- as.character(tipoDoc)
cant <- head(newtmp$V2,n=20)
data <- data.frame(tipo,cant)
newdata <- data[order(tipo,-cant),]
newdata$grupo <c("1","1","2","3","4","5","5","5","6","1","7","7","7","7","8","9","10","10","10","11")

ggplot(newdata, aes(newdata$tipo,newdata$cant, fill=grupo)) +
xlab("Tipo de Solicitud") +
ylab("Cantidad de Solicitud") +
ggtitle("Tipos de solicitudes Presentadas Durante los ultimos años") +
geom_bar(stat="identity") +
scale_fill_brewer(palette="Spectral") +
theme_linedraw() +
coord_flip()



Código (cantExpedientesFinalizados.R) del modelo de Regresión logística.
244

www.nitropdf.com

Sys.setenv(HADOOP_CMD="/usr/local/hadoop/bin/hadoop")
library(rhdfs)
hdfs.init()
cantExpFinalizado = system("/usr/local/hadoop/bin/hdfs dfs -cat
/datastore/tramitedocumentario/expedientes/finalizados/input/c1975caf-0af9-4cfb-ba0c5bc6356c3899.txt", intern=TRUE)
tmp1 = read.table(textConnection(cantExpFinalizado),sep=",")
names(tmp1) <c("idOficina","estado","tipoDocumento","numeroFolios","tipoEntidadDocumento","mesFinaliz
ado","anoFinalizado")
tmp1$estado <- factor(tmp1$estado)
summary(tmp1)
tmp <- table(tmp1$estado)
(tmp[[2]]/tmp[[1]])*100 #cerca del 57.35 % no estan finalizado
library(ggplot2)
library(gridExtra)
library(caret)
train <- tmp1[anoFinalizado==2013,]
test <- tmp1[anoFinalizado==2014,]
summary(train)
logit <- glm(estado ~ idOficina+tipoDocumento+numeroFolios+tipoEntidadDocumento,
family=binomial(link='logit'), data=train)
summary(logit)
test.probs <-predict(logit, test, type='response')
pred.logit <- rep('0',length(test.probs))
pred.logit[test.probs>=0.5] <- '1'
table(pred.logit, test$estado)
confusionMatrix(test$estado, pred.logit)
#ajustar el modelo
modelFit<- train(estado ~ idOficina+tipoDocumento, method='glm',preProcess=c('scale',
'center'), data=train, family=binomial(link='logit'))
summary(modelFit)
confusionMatrix(test$estado, predict(modelFit, test))
summary(train)
summary(test)
before <- ggplot(train, aes(x=c(1:12360), y=sort(estado), group=estado)) + geom_line()
after <- ggplot(test, aes(x=c(1:18580), y=sort(pred.logit), group=pred.logit)) + geom_line()
grid.arrange(before, after, nrow=1)

245

www.nitropdf.com

Anexo 4: Técnica de minería de datos.
Las técnicas de minería de datos son algoritmos que tienen por objetivo la
extracción de patrones del conjunto de datos. Las técnicas de clasificación
asumen que hay un conjunto de objetos que pertenecen a diferentes clases. La
etiqueta de clase es un valor (simbólico) discreto y es conocido para cada
objeto. El objetivo es construir modelos de clasificación (a veces llamados
clasificadores), que intentarán asignar la etiqueta de clase correcta a nuevos
objetos. Los modelos de clasificación son usados principalmente para el
modelado predictivo (Chapman, y otros, 2000). A continuación se describe a
una de las técnicas de clasificación utilizada en el proyecto: regresión logística.
Regresión logística Binaria
Es un modelo estadístico que permite establecer la relación entre una variable
dependiente cualitativa dicotómica o politómica y una o más variables
explicativas independientes, o covariables, ya sean cualitativas o cuantitativas y
predecir la pertenencia a un determinado grupo en términos de probabilidad
(Chatterjee & Hadi, 2006).
El modelo de regresión logística binaria es muy utilizado cuando se desea
estimar la probabilidad de que ocurra un suceso determinado. Es una
herramienta muy flexible en cuanto a las variables explicativas, debido a que
permite que las mismas sean numéricas o categóricas.
En esta técnica el problema de clasificación se aborda introduciendo una
variable ficticia binaria que representa la ocurrencia o no del suceso. Sea

la

246

www.nitropdf.com

variable de respuesta, entonces

si el suceso ocurre o bien

en caso

contrario.
En cuanto a los supuestos del modelo, se recomienda que no exista
multicolinealidad entre las variables explicativas numéricas, ya que esta
situación podría causar importantes sesgos en las estimaciones de la variable
dependiente.

Sea

el vector formado por las

variables explicativas, el primer enfoque para

el modelo de regresión sería:

Si llamamos

a la probabilidad de que

tome el valor

cuando

entonces:

La esperanza de

será:

Entonces:

Por lo tanto, el valor predicho por el modelo de regresión será la probabilidad
de que el suceso ocurra

, cuando

.

247

www.nitropdf.com

El inconveniente principal de esta formulación es que

en el intervalo

debería estar acotada

y no hay ninguna garantía de que la predicción verifique

esta restricción. Para garantizar esta situación debemos transformar la variable
de respuesta de algún modo tal que:

De esta forma, garantizaremos que el valor de

si exigimos que

se encuentre entre cero y uno

tenga esa propiedad.

Habitualmente se toma como

lineal resultante se denomina

la función de distribución logística, y el modelo

:

Los parámetros del modelo son:


La ordenada al origen



Los coeficientes de la regresión

El cálculo de los coeficientes del modelo y de sus errores estándar se realiza
por estimaciones de máxima verosimilitud, es decir, estimaciones que
maximizan la probabilidad de obtener los valores de la variable de respuesta

proporcionados por los datos de la muestra. A diferencia de la regresión lineal
múltiple, estas estimaciones no son de cálculo directo, por lo que debemos

248

www.nitropdf.com

recurrir a métodos iterativos como el método de Newton Raphson. Como el
cálculo es complejo, por lo general se requieren de rutinas de programación o
software estadístico.
Para determinar cuánto se modifican las probabilidades por unidad de cambio
en las variables explicativas, se utilizan los denominados “odd ratios” o “ratios
de probabilidades”:

Una vez que se han estimado los parámetros del modelo, dado un nuevo caso
descripto por el vector de variables explicativas

probabilidad de que ocurra el suceso

. En general, si

clasifica a la nueva instancia con la clase

como

, se puede calcular la

entonces se

, caso contrario se clasifica

.

249

www.nitropdf.com

250

www.nitropdf.com

Anexo 5: Diseño de cuestionario para la recolección de datos, con el fin
de realizar la construcción del modelo y a la vez tener mayor
conocimiento del negocio sobre la UGEL de Azángaro.
CUESTIONARIO
Recolección de datos en la UGEL de Azángaro para el apoyo en la Toma de
Decisiones.
1. ¿Cuál es el área a la cual usted presta servicios como profesional de la
UGEL de Azángaro?

2. ¿Cuál es su opinión acerca del uso de las tecnologías de información
dentro de la UGEL de Azángaro?

3. ¿Se encuentra en condiciones de responder rápidamente a los pedidos
de los documentos registrados en el sistema de gestión de los
documentos administrativos de la UGEL de Azángaro?

4. ¿Cuál es la importancia del análisis predictivo para la toma de
decisiones que debería considerar el Especialista del área dentro de la
UGEL de Azángaro?

5. ¿Qué cantidad de documentos se recepcionan y/o elaboran diariamente,
en el área al cual usted presta servicios dentro de la UGEL de
Azángaro?

251

www.nitropdf.com

6. ¿Qué cantidad de personal administrativo y/o docente trabajan en su
área, al cual usted presta servicios dentro de la UGEL de Azángaro?

7. ¿Con que recursos de hardware y software cuenta actualmente su área,
al cual usted presta servicios dentro de la UGEL de Azángaro?

8. ¿Qué datos de los documentos recepcionados y/o elaborados en su
área al cual usted presta servicios dentro de la UGEL de Azángaro se
registran en el sistema de gestión de los documentos administrativos?

252

www.nitropdf.com

Anexo 6: Encuesta POST-TEST para medir la variable “Toma de
decisiones en la UGEL de Azángaro”.
1. La información que brinda el análisis de datos con respecto a los Patrones
Encontrados para ser aprovechados en beneficio del negocio (en la UGEL
de Azángaro) ¿Cuál sería su calificación?
Muy bueno

Bueno

Regular

Malo

Muy malo

2. La información que brinda el análisis de datos con respecto a la
Comprensión del Negocio (en la UGEL de Azángaro) ¿Cuál sería su
calificación?
Muy bueno

Bueno

Regular

Malo

Muy malo

3. La información que brinda el análisis de datos con respecto al Desempeño
Pasado de la UGEL de Azángaro ¿Cuál sería su calificación?
Muy bueno

Bueno

Regular

Malo

Muy malo

4. Con respecto al consolidado final de los Patrones Encontrados,
Comprensión del Negocio y el Desempeño Pasado de la UGEL de Azángaro
para enriquecer su toma de decisiones y mejorar sus procesos ¿Cómo lo
calificaría?
Muy bueno

Bueno

Regular

Malo

Muy malo

5. En forma general ¿Cómo calificaría la información que brinda el análisis de
datos para la Toma de Decisiones en la UGEL de Azángaro?
Muy bueno

Bueno

Regular

Malo

Muy malo

6. Por favor escriba su opinión, ideas, aportes, preguntas para mejorar la
información que brinda el análisis de datos.

253

www.nitropdf.com

Anexo 7: Diseño del cuestionario para poder aplicar las pruebas de
calidad de McCall en la variable “Análisis de datos con herramientas de
Big Data”.
Los factores de calidad se clasifican en tres grupos, y de acuerdo ellas se
realizaron las siguientes preguntas.
1. Operaciones del producto, Características operativas
Corrección (¿Hace lo que se le pide?)
El grado en que una aplicación satisface sus especificaciones y consigue
los objetivos encomendados por el cliente.
Fiabilidad (¿Lo hace de forma fiable, todo el tiempo?)
El grado que se puede esperar de una aplicación lleve a cabo las
operaciones especificadas y con la precisión requerida.
Eficiencia (¿Qué recursos hardware y software necesito?)
La cantidad de recursos hardware y software que necesita una aplicación
para realizar las operaciones con los tiempos de respuesta adecuados.
Integridad (¿Puedo controlar su uso?)
El grado con que puede controlarse el acceso al software o a los datos a
personal, no autorizado.
Facilidad de uso (¿Es fácil y cómodo de manejar?
El esfuerzo requerido para aprender el manejo de una aplicación, trabajar
con ella, introducir y conseguir resultados.
254

www.nitropdf.com

2. Revisión del Producto, Capacidad para soportar cambios
Facilidad de Mantenimiento (¿Puedo localizar los fallos?)
El esfuerzo requerido para localizar y reparar errores
Flexibilidad (¿Puedo añadir nuevas opciones?)
El esfuerzo requerido para modificar una aplicación en funcionamientos
Facilidad de prueba (¿Puedo probar todas las opciones?)
El esfuerzo requerido para probar una aplicación de forma que cumpla con
lo especificado en los requisitos.
3. Transición del Producto, Adaptabilidad a nuevos entornos
Portabilidad (¿Podré usarlo en otra máquina?)
El esfuerzo requerido para transferir la aplicación a otro hardware o sistema
operativo.
Reusabilidad (¿Podré utilizar alguna parte de la solución en otra
aplicación?)
Grado que define que partes de la solución pueden utilizarse en otras
aplicaciones.
Interoperabilidad (¿Podrá comunicarse con otras aplicaciones o sistemas
informáticos?)
El esfuerzo necesario para comunicar la aplicación con otras aplicaciones o
sistemas informáticos.
255

www.nitropdf.com

